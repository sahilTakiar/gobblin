{
    "docs": [
        {
            "location": "/", 
            "text": "Over the years, LinkedIn's data infrastructure team built custom solutions for ingesting diverse data entities into our Hadoop eco-system. At one point, we were running 15 types of ingestion pipelines which created significant data quality, metadata management, development, and operation challenges.\n\n\nOur experiences and challenges motivated us to build \nGobblin\n. Gobblin is a universal data ingestion framework for extracting, transforming, and loading large volume of data from a variety of data sources, e.g., databases, rest APIs, FTP/SFTP servers, filers, etc., onto Hadoop. Gobblin handles the common routine tasks required for all data ingestion ETLs, including job/task scheduling, task partitioning, error handling, state management, data quality checking, data publishing, etc. Gobblin ingests data from different data sources in the same execution framework, and manages metadata of different sources all in one place. This, combined with other features such as auto scalability, fault tolerance, data quality assurance, extensibility, and the ability of handling data model evolution, makes Gobblin an easy-to-use, self-serving, and efficient data ingestion framework.\n\n\nYou can find a lot of useful resources in our wiki pages, including \nhow to get started\n, \narchitecture overview\n,\n\nuser guide\n, \ndeveloper guide\n, and \nproject related information\n. We also provide a discussion group: \nGoogle Gobblin-Users Group\n. Please feel free to post any questions or comments.\n\n\nFor a detailed overview, please take a look at the \nVLDB 2015 paper\n.", 
            "title": "Home"
        }, 
        {
            "location": "/Getting-Started/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nDownload and Build\n\n\nRun Your First Job\n\n\nPreliminary\n\n\nSteps\n\n\n\n\n\n\nOther Example Jobs\n\n\n\n\n\n\nIntroduction\n\n\nThis guide will help you setup Gobblin, and run your first job. Currently, Gobblin requires JDK 7 or later to compile and run.\n\n\nDownload and Build\n\n\n\n\nCheckout Gobblin:\n\n\n\n\ngit clone https://github.com/linkedin/gobblin.git\n\n\n\n\n\n\nBuild Gobblin: Gobblin is built using Gradle.\n\n\n\n\ncd gobblin\n./gradlew clean build\n\n\n\n\nTo build against Hadoop 2, add \n-PuseHadoop2\n. To skip unit tests, add \n-x test\n.\n\n\nRun Your First Job\n\n\nHere we illustrate how to run a simple job. This job will pull the five latest revisions of each of the four Wikipedia pages: NASA, Linkedin, Parris_Cues and Barbara_Corcoran. A total of 20 records, each corresponding to one revision, should be pulled if the job is successfully run. The records will be stored as Avro files.\n\n\nGobblin can run either in standalone mode or on MapReduce. In this example we will run Gobblin in standalone mode.\n\n\nThis page explains how to run the job from the terminal. You may also run this job from your favorite IDE (IntelliJ is recommended).\n\n\nPreliminary\n\n\nEach Gobblin job minimally involves several constructs, e.g. \nSource\n, \nExtractor\n, \nDataWriter\n and [DataPublisher] (https://github.com/linkedin/gobblin/blob/master/gobblin-api/src/main/java/gobblin/publisher/DataPublisher.java). As the names suggest, Source defines the source to pull data from, Extractor implements the logic to extract data records, DataWriter defines the way the extracted records are output, and DataPublisher publishes the data to the final output location. A job may optionally have one or more Converters, which transform the extracted records, as well as one or more PolicyCheckers that check the quality of the extracted records and determine whether they conform to certain policies.\n\n\nSome of the classes relevant to this example include \nWikipediaSource\n, \nWikipediaExtractor\n, \nWikipediaConverter\n, \nAvroHdfsDataWriter\n and [BaseDataPublisher] (https://github.com/linkedin/gobblin/blob/master/gobblin-core/src/main/java/gobblin/publisher/BaseDataPublisher.java).\n\n\nTo run Gobblin in standalone mode we need a Gobblin configuration file (such as uses \ngobblin-standalone.properties\n). And for each job we wish to run, we also need a job configuration file (such as \nwikipedia.pull\n). The Gobblin configuration file, which is passed to Gobblin as a command line argument, should contain a property \njobconf.dir\n which specifies where the job configuration files are located. By default, \njobconf.dir\n points to environment variable \nGOBBLIN_JOB_CONFIG_DIR\n. Each file in \njobconf.dir\n with extension \n.job\n or \n.pull\n is considered a job configuration file, and Gobblin will launch a job for each such file. For more information on Gobblin deployment in standalone mode, refer to the \nStandalone Deployment\n page.\n\n\nA list of commonly used configuration properties can be found here: \nConfiguration Properties Glossary\n.\n\n\nSteps\n\n\n\n\n\n\nCreate a folder to store the job configuration file. Put \nwikipedia.pull\n in this folder, and set environment variable \nGOBBLIN_JOB_CONFIG_DIR\n to point to this folder. Also, make sure that the environment variable \nJAVA_HOME\n is set correctly.\n\n\n\n\n\n\nCreate a folder as Gobblin's working directory. Gobblin will write job output as well as other information there, such as locks and state-store (for more information, see the \nStandalone Deployment\n page). Set environment variable \nGOBBLIN_WORK_DIR\n to point to that folder.\n\n\n\n\n\n\nUnpack Gobblin distribution:\n\n\n\n\ntar -zxvf gobblin-dist-[project-version].tar.gz\ncd gobblin-dist\n\n\n\n\n\n\nLaunch Gobblin:\n\n\n\n\nbin/gobblin-standalone.sh start\n\n\n\n\nThis script will launch Gobblin and pass the Gobblin configuration file (\ngobblin-standalone.properties\n) as an argument.\n\n\nThe job log, which contains the progress and status of the job, will be written into \nlogs/gobblin-current.log\n (to change where the log is written, modify the Log4j configuration file \nconf/log4j-standalone.xml\n). Stdout will be written into \nnohup.out\n.\n\n\nAmong the job logs there should be the following information:\n\n\nINFO JobScheduler - Loaded 1 job configuration\nINFO  AbstractJobLauncher - Starting job job_PullFromWikipedia_1422040355678\nINFO  TaskExecutor - Starting the task executor\nINFO  LocalTaskStateTracker2 - Starting the local task state tracker\nINFO  AbstractJobLauncher - Submitting task task_PullFromWikipedia_1422040355678_0 to run\nINFO  TaskExecutor - Submitting task task_PullFromWikipedia_1422040355678_0\nINFO  AbstractJobLauncher - Waiting for submitted tasks of job job_PullFromWikipedia_1422040355678 to complete... to complete...\nINFO  AbstractJobLauncher - 1 out of 1 tasks of job job_PullFromWikipedia_1422040355678 are running\nINFO  WikipediaExtractor - 5 record(s) retrieved for title NASA\nINFO  WikipediaExtractor - 5 record(s) retrieved for title LinkedIn\nINFO  WikipediaExtractor - 5 record(s) retrieved for title Parris_Cues\nINFO  WikipediaExtractor - 5 record(s) retrieved for title Barbara_Corcoran\nINFO  Task - Extracted 20 data records\nINFO  Fork-0 - Committing data of branch 0 of task task_PullFromWikipedia_1422040355678_0\nINFO  LocalTaskStateTracker2 - Task task_PullFromWikipedia_1422040355678_0 completed in 2334ms with state SUCCESSFUL\nINFO  AbstractJobLauncher - All tasks of job job_PullFromWikipedia_1422040355678 have completed\nINFO  TaskExecutor - Stopping the task executor \nINFO  LocalTaskStateTracker2 - Stopping the local task state tracker\nINFO  AbstractJobLauncher - Publishing job data of job job_PullFromWikipedia_1422040355678 with commit policy COMMIT_ON_FULL_SUCCESS\nINFO  AbstractJobLauncher - Persisting job/task states of job job_PullFromWikipedia_1422040355678\n\n\n\n\n\n\nAfter the job is done, stop Gobblin by running\n\n\n\n\nbin/gobblin-standalone.sh stop\n\n\n\n\nThe job output is written in \nGOBBLIN_WORK_DIR/job-output\n folder as an Avro file.\n\n\nTo see the content of the job output, use the Avro tools to convert Avro to JSON. Download the latest version of Avro tools (e.g. avro-tools-1.7.7.jar):\n\n\ncurl -O http://central.maven.org/maven2/org/apache/avro/avro-tools/1.7.7/avro-tools-1.7.7.jar\n\n\n\n\nand run \n\n\njava -jar avro-tools-1.7.7.jar tojson --pretty [job_output].avro \n output.json\n\n\n\n\noutput.json\n will contain all retrieved records in JSON format.\n\n\nNote that since this job configuration file we used (\nwikipedia.pull\n) doesn't specify a job schedule, the job will run immediately and will run only once. To schedule a job to run at a certain time and/or repeatedly, set the \njob.schedule\n property with a cron-based syntax. For example, \njob.schedule=0 0/2 * * * ?\n will run the job every two minutes. See \nthis link\n (Quartz CronTrigger) for more details.\n\n\nOther Example Jobs\n\n\nBesides the Wikipedia example, we have another example job \nSimpleJson\n, which extracts records from JSON files and store them in Avro files.\n\n\nTo create your own jobs, simply implement the relevant interfaces such as \nSource\n, \nExtractor\n, \nConverter\n and \nDataWriter\n. In the job configuration file, set properties such as \nsource.class\n and \nconverter.class\n to point to these classes.\n\n\nOn a side note: while users are free to directly implement the Extractor interface (e.g., WikipediaExtractor), Gobblin also provides several extractor implementations based on commonly used protocols, e.g., \nRestApiExtractor\n, \nJdbcExtractor\n, \nSftpExtractor\n, etc. Users are encouraged to extend these classes to take advantage of existing implementations.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/Getting-Started/#table-of-contents", 
            "text": "Table of Contents  Introduction  Download and Build  Run Your First Job  Preliminary  Steps    Other Example Jobs", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/Getting-Started/#introduction", 
            "text": "This guide will help you setup Gobblin, and run your first job. Currently, Gobblin requires JDK 7 or later to compile and run.", 
            "title": "Introduction"
        }, 
        {
            "location": "/Getting-Started/#download-and-build", 
            "text": "Checkout Gobblin:   git clone https://github.com/linkedin/gobblin.git   Build Gobblin: Gobblin is built using Gradle.   cd gobblin\n./gradlew clean build  To build against Hadoop 2, add  -PuseHadoop2 . To skip unit tests, add  -x test .", 
            "title": "Download and Build"
        }, 
        {
            "location": "/Getting-Started/#run-your-first-job", 
            "text": "Here we illustrate how to run a simple job. This job will pull the five latest revisions of each of the four Wikipedia pages: NASA, Linkedin, Parris_Cues and Barbara_Corcoran. A total of 20 records, each corresponding to one revision, should be pulled if the job is successfully run. The records will be stored as Avro files.  Gobblin can run either in standalone mode or on MapReduce. In this example we will run Gobblin in standalone mode.  This page explains how to run the job from the terminal. You may also run this job from your favorite IDE (IntelliJ is recommended).", 
            "title": "Run Your First Job"
        }, 
        {
            "location": "/Getting-Started/#preliminary", 
            "text": "Each Gobblin job minimally involves several constructs, e.g.  Source ,  Extractor ,  DataWriter  and [DataPublisher] (https://github.com/linkedin/gobblin/blob/master/gobblin-api/src/main/java/gobblin/publisher/DataPublisher.java). As the names suggest, Source defines the source to pull data from, Extractor implements the logic to extract data records, DataWriter defines the way the extracted records are output, and DataPublisher publishes the data to the final output location. A job may optionally have one or more Converters, which transform the extracted records, as well as one or more PolicyCheckers that check the quality of the extracted records and determine whether they conform to certain policies.  Some of the classes relevant to this example include  WikipediaSource ,  WikipediaExtractor ,  WikipediaConverter ,  AvroHdfsDataWriter  and [BaseDataPublisher] (https://github.com/linkedin/gobblin/blob/master/gobblin-core/src/main/java/gobblin/publisher/BaseDataPublisher.java).  To run Gobblin in standalone mode we need a Gobblin configuration file (such as uses  gobblin-standalone.properties ). And for each job we wish to run, we also need a job configuration file (such as  wikipedia.pull ). The Gobblin configuration file, which is passed to Gobblin as a command line argument, should contain a property  jobconf.dir  which specifies where the job configuration files are located. By default,  jobconf.dir  points to environment variable  GOBBLIN_JOB_CONFIG_DIR . Each file in  jobconf.dir  with extension  .job  or  .pull  is considered a job configuration file, and Gobblin will launch a job for each such file. For more information on Gobblin deployment in standalone mode, refer to the  Standalone Deployment  page.  A list of commonly used configuration properties can be found here:  Configuration Properties Glossary .", 
            "title": "Preliminary"
        }, 
        {
            "location": "/Getting-Started/#steps", 
            "text": "Create a folder to store the job configuration file. Put  wikipedia.pull  in this folder, and set environment variable  GOBBLIN_JOB_CONFIG_DIR  to point to this folder. Also, make sure that the environment variable  JAVA_HOME  is set correctly.    Create a folder as Gobblin's working directory. Gobblin will write job output as well as other information there, such as locks and state-store (for more information, see the  Standalone Deployment  page). Set environment variable  GOBBLIN_WORK_DIR  to point to that folder.    Unpack Gobblin distribution:   tar -zxvf gobblin-dist-[project-version].tar.gz\ncd gobblin-dist   Launch Gobblin:   bin/gobblin-standalone.sh start  This script will launch Gobblin and pass the Gobblin configuration file ( gobblin-standalone.properties ) as an argument.  The job log, which contains the progress and status of the job, will be written into  logs/gobblin-current.log  (to change where the log is written, modify the Log4j configuration file  conf/log4j-standalone.xml ). Stdout will be written into  nohup.out .  Among the job logs there should be the following information:  INFO JobScheduler - Loaded 1 job configuration\nINFO  AbstractJobLauncher - Starting job job_PullFromWikipedia_1422040355678\nINFO  TaskExecutor - Starting the task executor\nINFO  LocalTaskStateTracker2 - Starting the local task state tracker\nINFO  AbstractJobLauncher - Submitting task task_PullFromWikipedia_1422040355678_0 to run\nINFO  TaskExecutor - Submitting task task_PullFromWikipedia_1422040355678_0\nINFO  AbstractJobLauncher - Waiting for submitted tasks of job job_PullFromWikipedia_1422040355678 to complete... to complete...\nINFO  AbstractJobLauncher - 1 out of 1 tasks of job job_PullFromWikipedia_1422040355678 are running\nINFO  WikipediaExtractor - 5 record(s) retrieved for title NASA\nINFO  WikipediaExtractor - 5 record(s) retrieved for title LinkedIn\nINFO  WikipediaExtractor - 5 record(s) retrieved for title Parris_Cues\nINFO  WikipediaExtractor - 5 record(s) retrieved for title Barbara_Corcoran\nINFO  Task - Extracted 20 data records\nINFO  Fork-0 - Committing data of branch 0 of task task_PullFromWikipedia_1422040355678_0\nINFO  LocalTaskStateTracker2 - Task task_PullFromWikipedia_1422040355678_0 completed in 2334ms with state SUCCESSFUL\nINFO  AbstractJobLauncher - All tasks of job job_PullFromWikipedia_1422040355678 have completed\nINFO  TaskExecutor - Stopping the task executor \nINFO  LocalTaskStateTracker2 - Stopping the local task state tracker\nINFO  AbstractJobLauncher - Publishing job data of job job_PullFromWikipedia_1422040355678 with commit policy COMMIT_ON_FULL_SUCCESS\nINFO  AbstractJobLauncher - Persisting job/task states of job job_PullFromWikipedia_1422040355678   After the job is done, stop Gobblin by running   bin/gobblin-standalone.sh stop  The job output is written in  GOBBLIN_WORK_DIR/job-output  folder as an Avro file.  To see the content of the job output, use the Avro tools to convert Avro to JSON. Download the latest version of Avro tools (e.g. avro-tools-1.7.7.jar):  curl -O http://central.maven.org/maven2/org/apache/avro/avro-tools/1.7.7/avro-tools-1.7.7.jar  and run   java -jar avro-tools-1.7.7.jar tojson --pretty [job_output].avro   output.json  output.json  will contain all retrieved records in JSON format.  Note that since this job configuration file we used ( wikipedia.pull ) doesn't specify a job schedule, the job will run immediately and will run only once. To schedule a job to run at a certain time and/or repeatedly, set the  job.schedule  property with a cron-based syntax. For example,  job.schedule=0 0/2 * * * ?  will run the job every two minutes. See  this link  (Quartz CronTrigger) for more details.", 
            "title": "Steps"
        }, 
        {
            "location": "/Getting-Started/#other-example-jobs", 
            "text": "Besides the Wikipedia example, we have another example job  SimpleJson , which extracts records from JSON files and store them in Avro files.  To create your own jobs, simply implement the relevant interfaces such as  Source ,  Extractor ,  Converter  and  DataWriter . In the job configuration file, set properties such as  source.class  and  converter.class  to point to these classes.  On a side note: while users are free to directly implement the Extractor interface (e.g., WikipediaExtractor), Gobblin also provides several extractor implementations based on commonly used protocols, e.g.,  RestApiExtractor ,  JdbcExtractor ,  SftpExtractor , etc. Users are encouraged to extend these classes to take advantage of existing implementations.", 
            "title": "Other Example Jobs"
        }, 
        {
            "location": "/Gobblin-Architecture/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nGobblin Architecture Overview\n\n\nGobblin Job Flow\n\n\nGobblin Constructs\n\n\nSource and Extractor\n\n\nConverter\n\n\nQuality Checker\n\n\nFork Operator\n\n\nData Writer\n\n\nData Publisher\n\n\n\n\n\n\nGobblin Task Flow\n\n\nJob State Management\n\n\nHandling of Failures\n\n\nJob Scheduling\n\n\n\n\n\n\nGobblin Architecture Overview\n\n\nGobblin is built around the idea of extensibility, i.e., it should be easy for users to add new adapters or extend existing adapters to work with new sources and start extracting data from the new sources in any deployment settings. The architecture of Gobblin reflects this idea, as shown in Fig. 1 below:\n\n\n\n  \n    \n    \n\n    \nFigure 1: Gobblin Architecture Overview\n\n  \n\n\n\n\n\nA Gobblin job is built on a set of constructs (illustrated by the light green boxes in the diagram above) that work together in a certain way and get the data extraction work done. All the constructs are pluggable through the job configuration and extensible by adding new or extending existing implementations. The constructs will be discussed in \nGobblin Constructs\n.\n\n\nA Gobblin job consists of a set of tasks, each of which corresponds to a unit of work to be done and is responsible for extracting a portion of the data. The tasks of a Gobblin job are executed by the Gobblin runtime (illustrated by the orange boxes in the diagram above) on the deployment setting of choice (illustrated by the red boxes in the diagram above). \n\n\nThe Gobblin runtime is responsible for running user-defined Gobblin jobs on the deployment setting of choice. It handles the common tasks including job and task scheduling, error handling and task retries, resource negotiation and management, state management, data quality checking, data publishing, etc.\n\n\nGobblin currently supports two deployment modes: the standalone mode on a single node and the Hadoop MapReduce mode on a Hadoop cluster. We are also working on adding support for deploying and running Gobblin as a native application on \nYARN\n. Details on deployment of Gobblin can be found in \nGobblin Deployment\n.\n\n\nThe running and operation of Gobblin are supported by a few components and utilities (illustrated by the blue boxes in the diagram above) that handle important things such as metadata management, state management, metric collection and reporting, and monitoring. \n\n\nGobblin Job Flow\n\n\nA Gobblin job is responsible for extracting data in a defined scope/range from a data source and writing data to a sink such as HDFS. It manages the entire lifecycle of data ingestion in a certain flow as illustrated by Fig. 2 below.\n\n\n\n  \n\n    \n\n    \nFigure 2: Gobblin Job Flow\n\n  \n\n\n\n\n\n\n\n\n\nA Gobblin job starts with an optional phase of acquiring a job lock. The purpose of doing this is to prevent the next scheduled run of the same job from starting until the current run finishes. This phase is optional because some job schedulers such as \nAzkaban\n is already doing this. \n\n\n\n\n\n\nThe next thing the job does is to create an instance of the \nSource\n class specified in the job configuration. A \nSource\n is responsible for partitioning the data ingestion work into a set of \nWorkUnit\ns, each of which represents a logic unit of work for extracting a portion of the data from a data source. A \nSource\n is also responsible for creating a \nExtractor\n for each \nWorkUnit\n. A \nExtractor\n, as the name suggests, actually talks to the data source and extracts data from it. The reason for this design is that Gobblin's \nSource\n is modeled after Hadoop's \nInputFormat\n, which is responsible for partitioning the input into \nSplit\ns as well as creating a \nRecordReader\n for each \nSplit\n. \n\n\n\n\n\n\nFrom the set of \nWorkUnit\ns given by the \nSource\n, the job creates a set of tasks. A task is a runtime counterpart of a \nWorkUnit\n, which represents a logic unit of work. Normally, a task is created per \nWorkUnit\n. However, there is a special type of \nWorkUnit\ns called \nMultiWorkUnit\n that wraps multiple \nWorkUnit\ns for which multiple tasks may be created, one per wrapped \nWorkUnit\n. \n\n\n\n\n\n\nThe next phase is to launch and run the tasks. How tasks are executed and where they run depend on the deployment setting. In the standalone mode on a single node, tasks are running in a thread pool dedicated to that job, the size of which is configurable on a per-job basis. In the Hadoop MapReduce mode on a Hadoop cluster, tasks are running in the mappers (used purely as containers to run tasks). \n\n\n\n\n\n\nAfter all tasks of the job finish (either successfully or unsuccessfully), the job publishes the data if it is OK to do so. Whether extracted data should be published is determined by the task states and the \nJobCommitPolicy\n used (configurable). More specifically, extracted data should be published if and only if any one of the following two conditions holds:\n\n\n\n\n\n\nJobCommitPolicy.COMMIT_ON_PARTIAL_SUCCESS\n is specified in the job configuration.\n\n\n\n\n\n\nJobCommitPolicy.COMMIT_ON_FULL_SUCCESS\n is specified in the job configuration and all tasks were successful.\n\n\n\n\n\n\nAfter the data extracted is published, the job persists the job/task states into the state store. When the next scheduled run of the job starts, it will load the job/task states of the previous run to get things like watermarks so it knows where to start.\n\n\n\n\n\n\nDuring its execution, the job may create some temporary working data that is no longer needed after the job is done. So the job will cleanup such temporary work data before exiting.  \n\n\n\n\n\n\nFinally, an optional phase of the job is to release the job lock if it is acquired at the beginning. This gives green light to the next scheduled run of the same job to proceed.  \n\n\n\n\n\n\nIf a Gobblin job is cancelled before it finishes, the job will not persist any job/task state nor commit and publish any data (as the dotted line shows in the diagram).\n\n\nGobblin Constructs\n\n\nAs described above, a Gobblin job creates and runs tasks, each of which is responsible for extracting a portion of the data to be pulled by the job. A Gobblin task is created from a \nWorkUnit\n that represents a unit of work and serves as a container of job configuration at runtime. A task composes the Gobblin constructs into a flow to extract, transform, checks data quality on, and finally writes each extracted data record to the specified sink. Fig. 3 below gives an overview on the Gobblin constructs that constitute the task flows in a Gobblin job. \n\n\n\n  \n\n    \n\n    \nFigure 3: Gobblin Constructs\n\n  \n\n\n\n\n\nSource and Extractor\n\n\nA \nSource\n represents an adapter between a data source and Gobblin and is used by a Gobblin job at the beginning of the job flow. A \nSource\n is responsible for partitioning the data ingestion work into a set of \nWorkUnit\ns, each of which represents a logic unit of work for extracting a portion of the data from a data source. \n\n\nA \nSource\n is also responsible for creating an \nExtractor\n for each \nWorkUnit\n. An \nExtractor\n, as the name suggests, actually talks to the data source and extracts data from it. The reason for this design is that Gobblin's \nSource\n is modeled after Hadoop's \nInputFormat\n, which is responsible for partitioning the input into \nSplit\ns as well as creating a \nRecordReader\n for each \nSplit\n. \n\n\nGobblin out-of-the-box provides some built-in \nSource\n and \nExtractor\n implementations that work with various types of of data sources, e.g., web services offering some Rest APIs, databases supporting JDBC, FTP/SFTP servers, etc. Currently, \nExtractor\ns are record-oriented, i.e., an \nExtractor\n reads one data record at a time, although internally it may choose to pull and cache a batch of data records. We are planning to add options for \nExtractor\ns to support byte-oriented and file-oriented processing.   \n\n\nConverter\n\n\nA \nConverter\n is responsible for converting both schema and data records and is the core construct for data transformation. \nConverter\ns are composible and can be chained together as long as each adjacent pair of \nConverter\ns are compatible in the input and output schema and data record types. This allows building complex data transformation from simple \nConverter\ns. Note that a \nConverter\n converts an input schema to one output schema. It may, however, convert an input data record to zero (\n1:0\n mapping), one (\n1:1\n mapping), or many (\n1:N\n mapping) output data records. Each \nConverter\n converts every output records of the previous \nConverter\n, except for the first one that converts the original extracted data record. When converting a data record, a \nConverter\n also takes in the \noutput converted\n schema of itself, except for the first one that takes in the original input schema. So each converter first converts the input schema and then uses the output schema in the conversion of each data record. The output schema of each converter is fed into both the converter itself for data record conversion and also the next converter. Fig. 4 explains how \nConverter\n chaining works using three example converters that have \n1:1\n, \n1:N\n, and \n1:1\n mappings for data record conversion, respectively.\n\n\n\n  \n\n    \n\n    \nFigure 4: How Converter Chaining Works\n\n  \n\n\n\n\n\nQuality Checker\n\n\nA \nQualityChecker\n, as the name suggests, is responsible for data quality checking. There are two types of \nQualityChecker\ns: one that checks individual data records and decides if each record should proceed to the next phase in the task flow and the other one that checks the entire task output and decides if data can be committed. We call the two types row-level \nQualityChecker\ns and task-level \nQualityChecker\ns, respectively. A \nQualityChecker\n can be \nMANDATORY\n or \nOPTIONAL\n and will participate in the decision on if quality checking passes if and only if it is \nMANDATORY\n. \nOPTIONAL\n \nQualityChecker\ns are informational only. Similarly to \nConverter\ns, more than one \nQualityChecker\n can be specified and in this case, quality checking passes if and only if all \nMANDATORY\n \nQualityChecker\ns give a \nPASS\n.     \n\n\nFork Operator\n\n\nA \nForkOperator\n is a type of control operators that allow a task flow to branch into multiple streams, each of which goes to a separately configured sink. This is useful for situations, e.g., that data records need to be written into multiple different storages, or that data records need to be written out to the same storage (say, HDFS) but in different forms for different downstream consumers. \n\n\nData Writer\n\n\nA \nDataWriter\n is responsible for writing data records to the sink it is associated to. Gobblin out-of-the-box provides an \nAvroHdfsDataWriter\n for writing data in \nAvro\n format onto HDFS. Users can plugin their own \nDataWriter\ns by specifying a \nDataWriterBuilder\n class in the job configuration that Gobblin uses to build \nDataWriter\ns.\n\n\nData Publisher\n\n\nA \nDataPublisher\n is responsible for publishing extracted data of a Gobblin job. Gobblin ships with a default \nDataPublisher\n that works with file-based \nDataWriter\ns such as the \nAvroHdfsDataWriter\n and moves data from the output directory of each task to a final job output directory. \n\n\nGobblin Task Flow\n\n\nFig. 5 below zooms in further and shows the details on how different constructs are connected and composed to form a task flow. The same task flow is employed regardless of the deployment setting and where tasks are running.\n\n\n\n  \n\n    \n\n    \nFigure 5: Gobblin Task Flow\n\n  \n\n\n\n\n\nA Gobblin task flow consists of a main branch and a number of forked branches coming out of a \nForkOperator\n. It is optional to specify a \nForkOperator\n in the job configuration. When no \nForkOperator\n is specified in the job configuration, a Gobblin task flow uses a \nIdentityForkOperator\n by default with a single forked branch. The \nIdentityForkOperator\n simply connects the master branch and the \nsingle\n forked branch and passes schema and data records between them. The reason behind this is it avoids special logic from being introduced into the task flow when a \nForkOperator\n is indeed specified in the job configuration.\n\n\nThe master branch of a Gobblin task starts with schema extraction from the source. The extracted schema will go through a schema transformation phase if at least one \nConverter\n class is specified in the job configuration. The next phase is to repeatedly extract data records one at a time. Each extracted data record will also go through a transformation phase if at least one \nConverter\n class is specified. Each extracted (or converted if applicable) data record is fed into an optional list of row-level \nQualityChecker\ns.\n\n\nData records that pass the row-level \nQualityChecker\ns will go through the \nForkOperator\n and be further processed in the forked branches. The \nForkOperator\n allows users to specify if the input schema or data record should go to a specific forked branch. If the input schema is specified \nnot\n to go into a particular branch, that branch will be ignored. If the input schema or data record is specified to go into \nmore than one\n forked branch, Gobblin assumes that the schema or data record class implements the \nCopyable\n interface and will attempt to make a copy before passing it to each forked branch. So it is very important to make sure the input schema or data record to the \nForkOperator\n is an instance of \nCopyable\n if it is going into \nmore than one\n branch.\n\n\nSimilarly to the master branch, a forked branch also processes the input schema and each input data record (one at a time) through an optional transformation phase and a row-level quality checking phase. Data records that pass the branch's row-level \nQualityChecker\ns will be written out to a sink by a \nDataWriter\n. Each forked branch has its own sink configuration and a separate \nDataWriter\n. \n\n\nUpon successful processing of the last record, a forked branch applies an optional list of task-level \nQualityChecker\ns to the data processed by the branch in its entirety. If this quality checking passes, the branch commits the data and exits. \n\n\nA task flow completes its execution once every forked branches commit and exit. During the execution of a task, a \nTaskStateTracker\n keeps track of the task's state and a core set of task metrics, e.g., total records extracted, records extracted per second, total bytes extracted, bytes extracted per second, etc.    \n\n\nJob State Management\n\n\nTypically a Gobblin job runs periodically on some schedule and each run of the job is extracting data incrementally, i.e., extracting new data or changes to existing data within a specific range since the last run of the job. To make incremental extraction possible, Gobblin must persist the state of the job upon the completion of each run and load the state of the previous run so the next run knows where to start extracting. Gobblin maintains a state store that is responsible for job state persistence. Each run of a Gobblin job reads the state store for the state of the previous run and writes the state of itself to the state store upon its completion. The state of a run of a Gobblin job consists of the job configuration and any properties set at runtime at the job or task level. \n\n\nOut-of-the-box, Gobblin uses an implementation of the state store that serializes job and task states into Hadoop \nSequenceFile\ns, one per job run. Each job has a separate directory where its job and task state \nSequenceFile\ns are stored. The file system on which the \nSequenceFile\n-based state store resides is configurable.   \n\n\nHandling of Failures\n\n\nAs a fault tolerance data ingestion framework, Gobblin employs multiple level of defenses against job and task failures. For job failures, Gobblin keeps track of the number of times a job fails consecutively and optionally sends out an alert email if the number exceeds a defined threshold so the owner of the job can jump in and investigate the failures. For task failures, Gobblin retries failed tasks in a job run up to a configurable maximum number of times. In addition to that, Gobblin also provides an option to enable retries of \nWorkUnit\ns corresponding to failed tasks across job runs. The idea is that if a task fails after all retries fail, the \nWorkUnit\n based on which the task gets created will be automatically included in the next run of the job if this type of retries is enabled. This type of retries is very useful in handling intermittent failures such as those due to temporary data source outrage.\n\n\nJob Scheduling\n\n\nLike mentioned above, a Gobblin job typically runs periodically on some schedule. Gobblin can be integrated with job schedulers such as \nAzkaban\n,\nOozie\n, or Crontab. Out-of-the-box, Gobblin also ships with a built-in job scheduler backed by a \nQuartz\n scheduler, which is used as the default job scheduler in the standalone deployment. An important feature of Gobblin is that it decouples the job scheduler and the jobs scheduled by the scheduler such that different jobs may run in different deployment settings. This is achieved using the  abstraction \nJobLauncher\n that has different implementations for different deployment settings. For example, a job scheduler may have 5 jobs scheduled: 2 of them run locally on the same host as the scheduler using the \nLocalJobLauncher\n, whereas the rest 3 run on a Hadoop cluster somewhere using the \nMRJobLauncher\n. Which \nJobLauncher\n to use can be simply configured using the property \nlauncher.type\n.", 
            "title": "Architecture"
        }, 
        {
            "location": "/Gobblin-Architecture/#table-of-contents", 
            "text": "Table of Contents  Gobblin Architecture Overview  Gobblin Job Flow  Gobblin Constructs  Source and Extractor  Converter  Quality Checker  Fork Operator  Data Writer  Data Publisher    Gobblin Task Flow  Job State Management  Handling of Failures  Job Scheduling", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/Gobblin-Architecture/#gobblin-architecture-overview", 
            "text": "Gobblin is built around the idea of extensibility, i.e., it should be easy for users to add new adapters or extend existing adapters to work with new sources and start extracting data from the new sources in any deployment settings. The architecture of Gobblin reflects this idea, as shown in Fig. 1 below:  \n       \n     \n     Figure 1: Gobblin Architecture Overview \n     A Gobblin job is built on a set of constructs (illustrated by the light green boxes in the diagram above) that work together in a certain way and get the data extraction work done. All the constructs are pluggable through the job configuration and extensible by adding new or extending existing implementations. The constructs will be discussed in  Gobblin Constructs .  A Gobblin job consists of a set of tasks, each of which corresponds to a unit of work to be done and is responsible for extracting a portion of the data. The tasks of a Gobblin job are executed by the Gobblin runtime (illustrated by the orange boxes in the diagram above) on the deployment setting of choice (illustrated by the red boxes in the diagram above).   The Gobblin runtime is responsible for running user-defined Gobblin jobs on the deployment setting of choice. It handles the common tasks including job and task scheduling, error handling and task retries, resource negotiation and management, state management, data quality checking, data publishing, etc.  Gobblin currently supports two deployment modes: the standalone mode on a single node and the Hadoop MapReduce mode on a Hadoop cluster. We are also working on adding support for deploying and running Gobblin as a native application on  YARN . Details on deployment of Gobblin can be found in  Gobblin Deployment .  The running and operation of Gobblin are supported by a few components and utilities (illustrated by the blue boxes in the diagram above) that handle important things such as metadata management, state management, metric collection and reporting, and monitoring.", 
            "title": "Gobblin Architecture Overview"
        }, 
        {
            "location": "/Gobblin-Architecture/#gobblin-job-flow", 
            "text": "A Gobblin job is responsible for extracting data in a defined scope/range from a data source and writing data to a sink such as HDFS. It manages the entire lifecycle of data ingestion in a certain flow as illustrated by Fig. 2 below.  \n   \n     \n     Figure 2: Gobblin Job Flow \n       A Gobblin job starts with an optional phase of acquiring a job lock. The purpose of doing this is to prevent the next scheduled run of the same job from starting until the current run finishes. This phase is optional because some job schedulers such as  Azkaban  is already doing this.     The next thing the job does is to create an instance of the  Source  class specified in the job configuration. A  Source  is responsible for partitioning the data ingestion work into a set of  WorkUnit s, each of which represents a logic unit of work for extracting a portion of the data from a data source. A  Source  is also responsible for creating a  Extractor  for each  WorkUnit . A  Extractor , as the name suggests, actually talks to the data source and extracts data from it. The reason for this design is that Gobblin's  Source  is modeled after Hadoop's  InputFormat , which is responsible for partitioning the input into  Split s as well as creating a  RecordReader  for each  Split .     From the set of  WorkUnit s given by the  Source , the job creates a set of tasks. A task is a runtime counterpart of a  WorkUnit , which represents a logic unit of work. Normally, a task is created per  WorkUnit . However, there is a special type of  WorkUnit s called  MultiWorkUnit  that wraps multiple  WorkUnit s for which multiple tasks may be created, one per wrapped  WorkUnit .     The next phase is to launch and run the tasks. How tasks are executed and where they run depend on the deployment setting. In the standalone mode on a single node, tasks are running in a thread pool dedicated to that job, the size of which is configurable on a per-job basis. In the Hadoop MapReduce mode on a Hadoop cluster, tasks are running in the mappers (used purely as containers to run tasks).     After all tasks of the job finish (either successfully or unsuccessfully), the job publishes the data if it is OK to do so. Whether extracted data should be published is determined by the task states and the  JobCommitPolicy  used (configurable). More specifically, extracted data should be published if and only if any one of the following two conditions holds:    JobCommitPolicy.COMMIT_ON_PARTIAL_SUCCESS  is specified in the job configuration.    JobCommitPolicy.COMMIT_ON_FULL_SUCCESS  is specified in the job configuration and all tasks were successful.    After the data extracted is published, the job persists the job/task states into the state store. When the next scheduled run of the job starts, it will load the job/task states of the previous run to get things like watermarks so it knows where to start.    During its execution, the job may create some temporary working data that is no longer needed after the job is done. So the job will cleanup such temporary work data before exiting.      Finally, an optional phase of the job is to release the job lock if it is acquired at the beginning. This gives green light to the next scheduled run of the same job to proceed.      If a Gobblin job is cancelled before it finishes, the job will not persist any job/task state nor commit and publish any data (as the dotted line shows in the diagram).", 
            "title": "Gobblin Job Flow"
        }, 
        {
            "location": "/Gobblin-Architecture/#gobblin-constructs", 
            "text": "As described above, a Gobblin job creates and runs tasks, each of which is responsible for extracting a portion of the data to be pulled by the job. A Gobblin task is created from a  WorkUnit  that represents a unit of work and serves as a container of job configuration at runtime. A task composes the Gobblin constructs into a flow to extract, transform, checks data quality on, and finally writes each extracted data record to the specified sink. Fig. 3 below gives an overview on the Gobblin constructs that constitute the task flows in a Gobblin job.   \n   \n     \n     Figure 3: Gobblin Constructs", 
            "title": "Gobblin Constructs"
        }, 
        {
            "location": "/Gobblin-Architecture/#source-and-extractor", 
            "text": "A  Source  represents an adapter between a data source and Gobblin and is used by a Gobblin job at the beginning of the job flow. A  Source  is responsible for partitioning the data ingestion work into a set of  WorkUnit s, each of which represents a logic unit of work for extracting a portion of the data from a data source.   A  Source  is also responsible for creating an  Extractor  for each  WorkUnit . An  Extractor , as the name suggests, actually talks to the data source and extracts data from it. The reason for this design is that Gobblin's  Source  is modeled after Hadoop's  InputFormat , which is responsible for partitioning the input into  Split s as well as creating a  RecordReader  for each  Split .   Gobblin out-of-the-box provides some built-in  Source  and  Extractor  implementations that work with various types of of data sources, e.g., web services offering some Rest APIs, databases supporting JDBC, FTP/SFTP servers, etc. Currently,  Extractor s are record-oriented, i.e., an  Extractor  reads one data record at a time, although internally it may choose to pull and cache a batch of data records. We are planning to add options for  Extractor s to support byte-oriented and file-oriented processing.", 
            "title": "Source and Extractor"
        }, 
        {
            "location": "/Gobblin-Architecture/#converter", 
            "text": "A  Converter  is responsible for converting both schema and data records and is the core construct for data transformation.  Converter s are composible and can be chained together as long as each adjacent pair of  Converter s are compatible in the input and output schema and data record types. This allows building complex data transformation from simple  Converter s. Note that a  Converter  converts an input schema to one output schema. It may, however, convert an input data record to zero ( 1:0  mapping), one ( 1:1  mapping), or many ( 1:N  mapping) output data records. Each  Converter  converts every output records of the previous  Converter , except for the first one that converts the original extracted data record. When converting a data record, a  Converter  also takes in the  output converted  schema of itself, except for the first one that takes in the original input schema. So each converter first converts the input schema and then uses the output schema in the conversion of each data record. The output schema of each converter is fed into both the converter itself for data record conversion and also the next converter. Fig. 4 explains how  Converter  chaining works using three example converters that have  1:1 ,  1:N , and  1:1  mappings for data record conversion, respectively.  \n   \n     \n     Figure 4: How Converter Chaining Works", 
            "title": "Converter"
        }, 
        {
            "location": "/Gobblin-Architecture/#quality-checker", 
            "text": "A  QualityChecker , as the name suggests, is responsible for data quality checking. There are two types of  QualityChecker s: one that checks individual data records and decides if each record should proceed to the next phase in the task flow and the other one that checks the entire task output and decides if data can be committed. We call the two types row-level  QualityChecker s and task-level  QualityChecker s, respectively. A  QualityChecker  can be  MANDATORY  or  OPTIONAL  and will participate in the decision on if quality checking passes if and only if it is  MANDATORY .  OPTIONAL   QualityChecker s are informational only. Similarly to  Converter s, more than one  QualityChecker  can be specified and in this case, quality checking passes if and only if all  MANDATORY   QualityChecker s give a  PASS .", 
            "title": "Quality Checker"
        }, 
        {
            "location": "/Gobblin-Architecture/#fork-operator", 
            "text": "A  ForkOperator  is a type of control operators that allow a task flow to branch into multiple streams, each of which goes to a separately configured sink. This is useful for situations, e.g., that data records need to be written into multiple different storages, or that data records need to be written out to the same storage (say, HDFS) but in different forms for different downstream consumers.", 
            "title": "Fork Operator"
        }, 
        {
            "location": "/Gobblin-Architecture/#data-writer", 
            "text": "A  DataWriter  is responsible for writing data records to the sink it is associated to. Gobblin out-of-the-box provides an  AvroHdfsDataWriter  for writing data in  Avro  format onto HDFS. Users can plugin their own  DataWriter s by specifying a  DataWriterBuilder  class in the job configuration that Gobblin uses to build  DataWriter s.", 
            "title": "Data Writer"
        }, 
        {
            "location": "/Gobblin-Architecture/#data-publisher", 
            "text": "A  DataPublisher  is responsible for publishing extracted data of a Gobblin job. Gobblin ships with a default  DataPublisher  that works with file-based  DataWriter s such as the  AvroHdfsDataWriter  and moves data from the output directory of each task to a final job output directory.", 
            "title": "Data Publisher"
        }, 
        {
            "location": "/Gobblin-Architecture/#gobblin-task-flow", 
            "text": "Fig. 5 below zooms in further and shows the details on how different constructs are connected and composed to form a task flow. The same task flow is employed regardless of the deployment setting and where tasks are running.  \n   \n     \n     Figure 5: Gobblin Task Flow \n     A Gobblin task flow consists of a main branch and a number of forked branches coming out of a  ForkOperator . It is optional to specify a  ForkOperator  in the job configuration. When no  ForkOperator  is specified in the job configuration, a Gobblin task flow uses a  IdentityForkOperator  by default with a single forked branch. The  IdentityForkOperator  simply connects the master branch and the  single  forked branch and passes schema and data records between them. The reason behind this is it avoids special logic from being introduced into the task flow when a  ForkOperator  is indeed specified in the job configuration.  The master branch of a Gobblin task starts with schema extraction from the source. The extracted schema will go through a schema transformation phase if at least one  Converter  class is specified in the job configuration. The next phase is to repeatedly extract data records one at a time. Each extracted data record will also go through a transformation phase if at least one  Converter  class is specified. Each extracted (or converted if applicable) data record is fed into an optional list of row-level  QualityChecker s.  Data records that pass the row-level  QualityChecker s will go through the  ForkOperator  and be further processed in the forked branches. The  ForkOperator  allows users to specify if the input schema or data record should go to a specific forked branch. If the input schema is specified  not  to go into a particular branch, that branch will be ignored. If the input schema or data record is specified to go into  more than one  forked branch, Gobblin assumes that the schema or data record class implements the  Copyable  interface and will attempt to make a copy before passing it to each forked branch. So it is very important to make sure the input schema or data record to the  ForkOperator  is an instance of  Copyable  if it is going into  more than one  branch.  Similarly to the master branch, a forked branch also processes the input schema and each input data record (one at a time) through an optional transformation phase and a row-level quality checking phase. Data records that pass the branch's row-level  QualityChecker s will be written out to a sink by a  DataWriter . Each forked branch has its own sink configuration and a separate  DataWriter .   Upon successful processing of the last record, a forked branch applies an optional list of task-level  QualityChecker s to the data processed by the branch in its entirety. If this quality checking passes, the branch commits the data and exits.   A task flow completes its execution once every forked branches commit and exit. During the execution of a task, a  TaskStateTracker  keeps track of the task's state and a core set of task metrics, e.g., total records extracted, records extracted per second, total bytes extracted, bytes extracted per second, etc.", 
            "title": "Gobblin Task Flow"
        }, 
        {
            "location": "/Gobblin-Architecture/#job-state-management", 
            "text": "Typically a Gobblin job runs periodically on some schedule and each run of the job is extracting data incrementally, i.e., extracting new data or changes to existing data within a specific range since the last run of the job. To make incremental extraction possible, Gobblin must persist the state of the job upon the completion of each run and load the state of the previous run so the next run knows where to start extracting. Gobblin maintains a state store that is responsible for job state persistence. Each run of a Gobblin job reads the state store for the state of the previous run and writes the state of itself to the state store upon its completion. The state of a run of a Gobblin job consists of the job configuration and any properties set at runtime at the job or task level.   Out-of-the-box, Gobblin uses an implementation of the state store that serializes job and task states into Hadoop  SequenceFile s, one per job run. Each job has a separate directory where its job and task state  SequenceFile s are stored. The file system on which the  SequenceFile -based state store resides is configurable.", 
            "title": "Job State Management"
        }, 
        {
            "location": "/Gobblin-Architecture/#handling-of-failures", 
            "text": "As a fault tolerance data ingestion framework, Gobblin employs multiple level of defenses against job and task failures. For job failures, Gobblin keeps track of the number of times a job fails consecutively and optionally sends out an alert email if the number exceeds a defined threshold so the owner of the job can jump in and investigate the failures. For task failures, Gobblin retries failed tasks in a job run up to a configurable maximum number of times. In addition to that, Gobblin also provides an option to enable retries of  WorkUnit s corresponding to failed tasks across job runs. The idea is that if a task fails after all retries fail, the  WorkUnit  based on which the task gets created will be automatically included in the next run of the job if this type of retries is enabled. This type of retries is very useful in handling intermittent failures such as those due to temporary data source outrage.", 
            "title": "Handling of Failures"
        }, 
        {
            "location": "/Gobblin-Architecture/#job-scheduling", 
            "text": "Like mentioned above, a Gobblin job typically runs periodically on some schedule. Gobblin can be integrated with job schedulers such as  Azkaban , Oozie , or Crontab. Out-of-the-box, Gobblin also ships with a built-in job scheduler backed by a  Quartz  scheduler, which is used as the default job scheduler in the standalone deployment. An important feature of Gobblin is that it decouples the job scheduler and the jobs scheduled by the scheduler such that different jobs may run in different deployment settings. This is achieved using the  abstraction  JobLauncher  that has different implementations for different deployment settings. For example, a job scheduler may have 5 jobs scheduled: 2 of them run locally on the same host as the scheduler using the  LocalJobLauncher , whereas the rest 3 run on a Hadoop cluster somewhere using the  MRJobLauncher . Which  JobLauncher  to use can be simply configured using the property  launcher.type .", 
            "title": "Job Scheduling"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nJob Configuration Basics\n\n\nHierarchical Structure of Job Configuration Files\n\n\nPassword Encryption\n\n\nAdding or Changing Job Configuration Files\n\n\nScheduled Jobs\n\n\nOne Time Jobs\n\n\nDisabled Jobs\n\n\n\n\n\n\nJob Configuration Basics\n\n\nA Job configuration file is a text file with extension \n.pull\n or \n.job\n that defines the job properties that can be loaded into a Java \nProperties\n object. Gobblin uses \ncommons-configuration\n to allow variable substitutions in job configuration files. You can find some example Gobblin job configuration files \nhere\n. \n\n\nA Job configuration file typically includes the following properties, in additional to any mandatory configuration properties required by the custom \nGobblin Constructs\n classes. For a complete reference of all configuration properties supported by Gobblin, please refer to \nConfiguration Properties Glossary\n.\n\n\n\n\njob.name\n: job name.\n\n\njob.group\n: the group the job belongs to.\n\n\nsource.class\n: the \nSource\n class the job uses.\n\n\nconverter.classes\n: a comma-separated list of \nConverter\n classes to use in the job. This property is optional.\n\n\nQuality checker related configuration properties: a Gobblin job typically has both row-level and task-level quality checkers specified. Please refer to \nQuality Checker Properties\n for configuration properties related to quality checkers. \n\n\n\n\nHierarchical Structure of Job Configuration Files\n\n\nIt is often the case that a Gobblin instance runs many jobs and manages the job configuration files corresponding to those jobs. The jobs may belong to different job groups and are for different data sources. It is also highly likely that jobs for the same data source shares a lot of common properties. So it is very useful to support the following features:\n\n\n\n\nJob configuration files can be grouped by the job groups they belong to and put into different subdirectories under the root job configuration file directory.\n\n\nCommon job properties shared among multiple jobs can be extracted out to a common properties file that will be applied into the job configurations of all these jobs. \n\n\n\n\nGobblin supports the above features using a hierarchical structure to organize job configuration files under the root job configuration file directory. The basic idea is that there can be arbitrarily deep nesting of subdirectories under the root job configuration file directory. Each directory regardless how deep it is can have a single \n.properties\n file storing common properties that will be included when loading the job configuration files under the same directory or in any subdirectories. Below is an example directory structure.\n\n\nroot_job_config_dir/\n  common.properties\n  foo/\n    foo1.job\n    foo2.job\n    foo.properties\n  bar/\n    bar1.job\n    bar2.job\n    bar.properties\n    baz/\n      baz1.pull\n      baz2.pull\n      baz.properties\n\n\n\n\nIn this example, \ncommon.properties\n will be included when loading \nfoo1.job\n, \nfoo2.job\n, \nbar1.job\n, \nbar2.job\n, \nbaz1.pull\n, and \nbaz2.pull\n. \nfoo.properties\n will be included when loading \nfoo1.job\n and \nfoo2.job\n and properties set here are considered more special and will overwrite the same properties defined in \ncommon.properties\n. Similarly, \nbar.properties\n will be included when loading \nbar1.job\n and \nbar2.job\n, as well as \nbaz1.pull\n and \nbaz2.pull\n. \nbaz.properties\n will be included when loading \nbaz1.pull\n and \nbaz2.pull\n and will overwrite the same properties defined in \nbar.properties\n and \ncommon.properties\n.\n\n\nPassword Encryption\n\n\nTo avoid storing passwords in configuration files in plain text, Gobblin supports encryption of the password configuration properties. All such properties can be encrypted (and decrypted) using a master password. The master password is stored in a file available at runtime. The file can be on a local file system or HDFS and has restricted access.\n\n\nThe URI of the master password file is controlled by the configuration option \nencrypt.key.loc\n . By default, Gobblin will use \norg.jasypt.util.password.BasicPasswordEncryptor\n. If you have installed the \nJCE Unlimited Strength Policy\n, you can set\n\nencrypt.use.strong.encryptor=true\n which will configure Gobblin to use \norg.jasypt.util.password.StrongPasswordEncryptor\n.\n\n\nEncrypted passwords can be generated using the \nCLIPasswordEncryptor\n tool.\n\n\n$ gradle :gobblin-utility:assemble\n$ cd build/gobblin-utility/distributions/\n$ tar -zxf gobblin-utility.tar.gz\n$ bin/gobblin_password_encryptor.sh \n  usage:\n   -f \nmaster password file\n   file that contains the master password used\n                               to encrypt the plain password\n   -h                          print this message\n   -m \nmaster password\n        master password used to encrypt the plain\n                               password\n   -p \nplain password\n         plain password to be encrypted\n   -s                          use strong encryptor\n$ bin/gobblin_password_encryptor.sh -m Hello -p Bye\nENC(AQWoQ2Ybe8KXDXwPOA1Ziw==)\n\n\n\nIf you are extending Gobblin and you want some of your configurations (e.g. the ones containing credentials) to support encryption, you can use \ngobblin.password.PasswordManager.getInstance()\n methods to get an instance of \nPasswordManager\n. You can then use \nPasswordManager.readPassword(String)\n which will transparently decrypt the value if needed, i.e. if it is in the form \nENC(...)\n and a master password is provided.\n\n\nAdding or Changing Job Configuration Files\n\n\nThe Gobblin job scheduler in the standalone deployment monitors any changes to the job configuration file directory and reloads any new or updated job configuration files when detected. This allows adding new job configuration files or making changes to existing ones without bringing down the standalone instance. Currently, the following types of changes are monitored and supported:\n\n\n\n\nAdding a new job configuration file with a \n.job\n or \n.pull\n extension. The new job configuration file is loaded once it is detected. In the example hierarchical structure above, if a new job configuration file \nbaz3.pull\n is added under \nbar/baz\n, it is loaded with properties included from \ncommon.properties\n, \nbar.properties\n, and \nbaz.properties\n in that order.\n\n\nChanging an existing job configuration file with a \n.job\n or \n.pull\n extension. The job configuration file is reloaded once the change is detected. In the example above, if a change is made to \nfoo2.job\n, it is reloaded with properties included from \ncommon.properties\n and \nfoo.properties\n in that order.\n\n\nChanging an existing common properties file with a \n.properties\n extension. All job configuration files that include properties in the common properties file will be reloaded once the change is detected. In the example above, if \nbar.properties\n is updated, job configuration files \nbar1.job\n, \nbar2.job\n, \nbaz1.pull\n, and \nbaz2.pull\n will be reloaded. Properties from \nbar.properties\n will be included when loading \nbar1.job\n and \nbar2.job\n. Properties from \nbar.properties\n and \nbaz.properties\n will be included when loading \nbaz1.pull\n and \nbaz2.pull\n in that order.\n\n\n\n\nNote that this job configuration file change monitoring mechanism uses the \nFileAlterationMonitor\n of Apache's \ncommons-io\n with a custom \nFileAlterationListener\n. Regardless of how close two adjacent file system checks are, there are still chances that more than one files are changed between two file system checks. In case more than one file including at least one common properties file are changed between two adjacent checks, the reloading of affected job configuration files may be intermixed and applied in an order that is not desirable. This is because the order the listener is called on the changes is not controlled by Gobblin, but instead by the monitor itself. So the best practice to use this feature is to avoid making multiple changes together in a short period of time.   \n\n\nScheduled Jobs\n\n\nGobblin ships with a job scheduler backed by a \nQuartz\n scheduler and supports Quartz's \ncron triggers\n. A job that is to be scheduled should have a cron schedule defined using the property \njob.schedule\n. Here is an example cron schedule that triggers every two minutes:\n\n\njob.schedule=0 0/2 * * * ?\n\n\n\n\nOne Time Jobs\n\n\nSome Gobblin jobs may only need to be run once. A job without a cron schedule in the job configuration is considered a run-once job and will not be scheduled but run immediately after being loaded. A job with a cron schedule but also the property \njob.runonce=true\n specified in the job configuration is also treated as a run-once job and will only be run the first time the cron schedule is triggered.\n\n\nDisabled Jobs\n\n\nA Gobblin job can be disabled by setting the property \njob.disabled\n to \ntrue\n. A disabled job will not be loaded nor scheduled to run.", 
            "title": "Job Configuration Files"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/#table-of-contents", 
            "text": "Table of Contents  Job Configuration Basics  Hierarchical Structure of Job Configuration Files  Password Encryption  Adding or Changing Job Configuration Files  Scheduled Jobs  One Time Jobs  Disabled Jobs", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/#job-configuration-basics", 
            "text": "A Job configuration file is a text file with extension  .pull  or  .job  that defines the job properties that can be loaded into a Java  Properties  object. Gobblin uses  commons-configuration  to allow variable substitutions in job configuration files. You can find some example Gobblin job configuration files  here .   A Job configuration file typically includes the following properties, in additional to any mandatory configuration properties required by the custom  Gobblin Constructs  classes. For a complete reference of all configuration properties supported by Gobblin, please refer to  Configuration Properties Glossary .   job.name : job name.  job.group : the group the job belongs to.  source.class : the  Source  class the job uses.  converter.classes : a comma-separated list of  Converter  classes to use in the job. This property is optional.  Quality checker related configuration properties: a Gobblin job typically has both row-level and task-level quality checkers specified. Please refer to  Quality Checker Properties  for configuration properties related to quality checkers.", 
            "title": "Job Configuration Basics"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/#hierarchical-structure-of-job-configuration-files", 
            "text": "It is often the case that a Gobblin instance runs many jobs and manages the job configuration files corresponding to those jobs. The jobs may belong to different job groups and are for different data sources. It is also highly likely that jobs for the same data source shares a lot of common properties. So it is very useful to support the following features:   Job configuration files can be grouped by the job groups they belong to and put into different subdirectories under the root job configuration file directory.  Common job properties shared among multiple jobs can be extracted out to a common properties file that will be applied into the job configurations of all these jobs.    Gobblin supports the above features using a hierarchical structure to organize job configuration files under the root job configuration file directory. The basic idea is that there can be arbitrarily deep nesting of subdirectories under the root job configuration file directory. Each directory regardless how deep it is can have a single  .properties  file storing common properties that will be included when loading the job configuration files under the same directory or in any subdirectories. Below is an example directory structure.  root_job_config_dir/\n  common.properties\n  foo/\n    foo1.job\n    foo2.job\n    foo.properties\n  bar/\n    bar1.job\n    bar2.job\n    bar.properties\n    baz/\n      baz1.pull\n      baz2.pull\n      baz.properties  In this example,  common.properties  will be included when loading  foo1.job ,  foo2.job ,  bar1.job ,  bar2.job ,  baz1.pull , and  baz2.pull .  foo.properties  will be included when loading  foo1.job  and  foo2.job  and properties set here are considered more special and will overwrite the same properties defined in  common.properties . Similarly,  bar.properties  will be included when loading  bar1.job  and  bar2.job , as well as  baz1.pull  and  baz2.pull .  baz.properties  will be included when loading  baz1.pull  and  baz2.pull  and will overwrite the same properties defined in  bar.properties  and  common.properties .", 
            "title": "Hierarchical Structure of Job Configuration Files"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/#password-encryption", 
            "text": "To avoid storing passwords in configuration files in plain text, Gobblin supports encryption of the password configuration properties. All such properties can be encrypted (and decrypted) using a master password. The master password is stored in a file available at runtime. The file can be on a local file system or HDFS and has restricted access.  The URI of the master password file is controlled by the configuration option  encrypt.key.loc  . By default, Gobblin will use  org.jasypt.util.password.BasicPasswordEncryptor . If you have installed the  JCE Unlimited Strength Policy , you can set encrypt.use.strong.encryptor=true  which will configure Gobblin to use  org.jasypt.util.password.StrongPasswordEncryptor .  Encrypted passwords can be generated using the  CLIPasswordEncryptor  tool.  $ gradle :gobblin-utility:assemble\n$ cd build/gobblin-utility/distributions/\n$ tar -zxf gobblin-utility.tar.gz\n$ bin/gobblin_password_encryptor.sh \n  usage:\n   -f  master password file    file that contains the master password used\n                               to encrypt the plain password\n   -h                          print this message\n   -m  master password         master password used to encrypt the plain\n                               password\n   -p  plain password          plain password to be encrypted\n   -s                          use strong encryptor\n$ bin/gobblin_password_encryptor.sh -m Hello -p Bye\nENC(AQWoQ2Ybe8KXDXwPOA1Ziw==)  If you are extending Gobblin and you want some of your configurations (e.g. the ones containing credentials) to support encryption, you can use  gobblin.password.PasswordManager.getInstance()  methods to get an instance of  PasswordManager . You can then use  PasswordManager.readPassword(String)  which will transparently decrypt the value if needed, i.e. if it is in the form  ENC(...)  and a master password is provided.", 
            "title": "Password Encryption"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/#adding-or-changing-job-configuration-files", 
            "text": "The Gobblin job scheduler in the standalone deployment monitors any changes to the job configuration file directory and reloads any new or updated job configuration files when detected. This allows adding new job configuration files or making changes to existing ones without bringing down the standalone instance. Currently, the following types of changes are monitored and supported:   Adding a new job configuration file with a  .job  or  .pull  extension. The new job configuration file is loaded once it is detected. In the example hierarchical structure above, if a new job configuration file  baz3.pull  is added under  bar/baz , it is loaded with properties included from  common.properties ,  bar.properties , and  baz.properties  in that order.  Changing an existing job configuration file with a  .job  or  .pull  extension. The job configuration file is reloaded once the change is detected. In the example above, if a change is made to  foo2.job , it is reloaded with properties included from  common.properties  and  foo.properties  in that order.  Changing an existing common properties file with a  .properties  extension. All job configuration files that include properties in the common properties file will be reloaded once the change is detected. In the example above, if  bar.properties  is updated, job configuration files  bar1.job ,  bar2.job ,  baz1.pull , and  baz2.pull  will be reloaded. Properties from  bar.properties  will be included when loading  bar1.job  and  bar2.job . Properties from  bar.properties  and  baz.properties  will be included when loading  baz1.pull  and  baz2.pull  in that order.   Note that this job configuration file change monitoring mechanism uses the  FileAlterationMonitor  of Apache's  commons-io  with a custom  FileAlterationListener . Regardless of how close two adjacent file system checks are, there are still chances that more than one files are changed between two file system checks. In case more than one file including at least one common properties file are changed between two adjacent checks, the reloading of affected job configuration files may be intermixed and applied in an order that is not desirable. This is because the order the listener is called on the changes is not controlled by Gobblin, but instead by the monitor itself. So the best practice to use this feature is to avoid making multiple changes together in a short period of time.", 
            "title": "Adding or Changing Job Configuration Files"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/#scheduled-jobs", 
            "text": "Gobblin ships with a job scheduler backed by a  Quartz  scheduler and supports Quartz's  cron triggers . A job that is to be scheduled should have a cron schedule defined using the property  job.schedule . Here is an example cron schedule that triggers every two minutes:  job.schedule=0 0/2 * * * ?", 
            "title": "Scheduled Jobs"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/#one-time-jobs", 
            "text": "Some Gobblin jobs may only need to be run once. A job without a cron schedule in the job configuration is considered a run-once job and will not be scheduled but run immediately after being loaded. A job with a cron schedule but also the property  job.runonce=true  specified in the job configuration is also treated as a run-once job and will only be run the first time the cron schedule is triggered.", 
            "title": "One Time Jobs"
        }, 
        {
            "location": "/user-guide/Working-with-Job-Configuration-Files/#disabled-jobs", 
            "text": "A Gobblin job can be disabled by setting the property  job.disabled  to  true . A disabled job will not be loaded nor scheduled to run.", 
            "title": "Disabled Jobs"
        }, 
        {
            "location": "/user-guide/Gobblin-Deployment/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nDeployment Overview \n\n\nStandalone Architecture \n\n\nStandalone Deployment \n\n\nHadoop MapReduce Architecture \n\n\nHadoop MapReduce Deployment \n\n\n\n\n\n\nDeployment Overview \n\n\nOne important feature of Gobblin is that it can be run on different platforms. Currently, Gobblin can run in standalone mode (which runs on a single machine), and on Hadoop MapReduce mode (which runs on a Hadoop cluster, both Hadoop 1.x and Hadoop 2.x are supported). This page summarizes the different deployment modes of Gobblin. It is important to understand the architecture of Gobblin in a specific deployment mode, so this page also describes the architecture of each deployment mode.  \n\n\nGobblin supports Java 6 and up, and can run on either Hadoop 1.x or Hadoop 2.x. By default, Gobblin will build against Hadoop 1.x, in order to build against Hadoop 2.x, run \n./gradlew -PuseHadoop2 clean build\n. More information on how to build Gobblin can be found \nhere\n. All directories/paths referred below are relative to \ngobblin-dist\n.\n\n\nStandalone Architecture \n\n\nThe following diagram illustrates the Gobblin standalone architecture. In the standalone mode, a Gobblin instance runs in a single JVM and tasks run in a thread pool, the size of which is configurable. The standalone mode is good for light-weight data sources such as small databases. The standalone mode is also the default mode for trying and testing Gobblin. \n\n\n\n\n\nIn the standalone deployment, the \nJobScheduler\n runs as a daemon process that schedules and runs jobs using the so-called \nJobLauncher\ns. The \nJobScheduler\n maintains a thread pool in which a new \nJobLauncher\n is started for each job run. Gobblin ships with two types of \nJobLauncher\ns, namely, the \nLocalJobLauncher\n and \nMRJobLauncher\n for launching and running Gobblin jobs on a single machine and on Hadoop MapReduce, respectively. Which \nJobLauncher\n to use can be configured on a per-job basis, which means the \nJobScheduler\n can schedule and run jobs in different deployment modes. This section will focus on the \nLocalJobLauncher\n for launching and running Gobblin jobs on a single machine. The \nMRJobLauncher\n will be covered in a later section on the architecture of Gobblin on Hadoop MapReduce.  \n\n\nEach \nLocalJobLauncher\n starts and manages a few components for executing tasks of a Gobblin job. Specifically, a \nTaskExecutor\n is responsible for executing tasks in a thread pool, whose size is configurable on a per-job basis. A \nLocalTaskStateTracker\n is responsible for keep tracking of the state of running tasks, and particularly updating the task metrics. The \nLocalJobLauncher\n follows the steps below to launch and run a Gobblin job:    \n\n\n\n\nStarting the \nTaskExecutor\n and \nLocalTaskStateTracker\n.\n\n\nCreating an instance of the \nSource\n class specified in the job configuration and getting the list of \nWorkUnit\ns to do.\n\n\nCreating a task for each \nWorkUnit\n in the list, registering the task with the \nLocalTaskStateTracker\n, and submitting the task to the \nTaskExecutor\n to run.\n\n\nWaiting for all the submitted tasks to finish.\n\n\nUpon completion of all the submitted tasks, collecting tasks states and persisting them to the state store, and publishing the extracted data.  \n\n\n\n\nStandalone Deployment \n\n\nGobblin ships with a script \nbin/gobblin-standalone.sh\n for starting and stopping the standalone Gobblin daemon on a single node. Below is the usage of this launch script:\n\n\ngobblin-standalone.sh \nstart | status | restart | stop\n [OPTION]\nWhere:\n  --workdir \njob work dir\n                       Gobblin's base work directory: if not set, taken from ${GOBBLIN_WORK_DIR}\n  --jars \ncomma-separated list of job jars\n      Job jar(s): if not set, lib is examined\n  --conf \ndirectory of job configuration files\n  Directory of job configuration files: if not set, taken from \n  --help                                         Display this help and exit\n\n\n\n\nIn the standalone mode, the \nJobScheduler\n, upon startup, will pick up job configuration files from a user-defined directory and schedule the jobs to run. The job configuration file directory can be specified using the \n--conf\n command-line option of \nbin/gobblin-standalone.sh\n or through an environment variable named \nGOBBLIN_JOB_CONFIG_DIR\n. The \n--conf\n option takes precedence and will take the value of \nGOBBLIN_JOB_CONFIG_DIR\n if not set. Note that this job configuration directory is different from \nconf\n, which stores Gobblin system configuration files, in which deployment-specific configuration properties applicable to all jobs are stored. In comparison, job configuration files store job-specific configuration properties such as the \nSource\n and \nConverter\n classes used.\n\n\nThe \nJobScheduler\n is backed by a \nQuartz\n scheduler and it supports cron-based triggers using the configuration property \njob.schedule\n for defining the cron schedule. Please refer to this \ntutorial\n for more information on how to use and configure a cron-based trigger.  \n\n\nGobblin needs a working directory at runtime, which can be specified using the command-line option \n--workdir\n of \nbin/gobblin-standalone.sh\n or an environment variable named \nGOBBLIN_WORK_DIR\n. The \n--workdir\n option takes precedence and will take the value of \nGOBBLIN_WORK_DIR\n if not set. Once started, Gobblin will create some subdirectories under the root working directory, as follows: \n\n\nGOBBLIN_WORK_DIR\\\n    task-staging\\ # Staging area where data pulled by individual tasks lands\n    task-output\\  # Output area where data pulled by individual tasks lands\n    job-output\\   # Final output area of data pulled by jobs\n    state-store\\  # Persisted job/task state store\n    metrics\\      # Metrics store (in the form of metric log files), one subdirectory per job.\n\n\n\n\nBefore starting the Gobblin standalone daemon, make sure the environment variable \nJAVA_HOME\n is properly set to point to the home directory of the Java Runtime Environment (JRE) of choice. When starting the JVM process of the Gobblin standalone daemon, a default set of jars will be included on the \nclasspath\n. Additional jars needed by your Gobblin jobs can be specified as a comma-separated list using the command-line option \n--jars\n of \nbin/gobblin-standaline.sh\n. If the \n--jar\n option is not set, only the jars under \nlib\n will be included.\n\n\nBelow is a summary of the environment variables that may be set for standalone deployment.\n\n\n\n\nGOBBLIN_JOB_CONFIG_DIR\n: this variable defines the directory where job configuration files are stored. \n\n\nGOBBLIN_WORK_DIR\n: this variable defines the working directory for Gobblin to operate.\n\n\nJAVA_HOME\n: this variable defines the path to the home directory of the Java Runtime Environment (JRE) used to run the daemon process.\n\n\n\n\nTo start the Gobblin standalone daemon, run the following command:\n\n\nbin/gobblin-standalone.sh start [OPTION]\n\n\n\n\nAfter the Gobblin standalone daemon is started, the logs can be found under \nlogs\n. Gobblin uses \nSLF4J\n and the \nslf4j-log4j12\n binding for logging. The \nlog4j\n configuration can be found at \nconf/log4j-standalone.xml\n.\n\n\nBy default, the Gobblin standalone daemon uses the following JVM settings. Change the settings in \nbin/gobblin-standalone.sh\n if necessary for your deployment.\n\n\n-Xmx2g -Xms1g\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC\n-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution\n-XX:+UseCompressedOops\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=\ngobblin log dir\n\n\n\n\n\nTo restart the Gobblin standalone daemon, run the following command:\n\n\nbin/gobblin-standalone.sh restart [OPTION]\n\n\n\n\nTo stop the running Gobblin standalone daemon, run the following command:\n\n\nbin/gobblin-standalone.sh stop\n\n\n\n\nIf there are any additional jars that any jobs depend on, the jars can be added to the classpath using the \n--jars\n option.\n\n\nThe script also supports checking the status of the running daemon process using the \nbin/gobblin-standalone.sh status\n command.\n\n\nHadoop MapReduce Architecture \n\n\nThe digram below shows the architecture of Gobblin on Hadoop MapReduce. As the diagram shows, a Gobblin job runs as a mapper-only MapReduce job that runs tasks of the Gobblin job in the mappers. The basic idea here is to use the mappers purely as \ncontainers\n to run Gobblin tasks. This design also makes it easier to integrate with Yarn. Unlike in the standalone mode, task retries are not handled by Gobblin itself in the Hadoop MapReduce mode. Instead, Gobblin relies on the task retry mechanism of Hadoop MapReduce.  \n\n\n\n\n\nIn this mode, a \nMRJobLauncher\n is used to launch and run a Gobblin job on Hadoop MapReduce, following the steps below:\n\n\n\n\nCreating an instance of the \nSource\n class specified in the job configuration and getting the list of \nWorkUnit\ns to do.\n\n\nSerializing each \nWorkUnit\n into a file on HDFS that will be read later by a mapper.\n\n\nCreating a file that lists the paths of the files storing serialized \nWorkUnit\ns.\n\n\nCreating and configuring a mapper-only Hadoop MapReduce job that takes the file created in step 3 as input.\n\n\nStarting the MapReduce job to run on the cluster of choice and waiting for it to finish.\n\n\nUpon completion of the MapReduce job, collecting tasks states and persisting them to the state store, and publishing the extracted data. \n\n\n\n\nA mapper in a Gobblin MapReduce job runs one or more tasks, depending on the number of \nWorkUnit\ns to do and the (optional) maximum number of mappers specified in the job configuration. If there is no maximum number of mappers specified in the job configuration, each \nWorkUnit\n corresponds to one task that is executed by one mapper and each mapper only runs one task. Otherwise, if a maximum number of mappers is specified and there are more \nWorkUnit\ns than the maximum number of mappers allowed, each mapper may handle more than one \nWorkUnit\n. There is also a special type of \nWorkUnit\ns named \nMultiWorkUnit\n that group multiple \nWorkUnit\ns to be executed together in one batch in a single mapper.\n\n\nA mapper in a Gobblin MapReduce job follows the step below to run tasks assigned to it:\n\n\n\n\nStarting the \nTaskExecutor\n that is responsible for executing tasks in a configurable-size thread pool and the \nMRTaskStateTracker\n that is responsible for keep tracking of the state of running tasks in the mapper. \n\n\nReading the next input record that is the path to the file storing a serialized \nWorkUnit\n.\n\n\nDeserializing the \nWorkUnit\n and adding it to the list of \nWorkUnit\ns to do. If the input is a \nMultiWorkUnit\n, the \nWorkUnit\ns it wraps are all added to the list. Steps 2 and 3 are repeated until all assigned \nWorkUnit\ns are deserialized and added to the list.\n\n\nFor each \nWorkUnit\n on the list of \nWorkUnit\ns to do, creating a task for the \nWorkUnit\n, registering the task with the \nMRTaskStateTracker\n, and submitting the task to the \nTaskExecutor\n to run. Note that the tasks may run in parallel if the \nTaskExecutor\n is \nconfigured\n to have more than one thread in its thread pool.\n\n\nWaiting for all the submitted tasks to finish.\n\n\nUpon completion of all the submitted tasks, writing out the state of each task into a file that will be read by the \nMRJobLauncher\n when collecting task states.\n\n\nGoing back to step 2 and reading the next input record if available.\n\n\n\n\nHadoop MapReduce Deployment \n\n\nGobblin out-of-the-box ships with a script \nbin/gobblin-mapreduce.sh\n for launching a Gobblin job on Hadoop MapReduce. Below is the usage of this launch script:\n\n\nUsage: gobblin-mapreduce.sh [OPTION] --conf \njob configuration file\n\nWhere OPTION can be:\n  --jt \njob tracker / resource manager URL\n      Job submission URL: if not set, taken from ${HADOOP_HOME}/conf\n  --fs \nfile system URL\n                         Target file system: if not set, taken from ${HADOOP_HOME}/conf\n  --jars \ncomma-separated list of job jars\n      Job jar(s): if not set, lib is examined\n  --workdir \njob work dir\n                       Gobblin's base work directory: if not set, taken from ${GOBBLIN_WORK_DIR}\n  --projectversion \nversion\n                     Gobblin version to be used. If set, overrides the distribution build version\n  --logdir \nlog dir\n                             Gobblin's log directory: if not set, taken from ${GOBBLIN_LOG_DIR} if present. Otherwise ./logs is used\n  --help                                         Display this help and exit\n\n\n\n\nIt is assumed that you already have Hadoop (both MapReduce and HDFS) setup and running somewhere. Before launching any Gobblin jobs on Hadoop MapReduce, check the Gobblin system configuration file located at \nconf/gobblin-mapreduce.properties\n for property \nfs.uri\n, which defines the file system URI used. The default value is \nhdfs://localhost:8020\n, which points to the local HDFS on the default port 8020. Change it to the right value depending on your Hadoop/HDFS setup. For example, if you have HDFS setup somwhere on port 9000, then set the property as follows:\n\n\nfs.uri=hdfs://\nnamenode host name\n:9000/\n\n\n\n\nNote that if the option \n--fs\n of \nbin/gobblin-mapreduce.sh\n is set, the value of \n--fs\n should be consistent with the value of \nfs.uri\n. \n\n\nAll job data and persisted job/task states will be written to the specified file system. Before launching any jobs, make sure the environment variable \nHADOOP_BIN_DIR\n is set to point to the \nbin\n directory under the Hadoop installation directory. Similarly to the standalone deployment, the Hadoop MapReduce deployment also needs a working directory, which can be specified using the command-line option \n--workdir\n of \nbin/gobblin-mapreduce.sh\n or the environment variable \nGOBBLIN_WORK_DIR\n. Note that the Gobblin working directory will be created on the file system specified above. Below is a summary of the environment variables that may be set for deployment on Hadoop MapReduce:\n\n\n\n\nGOBBLIN_WORK_DIR\n: this variable defines the working directory for Gobblin to operate.\n\n\nHADOOP_BIN_DIR\n: this variable defines the path to the \nbin\n directory under the Hadoop installation directory.\n\n\n\n\nThis setup will have the minimum set of jars Gobblin needs to run the job added to the Hadoop \nDistributedCache\n for use in the mappers. If a job has additional jars needed for task executions (in the mappers), those jars can also be included by using the \n--jars\n option of \nbin/gobblin-mapreduce.sh\n or the following job configuration property in the job configuration file:\n\n\njob.jars=\ncomma-separated list of jars the job depends on\n\n\n\n\n\nThe \n--projectversion\n controls which version of the Gobblin jars to look for. Typically, this value is dynamically set during the build process. Users should use the \nbin/gobblin-mapreduce.sh\n script that is copied into the \ngobblin-distribution-[project-version].tar.gz\n file. This version of the script has the project version already set, in which case users do not need to specify the \n--projectversion\n parameter. If users want to use the \ngobblin/bin/gobblin-mapreduce.sh\n script they have to specify this parameter.\n\n\nThe \n--logdir\n parameter controls the directory where log files are written to. If not set log files are written under a the \n./logs\n directory.", 
            "title": "Deployment"
        }, 
        {
            "location": "/user-guide/Gobblin-Deployment/#table-of-contents", 
            "text": "Table of Contents  Deployment Overview   Standalone Architecture   Standalone Deployment   Hadoop MapReduce Architecture   Hadoop MapReduce Deployment", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Gobblin-Deployment/#deployment-overview", 
            "text": "One important feature of Gobblin is that it can be run on different platforms. Currently, Gobblin can run in standalone mode (which runs on a single machine), and on Hadoop MapReduce mode (which runs on a Hadoop cluster, both Hadoop 1.x and Hadoop 2.x are supported). This page summarizes the different deployment modes of Gobblin. It is important to understand the architecture of Gobblin in a specific deployment mode, so this page also describes the architecture of each deployment mode.    Gobblin supports Java 6 and up, and can run on either Hadoop 1.x or Hadoop 2.x. By default, Gobblin will build against Hadoop 1.x, in order to build against Hadoop 2.x, run  ./gradlew -PuseHadoop2 clean build . More information on how to build Gobblin can be found  here . All directories/paths referred below are relative to  gobblin-dist .", 
            "title": "Deployment Overview "
        }, 
        {
            "location": "/user-guide/Gobblin-Deployment/#standalone-architecture", 
            "text": "The following diagram illustrates the Gobblin standalone architecture. In the standalone mode, a Gobblin instance runs in a single JVM and tasks run in a thread pool, the size of which is configurable. The standalone mode is good for light-weight data sources such as small databases. The standalone mode is also the default mode for trying and testing Gobblin.    In the standalone deployment, the  JobScheduler  runs as a daemon process that schedules and runs jobs using the so-called  JobLauncher s. The  JobScheduler  maintains a thread pool in which a new  JobLauncher  is started for each job run. Gobblin ships with two types of  JobLauncher s, namely, the  LocalJobLauncher  and  MRJobLauncher  for launching and running Gobblin jobs on a single machine and on Hadoop MapReduce, respectively. Which  JobLauncher  to use can be configured on a per-job basis, which means the  JobScheduler  can schedule and run jobs in different deployment modes. This section will focus on the  LocalJobLauncher  for launching and running Gobblin jobs on a single machine. The  MRJobLauncher  will be covered in a later section on the architecture of Gobblin on Hadoop MapReduce.    Each  LocalJobLauncher  starts and manages a few components for executing tasks of a Gobblin job. Specifically, a  TaskExecutor  is responsible for executing tasks in a thread pool, whose size is configurable on a per-job basis. A  LocalTaskStateTracker  is responsible for keep tracking of the state of running tasks, and particularly updating the task metrics. The  LocalJobLauncher  follows the steps below to launch and run a Gobblin job:       Starting the  TaskExecutor  and  LocalTaskStateTracker .  Creating an instance of the  Source  class specified in the job configuration and getting the list of  WorkUnit s to do.  Creating a task for each  WorkUnit  in the list, registering the task with the  LocalTaskStateTracker , and submitting the task to the  TaskExecutor  to run.  Waiting for all the submitted tasks to finish.  Upon completion of all the submitted tasks, collecting tasks states and persisting them to the state store, and publishing the extracted data.", 
            "title": "Standalone Architecture "
        }, 
        {
            "location": "/user-guide/Gobblin-Deployment/#standalone-deployment", 
            "text": "Gobblin ships with a script  bin/gobblin-standalone.sh  for starting and stopping the standalone Gobblin daemon on a single node. Below is the usage of this launch script:  gobblin-standalone.sh  start | status | restart | stop  [OPTION]\nWhere:\n  --workdir  job work dir                        Gobblin's base work directory: if not set, taken from ${GOBBLIN_WORK_DIR}\n  --jars  comma-separated list of job jars       Job jar(s): if not set, lib is examined\n  --conf  directory of job configuration files   Directory of job configuration files: if not set, taken from \n  --help                                         Display this help and exit  In the standalone mode, the  JobScheduler , upon startup, will pick up job configuration files from a user-defined directory and schedule the jobs to run. The job configuration file directory can be specified using the  --conf  command-line option of  bin/gobblin-standalone.sh  or through an environment variable named  GOBBLIN_JOB_CONFIG_DIR . The  --conf  option takes precedence and will take the value of  GOBBLIN_JOB_CONFIG_DIR  if not set. Note that this job configuration directory is different from  conf , which stores Gobblin system configuration files, in which deployment-specific configuration properties applicable to all jobs are stored. In comparison, job configuration files store job-specific configuration properties such as the  Source  and  Converter  classes used.  The  JobScheduler  is backed by a  Quartz  scheduler and it supports cron-based triggers using the configuration property  job.schedule  for defining the cron schedule. Please refer to this  tutorial  for more information on how to use and configure a cron-based trigger.    Gobblin needs a working directory at runtime, which can be specified using the command-line option  --workdir  of  bin/gobblin-standalone.sh  or an environment variable named  GOBBLIN_WORK_DIR . The  --workdir  option takes precedence and will take the value of  GOBBLIN_WORK_DIR  if not set. Once started, Gobblin will create some subdirectories under the root working directory, as follows:   GOBBLIN_WORK_DIR\\\n    task-staging\\ # Staging area where data pulled by individual tasks lands\n    task-output\\  # Output area where data pulled by individual tasks lands\n    job-output\\   # Final output area of data pulled by jobs\n    state-store\\  # Persisted job/task state store\n    metrics\\      # Metrics store (in the form of metric log files), one subdirectory per job.  Before starting the Gobblin standalone daemon, make sure the environment variable  JAVA_HOME  is properly set to point to the home directory of the Java Runtime Environment (JRE) of choice. When starting the JVM process of the Gobblin standalone daemon, a default set of jars will be included on the  classpath . Additional jars needed by your Gobblin jobs can be specified as a comma-separated list using the command-line option  --jars  of  bin/gobblin-standaline.sh . If the  --jar  option is not set, only the jars under  lib  will be included.  Below is a summary of the environment variables that may be set for standalone deployment.   GOBBLIN_JOB_CONFIG_DIR : this variable defines the directory where job configuration files are stored.   GOBBLIN_WORK_DIR : this variable defines the working directory for Gobblin to operate.  JAVA_HOME : this variable defines the path to the home directory of the Java Runtime Environment (JRE) used to run the daemon process.   To start the Gobblin standalone daemon, run the following command:  bin/gobblin-standalone.sh start [OPTION]  After the Gobblin standalone daemon is started, the logs can be found under  logs . Gobblin uses  SLF4J  and the  slf4j-log4j12  binding for logging. The  log4j  configuration can be found at  conf/log4j-standalone.xml .  By default, the Gobblin standalone daemon uses the following JVM settings. Change the settings in  bin/gobblin-standalone.sh  if necessary for your deployment.  -Xmx2g -Xms1g\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC\n-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution\n-XX:+UseCompressedOops\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath= gobblin log dir   To restart the Gobblin standalone daemon, run the following command:  bin/gobblin-standalone.sh restart [OPTION]  To stop the running Gobblin standalone daemon, run the following command:  bin/gobblin-standalone.sh stop  If there are any additional jars that any jobs depend on, the jars can be added to the classpath using the  --jars  option.  The script also supports checking the status of the running daemon process using the  bin/gobblin-standalone.sh status  command.", 
            "title": "Standalone Deployment "
        }, 
        {
            "location": "/user-guide/Gobblin-Deployment/#hadoop-mapreduce-architecture", 
            "text": "The digram below shows the architecture of Gobblin on Hadoop MapReduce. As the diagram shows, a Gobblin job runs as a mapper-only MapReduce job that runs tasks of the Gobblin job in the mappers. The basic idea here is to use the mappers purely as  containers  to run Gobblin tasks. This design also makes it easier to integrate with Yarn. Unlike in the standalone mode, task retries are not handled by Gobblin itself in the Hadoop MapReduce mode. Instead, Gobblin relies on the task retry mechanism of Hadoop MapReduce.     In this mode, a  MRJobLauncher  is used to launch and run a Gobblin job on Hadoop MapReduce, following the steps below:   Creating an instance of the  Source  class specified in the job configuration and getting the list of  WorkUnit s to do.  Serializing each  WorkUnit  into a file on HDFS that will be read later by a mapper.  Creating a file that lists the paths of the files storing serialized  WorkUnit s.  Creating and configuring a mapper-only Hadoop MapReduce job that takes the file created in step 3 as input.  Starting the MapReduce job to run on the cluster of choice and waiting for it to finish.  Upon completion of the MapReduce job, collecting tasks states and persisting them to the state store, and publishing the extracted data.    A mapper in a Gobblin MapReduce job runs one or more tasks, depending on the number of  WorkUnit s to do and the (optional) maximum number of mappers specified in the job configuration. If there is no maximum number of mappers specified in the job configuration, each  WorkUnit  corresponds to one task that is executed by one mapper and each mapper only runs one task. Otherwise, if a maximum number of mappers is specified and there are more  WorkUnit s than the maximum number of mappers allowed, each mapper may handle more than one  WorkUnit . There is also a special type of  WorkUnit s named  MultiWorkUnit  that group multiple  WorkUnit s to be executed together in one batch in a single mapper.  A mapper in a Gobblin MapReduce job follows the step below to run tasks assigned to it:   Starting the  TaskExecutor  that is responsible for executing tasks in a configurable-size thread pool and the  MRTaskStateTracker  that is responsible for keep tracking of the state of running tasks in the mapper.   Reading the next input record that is the path to the file storing a serialized  WorkUnit .  Deserializing the  WorkUnit  and adding it to the list of  WorkUnit s to do. If the input is a  MultiWorkUnit , the  WorkUnit s it wraps are all added to the list. Steps 2 and 3 are repeated until all assigned  WorkUnit s are deserialized and added to the list.  For each  WorkUnit  on the list of  WorkUnit s to do, creating a task for the  WorkUnit , registering the task with the  MRTaskStateTracker , and submitting the task to the  TaskExecutor  to run. Note that the tasks may run in parallel if the  TaskExecutor  is  configured  to have more than one thread in its thread pool.  Waiting for all the submitted tasks to finish.  Upon completion of all the submitted tasks, writing out the state of each task into a file that will be read by the  MRJobLauncher  when collecting task states.  Going back to step 2 and reading the next input record if available.", 
            "title": "Hadoop MapReduce Architecture "
        }, 
        {
            "location": "/user-guide/Gobblin-Deployment/#hadoop-mapreduce-deployment", 
            "text": "Gobblin out-of-the-box ships with a script  bin/gobblin-mapreduce.sh  for launching a Gobblin job on Hadoop MapReduce. Below is the usage of this launch script:  Usage: gobblin-mapreduce.sh [OPTION] --conf  job configuration file \nWhere OPTION can be:\n  --jt  job tracker / resource manager URL       Job submission URL: if not set, taken from ${HADOOP_HOME}/conf\n  --fs  file system URL                          Target file system: if not set, taken from ${HADOOP_HOME}/conf\n  --jars  comma-separated list of job jars       Job jar(s): if not set, lib is examined\n  --workdir  job work dir                        Gobblin's base work directory: if not set, taken from ${GOBBLIN_WORK_DIR}\n  --projectversion  version                      Gobblin version to be used. If set, overrides the distribution build version\n  --logdir  log dir                              Gobblin's log directory: if not set, taken from ${GOBBLIN_LOG_DIR} if present. Otherwise ./logs is used\n  --help                                         Display this help and exit  It is assumed that you already have Hadoop (both MapReduce and HDFS) setup and running somewhere. Before launching any Gobblin jobs on Hadoop MapReduce, check the Gobblin system configuration file located at  conf/gobblin-mapreduce.properties  for property  fs.uri , which defines the file system URI used. The default value is  hdfs://localhost:8020 , which points to the local HDFS on the default port 8020. Change it to the right value depending on your Hadoop/HDFS setup. For example, if you have HDFS setup somwhere on port 9000, then set the property as follows:  fs.uri=hdfs:// namenode host name :9000/  Note that if the option  --fs  of  bin/gobblin-mapreduce.sh  is set, the value of  --fs  should be consistent with the value of  fs.uri .   All job data and persisted job/task states will be written to the specified file system. Before launching any jobs, make sure the environment variable  HADOOP_BIN_DIR  is set to point to the  bin  directory under the Hadoop installation directory. Similarly to the standalone deployment, the Hadoop MapReduce deployment also needs a working directory, which can be specified using the command-line option  --workdir  of  bin/gobblin-mapreduce.sh  or the environment variable  GOBBLIN_WORK_DIR . Note that the Gobblin working directory will be created on the file system specified above. Below is a summary of the environment variables that may be set for deployment on Hadoop MapReduce:   GOBBLIN_WORK_DIR : this variable defines the working directory for Gobblin to operate.  HADOOP_BIN_DIR : this variable defines the path to the  bin  directory under the Hadoop installation directory.   This setup will have the minimum set of jars Gobblin needs to run the job added to the Hadoop  DistributedCache  for use in the mappers. If a job has additional jars needed for task executions (in the mappers), those jars can also be included by using the  --jars  option of  bin/gobblin-mapreduce.sh  or the following job configuration property in the job configuration file:  job.jars= comma-separated list of jars the job depends on   The  --projectversion  controls which version of the Gobblin jars to look for. Typically, this value is dynamically set during the build process. Users should use the  bin/gobblin-mapreduce.sh  script that is copied into the  gobblin-distribution-[project-version].tar.gz  file. This version of the script has the project version already set, in which case users do not need to specify the  --projectversion  parameter. If users want to use the  gobblin/bin/gobblin-mapreduce.sh  script they have to specify this parameter.  The  --logdir  parameter controls the directory where log files are written to. If not set log files are written under a the  ./logs  directory.", 
            "title": "Hadoop MapReduce Deployment "
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nArchitecture\n\n\nOverview\n\n\nThe Role of Apache Helix\n\n\nGobblin Yarn Application Launcher\n\n\nYarnAppSecurityManager\n\n\nLogCopier\n\n\n\n\n\n\nGobblin ApplicationMaster\n\n\nYarnService\n\n\nGobblinHelixJobScheduler\n\n\nLogCopier\n\n\nYarnContainerSecurityManager\n\n\n\n\n\n\nGobblin WorkUnitRunner\n\n\nTaskExecutor\n\n\nGobblinHelixTaskStateTracker\n\n\nLogCopier\n\n\nYarnContainerSecurityManager\n\n\n\n\n\n\nFailure Handling\n\n\nApplicationMaster Failure Handling\n\n\nContainer Failure Handling\n\n\nHandling Failures to get ApplicationReport\n\n\n\n\n\n\n\n\n\n\nLog Aggregation\n\n\nSecurity and Delegation Token Management\n\n\nConfiguration\n\n\nConfiguration Properties\n\n\nConfiguration System\n\n\n\n\n\n\nDeployment\n\n\nDeployment on a Unsecured Yarn Cluster\n\n\nDeployment on a Secured Yarn Cluster\n\n\nSupporting Existing Gobblin Jobs\n\n\n\n\n\n\nMonitoring\n\n\n\n\n\n\nIntroduction\n\n\nGobblin currently is capable of running in the standalone mode on a single machine or in the MapReduce (MR) mode as a MR job on a Hadoop cluster. A Gobblin job is typically running on a schedule through a scheduler, e.g., the built-in \nJobScheduler\n, Azkaban, or Oozie, and each job run ingests new data or data updated since the last run. So this is essentially a batch model for data ingestion and how soon new data becomes available on Hadoop depends on the schedule of the job. \n\n\nOn another aspect, for high data volume data sources such as Kafka, Gobblin typically runs in the MR mode with a considerable number of tasks running in the mappers of a MR job. This helps Gobblin to scale out for data sources with large volumes of data. The MR mode, however, suffers from problems such as large overhead mostly due to the overhead of submitting and launching a MR job and poor cluster resource usage. The MR mode is also fundamentally not appropriate for real-time data ingestion given its batch nature. These deficiencies are summarized in more details below:\n\n\n\n\nIn the MR mode, every Gobblin job run starts a new MR job, which costs a considerable amount of time to allocate and start the containers for running the mapper/reducer tasks. This cost can be totally eliminated if the containers are already up and running.\n\n\nEach Gobblin job running in the MR mode requests a new set of containers and releases them upon job completion. So it's impossible for two jobs to share the containers even though the containers are perfectly capable of running tasks of both jobs.\n\n\nIn the MR mode, All \nWorkUnit\ns are pre-assigned to the mappers before launching the MR job. The assignment is fixed by evenly distributing the \nWorkUnit\ns to the mappers so each mapper gets a fair share of the work in terms of the \nnumber of \nWorkUnits\n. However, an evenly distributed number of \nWorkUnit\ns per mapper does not always guarantee a fair share of the work in terms of the volume of data to pull. This, combined with the fact that the mappers that finish earlier cannot \"steal\" \nWorkUnit\ns assigned to other mappers, means the responsibility of load balancing is on the \nSource\n implementations, which is not trivial to do, and is virtually impossible in heterogeneous Hadoop clusters where different nodes have different capacity. This also means the duration of a job is determined by the slowest mapper.\n\n\nA MR job can only hold its containers for a limited of time, beyond which the job may get killed. Real-time data ingestion, however, requires the ingestion tasks to be running all the time or alternatively dividing a continuous data stream into well-defined mini-batches (as in Spark Streaming) that can be promptly executed once created. Both require long-running containers, which are not supported in the MR mode. \n\n\n\n\nThose deficiencies motivated the work on making Gobblin run on Yarn as a native Yarn application. Running Gobblin as a native Yarn application allows much more control over container provisioning and lifecycle management so it's possible to keep the containers running continuously. It also makes it possible to dynamically change the number of containers at runtime depending on the load to further improve the resource efficiency, something that's impossible in the MR mode.         \n\n\nThis wiki page documents the design and architecture of the native Gobblin Yarn application and some implementation details. It also covers the configuration system and properties for the application, as well as deployment settings on both unsecured and secured Yarn clusters. \n\n\nArchitecture\n\n\nOverview\n\n\nThe architecture of Gobblin on Yarn is illustrated in the following diagram. In addition to Yarn, Gobblin on Yarn also leverages \nApache Helix\n, whose role is discussed in \nThe Role of Apache Helix\n. A Gobblin Yarn application consists of three components: the Yarn Application Launcher, the Yarn ApplicationMaster (serving as the Helix \ncontroller\n), and the Yarn WorkUnitRunner (serving as the Helix \nparticipant\n). The following sections describe each component in details.\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\nThe Role of Apache Helix\n\n\nApache Helix\n is mainly used for managing the cluster of containers and running the \nWorkUnit\ns through its \nDistributed Task Execution Framework\n. \n\n\nThe assignment of tasks to available containers (or participants in Helix's term) is handled by Helix through a finite state model named the \nTaskStateModel\n. Using this \nTaskStateModel\n, Helix is also able to do task rebalancing in case new containers get added or some existing containers die. Clients can also choose to force a task rebalancing if some tasks take much longer time than the others. \n\n\nHelix also supports a way of doing messaging between different components of a cluster, e.g., between the controller to the participants, or between the client and the controller. The Gobblin Yarn application uses this messaging mechanism to implement graceful shutdown initiated by the client as well as delegation token renew notifications from the client to the ApplicationMaster and the WorkUnitRunner containers.\n\n\nHeiix relies on ZooKeeper for its operations, and particularly for maintaining the state of the cluster and the resources (tasks in this case). Both the Helix controller and participants connect to ZooKeeper during their entire lifetime. The ApplicationMaster serves as the Helix controller and the worker containers serve as the Helix participants, respectively, as discussed in details below.  \n\n\nGobblin Yarn Application Launcher\n\n\nThe Gobblin Yarn Application Launcher (implemented by class \nGobblinYarnAppLauncher\n) is the client/driver of a Gobblin Yarn application. The first thing the \nGobblinYarnAppLauncher\n does when it starts is to register itself with Helix as a \nspectator\n and creates a new Helix cluster with name specified through the configuration property \ngobblin.yarn.helix.cluster.name\n, if no cluster with the name exists. \n\n\nThe \nGobblinYarnAppLauncher\n then sets up the Gobblin Yarn application and submits it to run on Yarn. Once the Yarn application successfully starts running, it starts an application state monitor that periodically checks the state of the Gobblin Yarn application. If the state is one of the exit states (\nFINISHED\n, \nFAILED\n, or \nKILLED\n), the \nGobblinYarnAppLauncher\n shuts down itself. \n\n\nUpon successfully submitting the application to run on Yarn, the \nGobblinYarnAppLauncher\n also starts a \nServiceManager\n that manages the following services that auxiliate the running of the application:\n\n\nYarnAppSecurityManager\n\n\nThe \nYarnAppSecurityManager\n works with the \nYarnContainerSecurityManager\n running in the ApplicationMaster and the WorkUnitRunner for a complete solution for security and delegation token management. The \nYarnAppSecurityManager\n is responsible for periodically logging in through a Kerberos keytab and getting the delegation token refreshed regularly after each login. Each time the delegation token is refreshed, the \nYarnContainerSecurityManager\n writes the new token to a file on HDFS and sends a message to the ApplicationMaster and each WorkUnitRunner, notifying them the refresh of the delegation token. Checkout \nYarnContainerSecurityManager\n on how the other side of this system works.\n\n\nLogCopier\n\n\nThe service \nLogCopier\n in \nGobblinYarnAppLauncher\n streams the ApplicationMaster and WorkUnitRunner logs in near real-time from the central location on HDFS where the logs are streamed to from the ApplicationMaster and WorkUnitRunner containers, to the local directory specified through the configuration property \ngobblin.yarn.logs.sink.root.dir\n on the machine where the \nGobblinYarnAppLauncher\n runs. More details on this can be found in \nLog Aggregation\n.\n\n\nGobblin ApplicationMaster\n\n\nThe ApplicationMaster process runs the \nGobblinApplicationMaster\n, which uses a \nServiceManager\n to manage the services supporting the operation of the ApplicationMaster process. The services running in \nGobblinApplicationMaster\n will be discussed later. When it starts, the first thing \nGobblinApplicationMaster\n does is to connect to ZooKeeper and register itself as a Helix \ncontroller\n. It then starts the \nServiceManager\n, which in turn starts the services it manages, as described below. \n\n\nYarnService\n\n\nThe service \nYarnService\n handles all Yarn-related task including the following:\n\n\n\n\nRegistering and un-registering the ApplicationMaster with the Yarn ResourceManager.\n\n\nRequesting the initial set of containers from the Yarn ResourceManager.\n\n\nHandling any container changes at runtime, e.g., adding more containers or shutting down containers no longer needed. This also includes stopping running containers when the application is asked to stop.\n\n\n\n\nThis design makes it switch to a different resource manager, e.g., Mesos, by replacing the service \nYarnService\n with something else specific to the resource manager, e.g., \nMesosService\n.\n\n\nGobblinHelixJobScheduler\n\n\nGobblinApplicationMaster\n runs the \nGobblinHelixJobScheduler\n that schedules jobs to run through the Helix \nDistributed Task Execution Framework\n. For each Gobblin job run, the \nGobblinHelixJobScheduler\n starts a \nGobblinHelixJobLauncher\n that wraps the Gobblin job into a \nGobblinHelixJob\n and each Gobblin \nTask\n into a \nGobblinHelixTask\n, which implements the Helix's \nTask\n interface so Helix knows how to execute it. The \nGobblinHelixJobLauncher\n then submits the job to a Helix job queue named after the Gobblin job name, from which the Helix Distributed Task Execution Framework picks up the job and runs its tasks through the live participants (available containers).\n\n\nLike the \nLocalJobLauncher\n and \nMRJobLauncher\n, the \nGobblinHelixJobLauncher\n handles output data commit and job state persistence.   \n\n\nLogCopier\n\n\nThe service \nLogCopier\n in \nGobblinApplicationMaster\n streams the ApplicationMaster logs in near real-time from the machine running the ApplicationMaster container to a central location on HDFS so the logs can be accessed at runtime. More details on this can be found in \nLog Aggregation\n.\n\n\nYarnContainerSecurityManager\n\n\nThe \nYarnContainerSecurityManager\n runs in both the ApplicationMaster and the WorkUnitRunner. When it starts, it registers a message handler with the \nHelixManager\n for handling messages on refreshes of the delegation token. Once such a message is received, the \nYarnContainerSecurityManager\n gets the path to the token file on HDFS from the message, and updated the the current login user with the new token read from the file.\n\n\nGobblin WorkUnitRunner\n\n\nThe WorkUnitRunner process runs the \nGobblinWorkUnitRunner\n, which uses a \nServiceManager\n to manage the services supporting the operation of the WorkUnitRunner process. The services running in \nGobblinWorkUnitRunner\n will be discussed later. When it starts, the first thing \nGobblinWorkUnitRunner\n does is to connect to ZooKeeper and register itself as a Helix \nparticipant\n. It then starts the \nServiceManager\n, which in turn starts the services it manages, as discussed below. \n\n\nTaskExecutor\n\n\nThe \nTaskExecutor\n remains the same as in the standalone and MR modes, and is purely responsible for running tasks assigned to a WorkUnitRunner. \n\n\nGobblinHelixTaskStateTracker\n\n\nThe \nGobblinHelixTaskStateTracker\n has a similar responsibility as the \nLocalTaskStateTracker\n and \nMRTaskStateTracker\n: keeping track of the state of running tasks including operational metrics, e.g., total records pulled, records pulled per second, total bytes pulled, bytes pulled per second, etc.\n\n\nLogCopier\n\n\nThe service \nLogCopier\n in \nGobblinWorkUnitRunner\n streams the WorkUnitRunner logs in near real-time from the machine running the WorkUnitRunner container to a central location on HDFS so the logs can be accessed at runtime. More details on this can be found in \nLog Aggregation\n.\n\n\nYarnContainerSecurityManager\n\n\nThe \nYarnContainerSecurityManager\n in \nGobblinWorkUnitRunner\n works in the same way as it in \nGobblinApplicationMaster\n. \n\n\nFailure Handling\n\n\nApplicationMaster Failure Handling\n\n\nUnder normal operation, the Gobblin ApplicationMaster stays alive unless being asked to stop through a message sent from the launcher (the \nGobblinYarnAppLauncher\n) as part of the orderly shutdown process. It may, however, fail or get killed by the Yarn ResourceManager for various reasons. For example, the container running the ApplicationMaster may fail and exit due to node failures, or get killed because of using more memory than claimed. When a shutdown of the ApplicationMaster is triggered (e.g., when the shutdown hook is triggered) for any reason, it does so gracefully, i.e., it attempts to stop every services it manages, stop all the running containers, and unregister itself with the ResourceManager. Shutting down the ApplicationMaster shuts down the Yarn application and the application launcher will eventually know that the application completes through a periodic check on the application status. \n\n\nContainer Failure Handling\n\n\nUnder normal operation, a Gobblin Yarn container stays alive unless being released and stopped by the Gobblin ApplicationMaster, and in this case the exit status of the container will be zero. However, a container may exit unexpectedly due to various reasons. For example, a container may fail and exit due to node failures, or be killed because of using more memory than claimed. In this case when a container exits abnormally with a non-zero exit code, Gobblin Yarn tries to restart the Helix instance running in the container by requesting a new Yarn container as a replacement to run the instance. The maximum number of retries can be configured through the key \ngobblin.yarn.helix.instance.max.retries\n.\n\n\nWhen requesting a new container to replace the one that completes and exits abnormally, the application has a choice of specifying the same host that runs the completed container as the preferred host, depending on the boolean value of configuration key \ngobblin.yarn.container.affinity.enabled\n. Note that for certain exit codes that indicate something wrong with the host, the value of \ngobblin.yarn.container.affinity.enabled\n is ignored and no preferred host gets specified, leaving Yarn to figure out a good candidate node for the new container.     \n\n\nHandling Failures to get ApplicationReport\n\n\nAs mentioned above, once the Gobblin Yarn application successfully starts running, the \nGobblinYarnAppLauncher\n starts an application state monitor that periodically checks the state of the Yarn application by getting an \nApplicationReport\n. It may fail to do so and throw an exception, however, if the Yarn client is having some problem connecting and communicating with the Yarn cluster. For example, if the Yarn cluster is down for maintenance, the Yarn client will not be able to get an \nApplicationReport\n. The \nGobblinYarnAppLauncher\n keeps track of the number of consecutive failures to get an \nApplicationReport\n and initiates a shutdown if this number exceeds the threshold as specified through the configuration property \ngobblin.yarn.max.get.app.report.failures\n. The shutdown will trigger an email notification if the configuration property \ngobblin.yarn.email.notification.on.shutdown\n is set to \ntrue\n.\n\n\nLog Aggregation\n\n\nYarn provides both a Web UI and a command-line tool to access the logs of an application, and also does log aggregation so the logs of all the containers become available on the client side upon requested. However, there are a few limitations that make it hard to access the logs of an application at runtime:\n\n\n\n\nThe command-line utility for downloading the aggregated logs will only be able to do so after the application finishes, making it useless for getting access to the logs at the application runtime.  \n\n\nThe Web UI does allow logs to be viewed at runtime, but only when the user that access the UI is the same as the user that launches the application. On a Yarn cluster where security is enabled, the user launching the Gobblin Yarn application is typically a user of some headless account.\n\n\n\n\nBecause Gobblin runs on Yarn as a long-running native Yarn application, getting access to the logs at runtime is critical to know what's going on in the application and to detect any issues in the application as early as possible. Unfortunately we cannot use the log facility provided by Yarn here due to the above limitations. Alternatively, Gobblin on Yarn has its own mechanism for doing log aggregation and providing access to the logs at runtime, described as follows.\n\n\nBoth the Gobblin ApplicationMaster and WorkUnitRunner run a \nLogCopier\n that periodically copies new entries of both \nstdout\n and \nstderr\n logs of the corresponding processes from the containers to a central location on HDFS under the directory \n${gobblin.yarn.work.dir}/_applogs\n in the subdirectories named after the container IDs, one per container. The names of the log files on HDFS combine the container IDs and the original log file names so it's easy to tell which container generates which log file. More specifically, the log files produced by the ApplicationMaster are named \ncontainer id\n.GobblinApplicationMaster.{stdout,stderr}\n, and the log files produced by the WorkUnitRunner are named \ncontainer id\n.GobblinWorkUnitRunner.{stdout,stderr}\n.\n\n\nThe Gobblin YarnApplicationLauncher also runs a \nLogCopier\n that periodically copies new log entries from log files under \n${gobblin.yarn.work.dir}/_applogs\n on HDFS to the local filesystem under the directory configured by the property \ngobblin.yarn.logs.sink.root.dir\n. By default, the \nLogCopier\n checks for new log entries every 60 seconds and will keep reading new log entries until it reaches the end of the log file. This setup enables the Gobblin Yarn application to stream container process logs near real-time all the way to the client/driver. \n\n\nSecurity and Delegation Token Management\n\n\nOn a Yarn cluster with security enabled (e.g., Kerberos authentication is required to access HDFS), security and delegation token management is necessary to allow Gobblin run as a long-running Yarn application. Specifically, Gobblin running on a secured Yarn cluster needs to get its delegation token for accessing HDFS renewed periodically, which also requires periodic keytab re-logins because a delegation token can only be renewed up to a limited number of times in one login.\n\n\nThe Gobblin Yarn application supports Kerberos-based authentication and login through a keytab file. The \nYarnAppSecurityManager\n running in the Yarn Application Launcher and the \nYarnContainerSecurityManager\n running in the ApplicationMaster and WorkUnitRunner work together to get every Yarn containers updated whenever the delegation token gets updated on the client side by the \nYarnAppSecurityManager\n. More specifically, the \nYarnAppSecurityManager\n periodically logins through the keytab and gets the delegation token refreshed regularly after each successful login. Every time the \nYarnAppSecurityManager\n refreshes the delegation token, the \nYarnContainerSecurityManager\n writes the new token to a file on HDFS and sends a \nTOKEN_FILE_UPDATED\n message to the ApplicationMaster and each WorkUnitRunner, notifying them the refresh of the delegation token. Upon receiving such a message, the \nYarnContainerSecurityManager\n running in the ApplicationMaster or WorkUnitRunner gets the path to the token file on HDFS from the message, and updated the the current login user with the new token read from the file.\n\n\nBoth the interval between two Kerberos keytab logins and the interval between two delegation token refreshes are configurable, through the configuration properties \ngobblin.yarn.login.interval.minutes\n and \ngobblin.yarn.token.renew.interval.minutes\n, respectively.    \n\n\nConfiguration\n\n\nConfiguration Properties\n\n\nIn additional to the common Gobblin configuration properties, documented in \nConfiguration Properties Glossary\n, Gobblin on Yarn uses the following configuration properties. \n\n\n\n\n\n\n\n\nProperty\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngobblin.yarn.app.name\n\n\nGobblinYarn\n\n\nThe Gobblin Yarn appliation name.\n\n\n\n\n\n\ngobblin.yarn.app.queue\n\n\ndefault\n\n\nThe Yarn queue the Gobblin Yarn application will run in.\n\n\n\n\n\n\ngobblin.yarn.work.dir\n\n\n/gobblin\n\n\nThe working directory (typically on HDFS) for the Gobblin Yarn application.\n\n\n\n\n\n\ngobblin.yarn.app.report.interval.minutes\n\n\n5\n\n\nThe interval in minutes between two Gobblin Yarn application status reports.\n\n\n\n\n\n\ngobblin.yarn.max.get.app.report.failures\n\n\n4\n\n\nMaximum allowed number of consecutive failures to get a Yarn \nApplicationReport\n.\n\n\n\n\n\n\ngobblin.yarn.email.notification.on.shutdown\n\n\nfalse\n\n\nWhether email notification is enabled or not on shutdown of the \nGobblinYarnAppLauncher\n. If this is set to \ntrue\n, the following configuration properties also need to be set for email notification to work: \nemail.host\n, \nemail.smtp.port\n, \nemail.user\n, \nemail.password\n, \nemail.from\n, and \nemail.tos\n. Refer to \nEmail Alert Properties\n for more information on those configuration properties.\n\n\n\n\n\n\ngobblin.yarn.app.master.memory.mbs\n\n\n512\n\n\nHow much memory in MBs to request for the container running the Gobblin ApplicationMaster.\n\n\n\n\n\n\ngobblin.yarn.app.master.cores\n\n\n1\n\n\nThe number of vcores to request for the container running the Gobblin ApplicationMaster.\n\n\n\n\n\n\ngobblin.yarn.app.master.jars\n\n\nA comma-separated list of jars the Gobblin ApplicationMaster depends on but not in the \nlib\n directory.\n\n\n\n\n\n\n\n\ngobblin.yarn.app.master.files.local\n\n\nA comma-separated list of files on the local filesystem the Gobblin ApplicationMaster depends on.\n\n\n\n\n\n\n\n\ngobblin.yarn.app.master.files.remote\n\n\nA comma-separated list of files on a remote filesystem (typically HDFS) the Gobblin ApplicationMaster depends on.\n\n\n\n\n\n\n\n\ngobblin.yarn.app.master.jvm.args\n\n\nAdditional JVM arguments for the JVM process running the Gobblin ApplicationMaster, e.g., \n-XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m\n \n-XX:CompressedClassSpaceSize=256m -Dconfig.trace=loads\n.\n\n\n\n\n\n\n\n\ngobblin.yarn.initial.containers\n\n\n1\n\n\nThe number of containers to request initially when the application starts to run the WorkUnitRunner.\n\n\n\n\n\n\ngobblin.yarn.container.memory.mbs\n\n\n512\n\n\nHow much memory in MBs to request for the container running the Gobblin WorkUnitRunner.\n\n\n\n\n\n\ngobblin.yarn.container.cores\n\n\n1\n\n\nThe number of vcores to request for the container running the Gobblin WorkUnitRunner.\n\n\n\n\n\n\ngobblin.yarn.container.jars\n\n\nA comma-separated list of jars the Gobblin WorkUnitRunner depends on but not in the \nlib\n directory.\n\n\n\n\n\n\n\n\ngobblin.yarn.container.files.local\n\n\nA comma-separated list of files on the local filesystem the Gobblin WorkUnitRunner depends on.\n\n\n\n\n\n\n\n\ngobblin.yarn.container.files.remote\n\n\nA comma-separated list of files on a remote filesystem (typically HDFS) the Gobblin WorkUnitRunner depends on.\n\n\n\n\n\n\n\n\ngobblin.yarn.container.jvm.args\n\n\nAdditional JVM arguments for the JVM process running the Gobblin WorkUnitRunner, e.g., \n-XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m\n \n-XX:CompressedClassSpaceSize=256m -Dconfig.trace=loads\n.\n\n\n\n\n\n\n\n\ngobblin.yarn.container.affinity.enabled\n\n\ntrue\n\n\nWhether the same host should be used as the preferred host when requesting a replacement container for the one that exits.\n\n\n\n\n\n\ngobblin.yarn.helix.cluster.name\n\n\nGobblinYarn\n\n\nThe name of the Helix cluster that will be registered with ZooKeeper.\n\n\n\n\n\n\ngobblin.yarn.zk.connection.string\n\n\nlocalhost:2181\n\n\nThe ZooKeeper connection string used by Helix.\n\n\n\n\n\n\nhelix.instance.max.retries\n\n\n2\n\n\nMaximum number of times the application tries to restart a failed Helix instance (corresponding to a Yarn container).\n\n\n\n\n\n\ngobblin.yarn.lib.jars.dir\n\n\nThe directory where library jars are stored, typically \ngobblin-dist/lib\n.\n\n\n\n\n\n\n\n\ngobblin.yarn.job.conf.path\n\n\nThe path to either a directory where Gobblin job configuration files are stored or a single job configuration file. Internally Gobblin Yarn will package the configuration files as a tarball so you don't need to.\n\n\n\n\n\n\n\n\ngobblin.yarn.logs.sink.root.dir\n\n\nThe directory on local filesystem on the driver/client side where the aggregated container logs of both the ApplicationMaster and WorkUnitRunner are stored.\n\n\n\n\n\n\n\n\ngobblin.yarn.keytab.file.path\n\n\nThe path to the Kerberos keytab file used for keytab-based authentication/login.\n\n\n\n\n\n\n\n\ngobblin.yarn.keytab.principal.name\n\n\nThe principal name of the keytab.\n\n\n\n\n\n\n\n\ngobblin.yarn.login.interval.minutes\n\n\n1440\n\n\nThe interval in minutes between two keytab logins.\n\n\n\n\n\n\ngobblin.yarn.token.renew.interval.minutes\n\n\n720\n\n\nThe interval in minutes between two delegation token renews.\n\n\n\n\n\n\n\n\nConfiguration System\n\n\nThe Gobblin Yarn application uses the \nTypesafe Config\n library to handle the application configuration. Following \nTypesafe Config\n's model, the Gobblin Yarn application uses a single file named \napplication.conf\n for all configuration properties and another file named \nreference.conf\n for default values. A sample \napplication.conf\n is shown below: \n\n\n# Yarn/Helix configuration properties\ngobblin.yarn.helix.cluster.name=GobblinYarnTest\ngobblin.yarn.app.name=GobblinYarnTest\ngobblin.yarn.lib.jars.dir=\n/home/gobblin/gobblin-dist/lib/\n\ngobblin.yarn.app.master.files.local=\n/home/gobblin/gobblin-dist/conf/log4j-yarn.properties,/home/gobblin/gobblin-dist/conf/application.conf,/home/gobblin/gobblin-dist/conf/reference.conf\n\ngobblin.yarn.container.files.local=${gobblin.yarn.app.master.files.local}\ngobblin.yarn.job.conf.path=\n/home/gobblin/gobblin-dist/job-conf\n\ngobblin.yarn.keytab.file.path=\n/home/gobblin/gobblin.headless.keytab\n\ngobblin.yarn.keytab.principal.name=gobblin\ngobblin.yarn.app.master.jvm.args=\n-XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m\n\ngobblin.yarn.container.jvm.args=\n-XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m\n\ngobblin.yarn.logs.sink.root.dir=/home/gobblin/gobblin-dist/applogs\n\n# File system URIs\nwriter.fs.uri=${fs.uri}\nstate.store.fs.uri=${fs.uri}\n\n# Writer related configuration properties\nwriter.destination.type=HDFS\nwriter.output.format=AVRO\nwriter.staging.dir=${gobblin.yarn.work.dir}/task-staging\nwriter.output.dir=${gobblin.yarn.work.dir}/task-output\n\n# Data publisher related configuration properties\ndata.publisher.type=gobblin.publisher.BaseDataPublisher\ndata.publisher.final.dir=${gobblin.yarn.work.dir}/job-output\ndata.publisher.replace.final.dir=false\n\n# Directory where job/task state files are stored\nstate.store.dir=${gobblin.yarn.work.dir}/state-store\n\n# Directory where error files from the quality checkers are stored\nqualitychecker.row.err.file=${gobblin.yarn.work.dir}/err\n\n# Disable job locking for now\njob.lock.enabled=false\n\n# Directory where job locks are stored\njob.lock.dir=${gobblin.yarn.work.dir}/locks\n\n# Directory where metrics log files are stored\nmetrics.log.dir=${gobblin.yarn.work.dir}/metrics\n\n\n\n\nA sample \nreference.conf\n is shown below:\n\n\n# Yarn/Helix configuration properties\ngobblin.yarn.app.queue=default\ngobblin.yarn.helix.cluster.name=GobblinYarn\ngobblin.yarn.app.name=GobblinYarn\ngobblin.yarn.app.master.memory.mbs=512\ngobblin.yarn.app.master.cores=1\ngobblin.yarn.app.report.interval.minutes=5\ngobblin.yarn.max.get.app.report.failures=4\ngobblin.yarn.email.notification.on.shutdown=false\ngobblin.yarn.initial.containers=1\ngobblin.yarn.container.memory.mbs=512\ngobblin.yarn.container.cores=1\ngobblin.yarn.container.affinity.enabled=true\ngobblin.yarn.helix.instance.max.retries=2\ngobblin.yarn.keytab.login.interval.minutes=1440\ngobblin.yarn.token.renew.interval.minutes=720\ngobblin.yarn.work.dir=/user/gobblin/gobblin-yarn\ngobblin.yarn.zk.connection.string=\nlocalhost:2181\n\n\nfs.uri=\nhdfs://localhost:9000\n\n\n\n\n\nDeployment\n\n\nA standard deployment of Gobblin on Yarn requires a Yarn cluster running Hadoop 2.x (\n2.3.0\n and above recommended) and a ZooKeeper cluster. Make sure the client machine (typically the gateway of the Yarn cluster) is able to access the ZooKeeper instance. \n\n\nDeployment on a Unsecured Yarn Cluster\n\n\nTo do a deployment of the Gobblin Yarn application, first build Gobblin using the following command from the root directory of the Gobblin project. Gobblin on Yarn requires Hadoop 2.x, so make sure \n-PuseHadoop2\n is used.\n\n\n./gradlew clean build -PuseHadoop2\n\n\n\n\nTo build Gobblin against a specific version of Hadoop 2.x, e.g., \n2.7.0\n, run the following command instead:\n\n\n./gradlew clean build -PuseHadoop2 -PhadoopVersion=2.7.0\n\n\n\n\nAfter Gobblin is successfully built, a tarball named \ngobblin-dist-[project-version].tar.gz\n should have been created under the root directory of the project. To deploy the Gobblin Yarn application on a unsecured Yarn cluster, uncompress the tarball somewhere and run the following commands:  \n\n\ncd gobblin-dist\nbin/gobblin-yarn.sh\n\n\n\n\nNote that for the above commands to work, the Hadoop/Yarn configuration directory must be on the classpath and the configuration must be pointing to the right Yarn cluster, or specifically the right ResourceManager and NameNode URLs. This is defined like the following in \ngobblin-yarn.sh\n:\n\n\nCLASSPATH=${FWDIR_CONF}:${GOBBLIN_JARS}:${YARN_CONF_DIR}:${HADOOP_YARN_HOME}/lib\n\n\n\n\nDeployment on a Secured Yarn Cluster\n\n\nWhen deploying the Gobblin Yarn application on a secured Yarn cluster, make sure the keytab file path is correctly specified in \napplication.conf\n and the correct principal for the keytab is used as follows. The rest of the deployment is the same as that on a unsecured Yarn cluster.\n\n\ngobblin.yarn.keytab.file.path=\n/home/gobblin/gobblin.headless.keytab\n\ngobblin.yarn.keytab.principal.name=gobblin\n\n\n\n\nSupporting Existing Gobblin Jobs\n\n\nGobblin on Yarn is backward compatible and supports existing Gobblin jobs running in the standalone and MR modes. To run existing Gobblin jobs, simply put the job configuration files into a directory on the local file system of the driver and setting the configuration property \ngobblin.yarn.job.conf.path\n to point to the directory. When the Gobblin Yarn application starts, Yarn will package the configuration files as a tarball and make sure the tarball gets copied to the ApplicationMaster and properly uncompressed. The \nGobblinHelixJobScheduler\n then loads the job configuration files and schedule the jobs to run.\n\n\nMonitoring\n\n\nGobblin Yarn uses the \nGobblin Metrics\n library for collecting and reporting metrics at the container, job, and task levels. Each \nGobblinWorkUnitRunner\n maintains a \nContainerMetrics\n that is the parent of the \nJobMetrics\n of each job run the container is involved, which is the parent of the \nTaskMetrics\n of each task of the job run. This hierarchical structure allows us to do pre-aggregation in the containers before reporting the metrics to the backend. \n\n\nCollected metrics can be reported to various sinks such as Kafka, files, and JMX, depending on the configuration. Specifically, \nmetrics.enabled\n controls whether metrics collecting and reporting are enabled or not. \nmetrics.reporting.kafka.enabled\n, \nmetrics.reporting.file.enabled\n, and \nmetrics.reporting.jmx.enabled\n control whether collected metrics should be reported or not to Kafka, files, and JMX, respectively. Please refer to \nMetrics Properties\n for the available configuration properties related to metrics collecting and reporting.  \n\n\nIn addition to metric collecting and reporting, Gobblin Yarn also supports writing job execution information to a MySQL-backed job execution history store, which keeps track of job execution information. Please refer to the \nDDL\n for the relevant MySQL tables. Detailed information on the job execution history store including how to configure it can be found \nhere\n.", 
            "title": "Gobblin on Yarn"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#table-of-contents", 
            "text": "Table of Contents  Introduction  Architecture  Overview  The Role of Apache Helix  Gobblin Yarn Application Launcher  YarnAppSecurityManager  LogCopier    Gobblin ApplicationMaster  YarnService  GobblinHelixJobScheduler  LogCopier  YarnContainerSecurityManager    Gobblin WorkUnitRunner  TaskExecutor  GobblinHelixTaskStateTracker  LogCopier  YarnContainerSecurityManager    Failure Handling  ApplicationMaster Failure Handling  Container Failure Handling  Handling Failures to get ApplicationReport      Log Aggregation  Security and Delegation Token Management  Configuration  Configuration Properties  Configuration System    Deployment  Deployment on a Unsecured Yarn Cluster  Deployment on a Secured Yarn Cluster  Supporting Existing Gobblin Jobs    Monitoring", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#introduction", 
            "text": "Gobblin currently is capable of running in the standalone mode on a single machine or in the MapReduce (MR) mode as a MR job on a Hadoop cluster. A Gobblin job is typically running on a schedule through a scheduler, e.g., the built-in  JobScheduler , Azkaban, or Oozie, and each job run ingests new data or data updated since the last run. So this is essentially a batch model for data ingestion and how soon new data becomes available on Hadoop depends on the schedule of the job.   On another aspect, for high data volume data sources such as Kafka, Gobblin typically runs in the MR mode with a considerable number of tasks running in the mappers of a MR job. This helps Gobblin to scale out for data sources with large volumes of data. The MR mode, however, suffers from problems such as large overhead mostly due to the overhead of submitting and launching a MR job and poor cluster resource usage. The MR mode is also fundamentally not appropriate for real-time data ingestion given its batch nature. These deficiencies are summarized in more details below:   In the MR mode, every Gobblin job run starts a new MR job, which costs a considerable amount of time to allocate and start the containers for running the mapper/reducer tasks. This cost can be totally eliminated if the containers are already up and running.  Each Gobblin job running in the MR mode requests a new set of containers and releases them upon job completion. So it's impossible for two jobs to share the containers even though the containers are perfectly capable of running tasks of both jobs.  In the MR mode, All  WorkUnit s are pre-assigned to the mappers before launching the MR job. The assignment is fixed by evenly distributing the  WorkUnit s to the mappers so each mapper gets a fair share of the work in terms of the  number of  WorkUnits . However, an evenly distributed number of  WorkUnit s per mapper does not always guarantee a fair share of the work in terms of the volume of data to pull. This, combined with the fact that the mappers that finish earlier cannot \"steal\"  WorkUnit s assigned to other mappers, means the responsibility of load balancing is on the  Source  implementations, which is not trivial to do, and is virtually impossible in heterogeneous Hadoop clusters where different nodes have different capacity. This also means the duration of a job is determined by the slowest mapper.  A MR job can only hold its containers for a limited of time, beyond which the job may get killed. Real-time data ingestion, however, requires the ingestion tasks to be running all the time or alternatively dividing a continuous data stream into well-defined mini-batches (as in Spark Streaming) that can be promptly executed once created. Both require long-running containers, which are not supported in the MR mode.    Those deficiencies motivated the work on making Gobblin run on Yarn as a native Yarn application. Running Gobblin as a native Yarn application allows much more control over container provisioning and lifecycle management so it's possible to keep the containers running continuously. It also makes it possible to dynamically change the number of containers at runtime depending on the load to further improve the resource efficiency, something that's impossible in the MR mode.           This wiki page documents the design and architecture of the native Gobblin Yarn application and some implementation details. It also covers the configuration system and properties for the application, as well as deployment settings on both unsecured and secured Yarn clusters.", 
            "title": "Introduction"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#overview", 
            "text": "The architecture of Gobblin on Yarn is illustrated in the following diagram. In addition to Yarn, Gobblin on Yarn also leverages  Apache Helix , whose role is discussed in  The Role of Apache Helix . A Gobblin Yarn application consists of three components: the Yarn Application Launcher, the Yarn ApplicationMaster (serving as the Helix  controller ), and the Yarn WorkUnitRunner (serving as the Helix  participant ). The following sections describe each component in details.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#the-role-of-apache-helix", 
            "text": "Apache Helix  is mainly used for managing the cluster of containers and running the  WorkUnit s through its  Distributed Task Execution Framework .   The assignment of tasks to available containers (or participants in Helix's term) is handled by Helix through a finite state model named the  TaskStateModel . Using this  TaskStateModel , Helix is also able to do task rebalancing in case new containers get added or some existing containers die. Clients can also choose to force a task rebalancing if some tasks take much longer time than the others.   Helix also supports a way of doing messaging between different components of a cluster, e.g., between the controller to the participants, or between the client and the controller. The Gobblin Yarn application uses this messaging mechanism to implement graceful shutdown initiated by the client as well as delegation token renew notifications from the client to the ApplicationMaster and the WorkUnitRunner containers.  Heiix relies on ZooKeeper for its operations, and particularly for maintaining the state of the cluster and the resources (tasks in this case). Both the Helix controller and participants connect to ZooKeeper during their entire lifetime. The ApplicationMaster serves as the Helix controller and the worker containers serve as the Helix participants, respectively, as discussed in details below.", 
            "title": "The Role of Apache Helix"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#gobblin-yarn-application-launcher", 
            "text": "The Gobblin Yarn Application Launcher (implemented by class  GobblinYarnAppLauncher ) is the client/driver of a Gobblin Yarn application. The first thing the  GobblinYarnAppLauncher  does when it starts is to register itself with Helix as a  spectator  and creates a new Helix cluster with name specified through the configuration property  gobblin.yarn.helix.cluster.name , if no cluster with the name exists.   The  GobblinYarnAppLauncher  then sets up the Gobblin Yarn application and submits it to run on Yarn. Once the Yarn application successfully starts running, it starts an application state monitor that periodically checks the state of the Gobblin Yarn application. If the state is one of the exit states ( FINISHED ,  FAILED , or  KILLED ), the  GobblinYarnAppLauncher  shuts down itself.   Upon successfully submitting the application to run on Yarn, the  GobblinYarnAppLauncher  also starts a  ServiceManager  that manages the following services that auxiliate the running of the application:", 
            "title": "Gobblin Yarn Application Launcher"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#yarnappsecuritymanager", 
            "text": "The  YarnAppSecurityManager  works with the  YarnContainerSecurityManager  running in the ApplicationMaster and the WorkUnitRunner for a complete solution for security and delegation token management. The  YarnAppSecurityManager  is responsible for periodically logging in through a Kerberos keytab and getting the delegation token refreshed regularly after each login. Each time the delegation token is refreshed, the  YarnContainerSecurityManager  writes the new token to a file on HDFS and sends a message to the ApplicationMaster and each WorkUnitRunner, notifying them the refresh of the delegation token. Checkout  YarnContainerSecurityManager  on how the other side of this system works.", 
            "title": "YarnAppSecurityManager"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#logcopier", 
            "text": "The service  LogCopier  in  GobblinYarnAppLauncher  streams the ApplicationMaster and WorkUnitRunner logs in near real-time from the central location on HDFS where the logs are streamed to from the ApplicationMaster and WorkUnitRunner containers, to the local directory specified through the configuration property  gobblin.yarn.logs.sink.root.dir  on the machine where the  GobblinYarnAppLauncher  runs. More details on this can be found in  Log Aggregation .", 
            "title": "LogCopier"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#gobblin-applicationmaster", 
            "text": "The ApplicationMaster process runs the  GobblinApplicationMaster , which uses a  ServiceManager  to manage the services supporting the operation of the ApplicationMaster process. The services running in  GobblinApplicationMaster  will be discussed later. When it starts, the first thing  GobblinApplicationMaster  does is to connect to ZooKeeper and register itself as a Helix  controller . It then starts the  ServiceManager , which in turn starts the services it manages, as described below.", 
            "title": "Gobblin ApplicationMaster"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#yarnservice", 
            "text": "The service  YarnService  handles all Yarn-related task including the following:   Registering and un-registering the ApplicationMaster with the Yarn ResourceManager.  Requesting the initial set of containers from the Yarn ResourceManager.  Handling any container changes at runtime, e.g., adding more containers or shutting down containers no longer needed. This also includes stopping running containers when the application is asked to stop.   This design makes it switch to a different resource manager, e.g., Mesos, by replacing the service  YarnService  with something else specific to the resource manager, e.g.,  MesosService .", 
            "title": "YarnService"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#gobblinhelixjobscheduler", 
            "text": "GobblinApplicationMaster  runs the  GobblinHelixJobScheduler  that schedules jobs to run through the Helix  Distributed Task Execution Framework . For each Gobblin job run, the  GobblinHelixJobScheduler  starts a  GobblinHelixJobLauncher  that wraps the Gobblin job into a  GobblinHelixJob  and each Gobblin  Task  into a  GobblinHelixTask , which implements the Helix's  Task  interface so Helix knows how to execute it. The  GobblinHelixJobLauncher  then submits the job to a Helix job queue named after the Gobblin job name, from which the Helix Distributed Task Execution Framework picks up the job and runs its tasks through the live participants (available containers).  Like the  LocalJobLauncher  and  MRJobLauncher , the  GobblinHelixJobLauncher  handles output data commit and job state persistence.", 
            "title": "GobblinHelixJobScheduler"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#logcopier_1", 
            "text": "The service  LogCopier  in  GobblinApplicationMaster  streams the ApplicationMaster logs in near real-time from the machine running the ApplicationMaster container to a central location on HDFS so the logs can be accessed at runtime. More details on this can be found in  Log Aggregation .", 
            "title": "LogCopier"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#yarncontainersecuritymanager", 
            "text": "The  YarnContainerSecurityManager  runs in both the ApplicationMaster and the WorkUnitRunner. When it starts, it registers a message handler with the  HelixManager  for handling messages on refreshes of the delegation token. Once such a message is received, the  YarnContainerSecurityManager  gets the path to the token file on HDFS from the message, and updated the the current login user with the new token read from the file.", 
            "title": "YarnContainerSecurityManager"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#gobblin-workunitrunner", 
            "text": "The WorkUnitRunner process runs the  GobblinWorkUnitRunner , which uses a  ServiceManager  to manage the services supporting the operation of the WorkUnitRunner process. The services running in  GobblinWorkUnitRunner  will be discussed later. When it starts, the first thing  GobblinWorkUnitRunner  does is to connect to ZooKeeper and register itself as a Helix  participant . It then starts the  ServiceManager , which in turn starts the services it manages, as discussed below.", 
            "title": "Gobblin WorkUnitRunner"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#taskexecutor", 
            "text": "The  TaskExecutor  remains the same as in the standalone and MR modes, and is purely responsible for running tasks assigned to a WorkUnitRunner.", 
            "title": "TaskExecutor"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#gobblinhelixtaskstatetracker", 
            "text": "The  GobblinHelixTaskStateTracker  has a similar responsibility as the  LocalTaskStateTracker  and  MRTaskStateTracker : keeping track of the state of running tasks including operational metrics, e.g., total records pulled, records pulled per second, total bytes pulled, bytes pulled per second, etc.", 
            "title": "GobblinHelixTaskStateTracker"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#logcopier_2", 
            "text": "The service  LogCopier  in  GobblinWorkUnitRunner  streams the WorkUnitRunner logs in near real-time from the machine running the WorkUnitRunner container to a central location on HDFS so the logs can be accessed at runtime. More details on this can be found in  Log Aggregation .", 
            "title": "LogCopier"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#yarncontainersecuritymanager_1", 
            "text": "The  YarnContainerSecurityManager  in  GobblinWorkUnitRunner  works in the same way as it in  GobblinApplicationMaster .", 
            "title": "YarnContainerSecurityManager"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#failure-handling", 
            "text": "", 
            "title": "Failure Handling"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#applicationmaster-failure-handling", 
            "text": "Under normal operation, the Gobblin ApplicationMaster stays alive unless being asked to stop through a message sent from the launcher (the  GobblinYarnAppLauncher ) as part of the orderly shutdown process. It may, however, fail or get killed by the Yarn ResourceManager for various reasons. For example, the container running the ApplicationMaster may fail and exit due to node failures, or get killed because of using more memory than claimed. When a shutdown of the ApplicationMaster is triggered (e.g., when the shutdown hook is triggered) for any reason, it does so gracefully, i.e., it attempts to stop every services it manages, stop all the running containers, and unregister itself with the ResourceManager. Shutting down the ApplicationMaster shuts down the Yarn application and the application launcher will eventually know that the application completes through a periodic check on the application status.", 
            "title": "ApplicationMaster Failure Handling"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#container-failure-handling", 
            "text": "Under normal operation, a Gobblin Yarn container stays alive unless being released and stopped by the Gobblin ApplicationMaster, and in this case the exit status of the container will be zero. However, a container may exit unexpectedly due to various reasons. For example, a container may fail and exit due to node failures, or be killed because of using more memory than claimed. In this case when a container exits abnormally with a non-zero exit code, Gobblin Yarn tries to restart the Helix instance running in the container by requesting a new Yarn container as a replacement to run the instance. The maximum number of retries can be configured through the key  gobblin.yarn.helix.instance.max.retries .  When requesting a new container to replace the one that completes and exits abnormally, the application has a choice of specifying the same host that runs the completed container as the preferred host, depending on the boolean value of configuration key  gobblin.yarn.container.affinity.enabled . Note that for certain exit codes that indicate something wrong with the host, the value of  gobblin.yarn.container.affinity.enabled  is ignored and no preferred host gets specified, leaving Yarn to figure out a good candidate node for the new container.", 
            "title": "Container Failure Handling"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#handling-failures-to-get-applicationreport", 
            "text": "As mentioned above, once the Gobblin Yarn application successfully starts running, the  GobblinYarnAppLauncher  starts an application state monitor that periodically checks the state of the Yarn application by getting an  ApplicationReport . It may fail to do so and throw an exception, however, if the Yarn client is having some problem connecting and communicating with the Yarn cluster. For example, if the Yarn cluster is down for maintenance, the Yarn client will not be able to get an  ApplicationReport . The  GobblinYarnAppLauncher  keeps track of the number of consecutive failures to get an  ApplicationReport  and initiates a shutdown if this number exceeds the threshold as specified through the configuration property  gobblin.yarn.max.get.app.report.failures . The shutdown will trigger an email notification if the configuration property  gobblin.yarn.email.notification.on.shutdown  is set to  true .", 
            "title": "Handling Failures to get ApplicationReport"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#log-aggregation", 
            "text": "Yarn provides both a Web UI and a command-line tool to access the logs of an application, and also does log aggregation so the logs of all the containers become available on the client side upon requested. However, there are a few limitations that make it hard to access the logs of an application at runtime:   The command-line utility for downloading the aggregated logs will only be able to do so after the application finishes, making it useless for getting access to the logs at the application runtime.    The Web UI does allow logs to be viewed at runtime, but only when the user that access the UI is the same as the user that launches the application. On a Yarn cluster where security is enabled, the user launching the Gobblin Yarn application is typically a user of some headless account.   Because Gobblin runs on Yarn as a long-running native Yarn application, getting access to the logs at runtime is critical to know what's going on in the application and to detect any issues in the application as early as possible. Unfortunately we cannot use the log facility provided by Yarn here due to the above limitations. Alternatively, Gobblin on Yarn has its own mechanism for doing log aggregation and providing access to the logs at runtime, described as follows.  Both the Gobblin ApplicationMaster and WorkUnitRunner run a  LogCopier  that periodically copies new entries of both  stdout  and  stderr  logs of the corresponding processes from the containers to a central location on HDFS under the directory  ${gobblin.yarn.work.dir}/_applogs  in the subdirectories named after the container IDs, one per container. The names of the log files on HDFS combine the container IDs and the original log file names so it's easy to tell which container generates which log file. More specifically, the log files produced by the ApplicationMaster are named  container id .GobblinApplicationMaster.{stdout,stderr} , and the log files produced by the WorkUnitRunner are named  container id .GobblinWorkUnitRunner.{stdout,stderr} .  The Gobblin YarnApplicationLauncher also runs a  LogCopier  that periodically copies new log entries from log files under  ${gobblin.yarn.work.dir}/_applogs  on HDFS to the local filesystem under the directory configured by the property  gobblin.yarn.logs.sink.root.dir . By default, the  LogCopier  checks for new log entries every 60 seconds and will keep reading new log entries until it reaches the end of the log file. This setup enables the Gobblin Yarn application to stream container process logs near real-time all the way to the client/driver.", 
            "title": "Log Aggregation"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#security-and-delegation-token-management", 
            "text": "On a Yarn cluster with security enabled (e.g., Kerberos authentication is required to access HDFS), security and delegation token management is necessary to allow Gobblin run as a long-running Yarn application. Specifically, Gobblin running on a secured Yarn cluster needs to get its delegation token for accessing HDFS renewed periodically, which also requires periodic keytab re-logins because a delegation token can only be renewed up to a limited number of times in one login.  The Gobblin Yarn application supports Kerberos-based authentication and login through a keytab file. The  YarnAppSecurityManager  running in the Yarn Application Launcher and the  YarnContainerSecurityManager  running in the ApplicationMaster and WorkUnitRunner work together to get every Yarn containers updated whenever the delegation token gets updated on the client side by the  YarnAppSecurityManager . More specifically, the  YarnAppSecurityManager  periodically logins through the keytab and gets the delegation token refreshed regularly after each successful login. Every time the  YarnAppSecurityManager  refreshes the delegation token, the  YarnContainerSecurityManager  writes the new token to a file on HDFS and sends a  TOKEN_FILE_UPDATED  message to the ApplicationMaster and each WorkUnitRunner, notifying them the refresh of the delegation token. Upon receiving such a message, the  YarnContainerSecurityManager  running in the ApplicationMaster or WorkUnitRunner gets the path to the token file on HDFS from the message, and updated the the current login user with the new token read from the file.  Both the interval between two Kerberos keytab logins and the interval between two delegation token refreshes are configurable, through the configuration properties  gobblin.yarn.login.interval.minutes  and  gobblin.yarn.token.renew.interval.minutes , respectively.", 
            "title": "Security and Delegation Token Management"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#configuration-properties", 
            "text": "In additional to the common Gobblin configuration properties, documented in  Configuration Properties Glossary , Gobblin on Yarn uses the following configuration properties.      Property  Default Value  Description      gobblin.yarn.app.name  GobblinYarn  The Gobblin Yarn appliation name.    gobblin.yarn.app.queue  default  The Yarn queue the Gobblin Yarn application will run in.    gobblin.yarn.work.dir  /gobblin  The working directory (typically on HDFS) for the Gobblin Yarn application.    gobblin.yarn.app.report.interval.minutes  5  The interval in minutes between two Gobblin Yarn application status reports.    gobblin.yarn.max.get.app.report.failures  4  Maximum allowed number of consecutive failures to get a Yarn  ApplicationReport .    gobblin.yarn.email.notification.on.shutdown  false  Whether email notification is enabled or not on shutdown of the  GobblinYarnAppLauncher . If this is set to  true , the following configuration properties also need to be set for email notification to work:  email.host ,  email.smtp.port ,  email.user ,  email.password ,  email.from , and  email.tos . Refer to  Email Alert Properties  for more information on those configuration properties.    gobblin.yarn.app.master.memory.mbs  512  How much memory in MBs to request for the container running the Gobblin ApplicationMaster.    gobblin.yarn.app.master.cores  1  The number of vcores to request for the container running the Gobblin ApplicationMaster.    gobblin.yarn.app.master.jars  A comma-separated list of jars the Gobblin ApplicationMaster depends on but not in the  lib  directory.     gobblin.yarn.app.master.files.local  A comma-separated list of files on the local filesystem the Gobblin ApplicationMaster depends on.     gobblin.yarn.app.master.files.remote  A comma-separated list of files on a remote filesystem (typically HDFS) the Gobblin ApplicationMaster depends on.     gobblin.yarn.app.master.jvm.args  Additional JVM arguments for the JVM process running the Gobblin ApplicationMaster, e.g.,  -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m   -XX:CompressedClassSpaceSize=256m -Dconfig.trace=loads .     gobblin.yarn.initial.containers  1  The number of containers to request initially when the application starts to run the WorkUnitRunner.    gobblin.yarn.container.memory.mbs  512  How much memory in MBs to request for the container running the Gobblin WorkUnitRunner.    gobblin.yarn.container.cores  1  The number of vcores to request for the container running the Gobblin WorkUnitRunner.    gobblin.yarn.container.jars  A comma-separated list of jars the Gobblin WorkUnitRunner depends on but not in the  lib  directory.     gobblin.yarn.container.files.local  A comma-separated list of files on the local filesystem the Gobblin WorkUnitRunner depends on.     gobblin.yarn.container.files.remote  A comma-separated list of files on a remote filesystem (typically HDFS) the Gobblin WorkUnitRunner depends on.     gobblin.yarn.container.jvm.args  Additional JVM arguments for the JVM process running the Gobblin WorkUnitRunner, e.g.,  -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m   -XX:CompressedClassSpaceSize=256m -Dconfig.trace=loads .     gobblin.yarn.container.affinity.enabled  true  Whether the same host should be used as the preferred host when requesting a replacement container for the one that exits.    gobblin.yarn.helix.cluster.name  GobblinYarn  The name of the Helix cluster that will be registered with ZooKeeper.    gobblin.yarn.zk.connection.string  localhost:2181  The ZooKeeper connection string used by Helix.    helix.instance.max.retries  2  Maximum number of times the application tries to restart a failed Helix instance (corresponding to a Yarn container).    gobblin.yarn.lib.jars.dir  The directory where library jars are stored, typically  gobblin-dist/lib .     gobblin.yarn.job.conf.path  The path to either a directory where Gobblin job configuration files are stored or a single job configuration file. Internally Gobblin Yarn will package the configuration files as a tarball so you don't need to.     gobblin.yarn.logs.sink.root.dir  The directory on local filesystem on the driver/client side where the aggregated container logs of both the ApplicationMaster and WorkUnitRunner are stored.     gobblin.yarn.keytab.file.path  The path to the Kerberos keytab file used for keytab-based authentication/login.     gobblin.yarn.keytab.principal.name  The principal name of the keytab.     gobblin.yarn.login.interval.minutes  1440  The interval in minutes between two keytab logins.    gobblin.yarn.token.renew.interval.minutes  720  The interval in minutes between two delegation token renews.", 
            "title": "Configuration Properties"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#configuration-system", 
            "text": "The Gobblin Yarn application uses the  Typesafe Config  library to handle the application configuration. Following  Typesafe Config 's model, the Gobblin Yarn application uses a single file named  application.conf  for all configuration properties and another file named  reference.conf  for default values. A sample  application.conf  is shown below:   # Yarn/Helix configuration properties\ngobblin.yarn.helix.cluster.name=GobblinYarnTest\ngobblin.yarn.app.name=GobblinYarnTest\ngobblin.yarn.lib.jars.dir= /home/gobblin/gobblin-dist/lib/ \ngobblin.yarn.app.master.files.local= /home/gobblin/gobblin-dist/conf/log4j-yarn.properties,/home/gobblin/gobblin-dist/conf/application.conf,/home/gobblin/gobblin-dist/conf/reference.conf \ngobblin.yarn.container.files.local=${gobblin.yarn.app.master.files.local}\ngobblin.yarn.job.conf.path= /home/gobblin/gobblin-dist/job-conf \ngobblin.yarn.keytab.file.path= /home/gobblin/gobblin.headless.keytab \ngobblin.yarn.keytab.principal.name=gobblin\ngobblin.yarn.app.master.jvm.args= -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m \ngobblin.yarn.container.jvm.args= -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m \ngobblin.yarn.logs.sink.root.dir=/home/gobblin/gobblin-dist/applogs\n\n# File system URIs\nwriter.fs.uri=${fs.uri}\nstate.store.fs.uri=${fs.uri}\n\n# Writer related configuration properties\nwriter.destination.type=HDFS\nwriter.output.format=AVRO\nwriter.staging.dir=${gobblin.yarn.work.dir}/task-staging\nwriter.output.dir=${gobblin.yarn.work.dir}/task-output\n\n# Data publisher related configuration properties\ndata.publisher.type=gobblin.publisher.BaseDataPublisher\ndata.publisher.final.dir=${gobblin.yarn.work.dir}/job-output\ndata.publisher.replace.final.dir=false\n\n# Directory where job/task state files are stored\nstate.store.dir=${gobblin.yarn.work.dir}/state-store\n\n# Directory where error files from the quality checkers are stored\nqualitychecker.row.err.file=${gobblin.yarn.work.dir}/err\n\n# Disable job locking for now\njob.lock.enabled=false\n\n# Directory where job locks are stored\njob.lock.dir=${gobblin.yarn.work.dir}/locks\n\n# Directory where metrics log files are stored\nmetrics.log.dir=${gobblin.yarn.work.dir}/metrics  A sample  reference.conf  is shown below:  # Yarn/Helix configuration properties\ngobblin.yarn.app.queue=default\ngobblin.yarn.helix.cluster.name=GobblinYarn\ngobblin.yarn.app.name=GobblinYarn\ngobblin.yarn.app.master.memory.mbs=512\ngobblin.yarn.app.master.cores=1\ngobblin.yarn.app.report.interval.minutes=5\ngobblin.yarn.max.get.app.report.failures=4\ngobblin.yarn.email.notification.on.shutdown=false\ngobblin.yarn.initial.containers=1\ngobblin.yarn.container.memory.mbs=512\ngobblin.yarn.container.cores=1\ngobblin.yarn.container.affinity.enabled=true\ngobblin.yarn.helix.instance.max.retries=2\ngobblin.yarn.keytab.login.interval.minutes=1440\ngobblin.yarn.token.renew.interval.minutes=720\ngobblin.yarn.work.dir=/user/gobblin/gobblin-yarn\ngobblin.yarn.zk.connection.string= localhost:2181 \n\nfs.uri= hdfs://localhost:9000", 
            "title": "Configuration System"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#deployment", 
            "text": "A standard deployment of Gobblin on Yarn requires a Yarn cluster running Hadoop 2.x ( 2.3.0  and above recommended) and a ZooKeeper cluster. Make sure the client machine (typically the gateway of the Yarn cluster) is able to access the ZooKeeper instance.", 
            "title": "Deployment"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#deployment-on-a-unsecured-yarn-cluster", 
            "text": "To do a deployment of the Gobblin Yarn application, first build Gobblin using the following command from the root directory of the Gobblin project. Gobblin on Yarn requires Hadoop 2.x, so make sure  -PuseHadoop2  is used.  ./gradlew clean build -PuseHadoop2  To build Gobblin against a specific version of Hadoop 2.x, e.g.,  2.7.0 , run the following command instead:  ./gradlew clean build -PuseHadoop2 -PhadoopVersion=2.7.0  After Gobblin is successfully built, a tarball named  gobblin-dist-[project-version].tar.gz  should have been created under the root directory of the project. To deploy the Gobblin Yarn application on a unsecured Yarn cluster, uncompress the tarball somewhere and run the following commands:    cd gobblin-dist\nbin/gobblin-yarn.sh  Note that for the above commands to work, the Hadoop/Yarn configuration directory must be on the classpath and the configuration must be pointing to the right Yarn cluster, or specifically the right ResourceManager and NameNode URLs. This is defined like the following in  gobblin-yarn.sh :  CLASSPATH=${FWDIR_CONF}:${GOBBLIN_JARS}:${YARN_CONF_DIR}:${HADOOP_YARN_HOME}/lib", 
            "title": "Deployment on a Unsecured Yarn Cluster"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#deployment-on-a-secured-yarn-cluster", 
            "text": "When deploying the Gobblin Yarn application on a secured Yarn cluster, make sure the keytab file path is correctly specified in  application.conf  and the correct principal for the keytab is used as follows. The rest of the deployment is the same as that on a unsecured Yarn cluster.  gobblin.yarn.keytab.file.path= /home/gobblin/gobblin.headless.keytab \ngobblin.yarn.keytab.principal.name=gobblin", 
            "title": "Deployment on a Secured Yarn Cluster"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#supporting-existing-gobblin-jobs", 
            "text": "Gobblin on Yarn is backward compatible and supports existing Gobblin jobs running in the standalone and MR modes. To run existing Gobblin jobs, simply put the job configuration files into a directory on the local file system of the driver and setting the configuration property  gobblin.yarn.job.conf.path  to point to the directory. When the Gobblin Yarn application starts, Yarn will package the configuration files as a tarball and make sure the tarball gets copied to the ApplicationMaster and properly uncompressed. The  GobblinHelixJobScheduler  then loads the job configuration files and schedule the jobs to run.", 
            "title": "Supporting Existing Gobblin Jobs"
        }, 
        {
            "location": "/user-guide/Gobblin-on-Yarn/#monitoring", 
            "text": "Gobblin Yarn uses the  Gobblin Metrics  library for collecting and reporting metrics at the container, job, and task levels. Each  GobblinWorkUnitRunner  maintains a  ContainerMetrics  that is the parent of the  JobMetrics  of each job run the container is involved, which is the parent of the  TaskMetrics  of each task of the job run. This hierarchical structure allows us to do pre-aggregation in the containers before reporting the metrics to the backend.   Collected metrics can be reported to various sinks such as Kafka, files, and JMX, depending on the configuration. Specifically,  metrics.enabled  controls whether metrics collecting and reporting are enabled or not.  metrics.reporting.kafka.enabled ,  metrics.reporting.file.enabled , and  metrics.reporting.jmx.enabled  control whether collected metrics should be reported or not to Kafka, files, and JMX, respectively. Please refer to  Metrics Properties  for the available configuration properties related to metrics collecting and reporting.    In addition to metric collecting and reporting, Gobblin Yarn also supports writing job execution information to a MySQL-backed job execution history store, which keeps track of job execution information. Please refer to the  DDL  for the relevant MySQL tables. Detailed information on the job execution history store including how to configure it can be found  here .", 
            "title": "Monitoring"
        }, 
        {
            "location": "/user-guide/Compaction/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nMapReduce Compactor\n\n\nExample Use Case\n\n\nBasic Usage\n\n\nNon-deduping Compaction via Map-only Jobs\n\n\nHandling Late Records\n\n\nVerifying Data Completeness Before Compaction\n\n\n\n\n\n\nHive Compactor\n\n\nUsage\n\n\nGlobal Config Properties (example: compaction.properties)\n\n\nJob Config Properties (example: jobconf/task1.conf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompaction can be used to post-process files pulled by Gobblin with certain semantics. Deduplication is one of the common reasons to do compaction, e.g., you may want to\n\n\n\n\ndeduplicate on all fields of the records.\n\n\ndeduplicate on key fields of the records, keep the one with the latest timestamp for records with the same key.\n\n\n\n\nThis is because duplicates can be generated for multiple reasons including both intended and unintended:\n\n\n\n\nFor ingestion from data sources with mutable records (e.g., relational databases), instead of ingesting a full snapshot of a table every time, one may wish to ingest only the records that were changed since the previous run (i.e., delta records), and merge these delta records with previously generated snapshots in a compaction. In this case, for records with the same primary key, the one with the latest timestamp should be kept.\n\n\nThe data source you ingest from may have duplicate records, e.g., if you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during the replication. In some data sources duplicate records may also be produced by the data producer.\n\n\nIn rare circumstances, Gobblin may pull the same data twice, thus creating duplicate records. This may happen if Gobblin publishes the data successfully, but for some reason fails to persist the checkpoints (watermarks) into the state store.\n\n\n\n\nGobblin provides two compactors out-of-the-box, a MapReduce compactor and a Hive compactor.\n\n\nMapReduce Compactor\n\n\nThe MapReduce compactor can be used to deduplicate on all or certain fields of the records. For duplicate records, one of them will be preserved; there is no guarantee which one will be preserved.\n\n\nA use case of MapReduce Compactor is for Kafka records deduplication. We will use the following example use case to explain the MapReduce Compactor.\n\n\nExample Use Case\n\n\nSuppose we ingest data from a Kafka broker, and we would like to publish the data by hour and by day, both of which are deduplicated:\n\n\n\n\nData in the Kafka broker is first ingested into an \nhourly_staging\n folder, e.g., \n/data/kafka_topics/NewUserEvent/hourly_staging/2015/10/29/08...\n\n\nA compaction with deduplication runs hourly, consumes data in \nhourly_staging\n and publish data into \nhourly\n, e.g., \n/data/kafka_topics/NewUserEvent/hourly/2015/10/29/08...\n\n\nA non-deduping compaction runs daily, consumes data in \nhourly\n and publish data into \ndaily\n, e.g., \n/data/kafka_topics/NewUserEvent/daily/2015/10/29...\n\n\n\n\nBasic Usage\n\n\nMRCompactor.compact()\n is the entry point for MapReduce-based compaction. The input data to be compacted is specified by \ncompaction.input.dir\n. Each subdir under \ncompaction.input.dir\n is considered a \ntopic\n. Each topic may contain multiple \ndatasets\n, each of which is a unit for compaction. It is up to \nMRCompactorJobPropCreator\n to determine what is a dataset under each topic. If a topic has multiple levels of folders, subsequent levels can be specified using \ncompaction.input.subdir\n.\n\n\nIn the above example use case, for hourly compaction, each dataset contains an hour's data in the \nhourly_staging\n folder, e.g., \n/data/kafka_topics/NewUserEvent/hourly_staging/2015/10/29/08\n; for daily compaction, each dataset contains 24 hourly folder of a day, e.g., \n/data/kafka_topics/NewUserEvent/hourly/2015/10/29\n. In hourly compaction, you may use the following config properties:\n\n\ncompaction.input.dir=/data/kafka_topics\ncompaction.dest.dir=/data/kafka_topics\ncompaction.input.subdir=hourly_staging\ncompaction.dest.subdir=hourly\ncompaction.folder.pattern=YYYY/MM/dd\ncompaction.timebased.max.time.ago=3h\ncompaction.timebased.min.time.ago=1h\ncompaction.jobprops.creator.class=gobblin.compaction.mapreduce.MRCompactorTimeBasedJobPropCreator\ncompaction.job.runner.class=gobblin.compaction.mapreduce.avro.MRCompactorAvroKeyDedupJobRunner (if your data is Avro)\n\n\n\n\nIf your data format is not Avro, you can implement a different job runner class for deduplicating your data format. \ncompaction.timebased.max.time.ago\n and \ncompaction.timebased.min.time.ago\n are used to control the earliest and latest folders to be processed, e.g., if there values are 3h and 1h, respectively, and suppose the current time is 10/07 9:20am, it will not process folders on 10/07/06 or before (since they are more than 3 hours ago) or folders on 10/07/09 (since they are less than 1 hour ago).\n\n\nNon-deduping Compaction via Map-only Jobs\n\n\nThere are two types of Non-deduping compaction.\n\n\n\n\nType 1\n: deduplication is not needed, for example you simply want to consolidate files in 24 hourly folders into a single daily folder.\n\n\nType 2\n: deduplication is needed, i.e., the published data should not contain duplicates, but the input data are already deduplicated. The daily compaction in the above example use case is of this type.\n\n\n\n\nProperty \ncompaction.input.deduplicated\n specifies whether the input data are deduplicated (default is false), and property \ncompaction.output.deduplicated\n specifies whether the output data should be deduplicated (default is true). For type 1 deduplication, set both to false. For type 2 deduplication, set both to true.\n\n\nThe reason these two types of compaction need to be separated is because of late data handling, which we will explain next.\n\n\nHandling Late Records\n\n\nLate records are records that arrived at a folder after compaction on this folder has started. We explain how Gobblin handles late records using the following example.\n\n\nIn this use case, both hourly compaction and daily compaction need a mechanism to handle late records. For hourly compaction, late records are records that arrived at an \nhourly_staging\n folder after the hourly compaction of that folder has started. It is similar for daily compaction.\n\n\nCompaction with Deduplication\n\n\nFor a compaction with deduplication (i.e., hourly compaction in the above use case), there are two options to deal with late data:\n\n\n\n\nOption 1\n: if there are late data, re-do the compaction. For example, you may run the hourly compaction multiple times per hour. The first run will do the normal compaction, and in each subsequent run, if it detects late data in a folder, it will re-do compaction for that folder.\n\n\n\n\nTo do so, set \ncompaction.job.overwrite.output.dir=true\n and \ncompaction.recompact.from.input.for.late.data=true\n.\n\n\nPlease note the following when you use this option: (1) this means that your already-published data will be re-published if late data are detected; (2) this is potentially dangerous if your input folders have short retention periods. For example, suppose \nhourly_staging\n folders have a 2-day retention period, i.e., folder \n/data/kafka_topics/NewUserEvent/hourly_staging/2015/10/29\n will be deleted on 2015/10/31. If, after 2015/10/31, new data arrived at this folder and you re-compact this folder and publish the data to \nhourly\n, all original data will be gone. To avoid this problem you may set \ncompaction.timebased.max.time.ago=2d\n so that compaction will not be performed on a folder more than 2 days ago. However, this means that if a late record is late for more than 2 days, it will never be published into \nhourly\n.\n\n\n\n\nOption 2\n: (this is the default option) if there are late data, copy the late data into a \n[output_subdir]/_late\n folder, e.g., for hourly compaction, late data in \nhourly_staging\n will be copied to \nhourly_late\n folders, e.g., \n/data/kafka_topics/NewUserEvent/hourly_late/2015/10/29...\n. \n\n\n\n\nIf re-compaction is not necessary, this is all you need to do. If re-compaction is needed, you may schedule or manually invoke a re-compaction job which will re-compact by consuming data in both \nhourly\n and \nhourly_late\n. For this job, you need to set \ncompaction.job.overwrite.output.dir=true\n and \ncompaction.recompact.from.dest.paths=true\n.\n\n\nNote that this re-compaction is different from the re-compaction in Option 1: this re-compaction consumes data in output folders (i.e., \nhourly\n) whereas the re-compaction in Option 1 consumes data in input folders (i.e., \nhourly_staging\n).\n\n\nCompaction without Deduplication\n\n\nFor a compaction without deduplication, if it is type 2, the same two options above apply. If it is type 1, late data will simply be copied to the output folder.\n\n\nHow to Determine if a Data File is Late\n\n\nEvery time a compaction finishes (except the case below), Gobblin will create a file named \n_COMPACTION_COMPLETE\n in the compaction output folder. This file contains the timestamp of when the compaction job starts. All files in the input folder with earlier modification timestamps have been compacted. Next time the compaction runs, files in the input folder with later timestamps are considered late data.\n\n\nThe \n_COMPACTION_COMPLETE\n file will be only be created if it is a regular compaction that consumes input data (including compaction jobs that just copy late data to the output folder or the \n[output_subdir]/_late\n folder without launching an MR job). It will not be created if it is a re-compaction that consumes output data. This is because whether a file in the input folder is a late file depends on whether it has been compacted or moved into the output folder, which is not affected by a re-compaction that consumes output data.\n\n\nOne way of reducing the chance of seeing late records is to verify data completeness before running compaction, which will be explained next.\n\n\nVerifying Data Completeness Before Compaction\n\n\nBesides aborting the compaction job for a dataset if new data in the input folder is found, another way to reduce the chance of seeing late events is to verify the completeness of input data before running compaction. To do so, set \ncompaction.completeness.verification.enabled=true\n, extend \nDataCompletenessVerifier.AbstractRunner\n and put in your verification logic, and pass it via \ncompaction.completeness.verification.class\n.\n\n\nWhen data completeness verification is enabled, \nMRCompactor\n will verify data completeness for the input datasets, and meanwhile speculatively start the compaction MR jobs. When the compaction MR job for a dataset finishes, if the completeness of the dataset is verified, its compacted data will be published, otherwise it is discarded, and the compaction MR job for this dataset will be launched again with a reduced priority.\n\n\nIt is possible to control which topics should or should not be verified via \ncompaction.completeness.verification.whitelist\n and \ncompaction.completeness.verification.blacklist\n. It is also possible to set a timeout for data completeness verification via \ncompaction.completeness.verification.timeout.minutes\n. A dataset whose completeness verification timed out can be configured to be either compacted anyway or not compacted.\n\n\nHive Compactor\n\n\nThe Hive compactor can be used to merge a snapshot with one or multiple deltas. It assumes the snapshot and the deltas meet the following requirements:\n\n\n\n\nSnapshot and all deltas are in Avro format.\n\n\nSnapshot and all deltas have the same primary key attributes (they do not need to have the same schema).\n\n\nSnapshot is pulled earlier than all deltas. Therefore if a key appears in both snapshot and deltas, the one in the snapshot should be discarded.\n\n\nThe deltas are pulled one after another, and ordered in ascending order of pull time. If a key appears in both the ith delta and the jth delta (i \n j), the one in the jth delta survives.\n\n\n\n\nIn the near future we also plan to support selecting records by timestamps (rather than which file they appear). This is useful if the snapshot and the deltas are pulled in parallel, where if a key has multiple occurrences we should keep the one with the latest timestamp.\n\n\nNote that since delta tables don't have information of deleted records, such information is only available the next time the full snapshot is pulled.\n\n\nUsage\n\n\nAfter building Gobblin (i.e., \n./gradlew clean build\n), a zipped file \nbuild/gobblin-compaction/distributions/gobblin-compaction.tar.gz\n should be created. It contains a jar file (\ngobblin-compaction.jar\n), a folder of dependencies (\ngobblin-compaction_lib\n), and a log4j config file (\nlog4j.xml\n).\n\n\nTo run compaction, extract it into a folder, go to that folder and run \n\n\njava -jar gobblin-compaction.jar \nglobal-config-file\n\n\nIf for whatever reason (e.g., your Hadoop cluster is in secure mode) you need to run the jar using Hadoop or Yarn, then you first need to make sure the correct log4j config file is used, since there is another log4j config file in the Hadoop classpath. To do so, run the following two commands:\n\n\nexport HADOOP_CLASSPATH=.\nexport HADOOP_USER_CLASSPATH_FIRST=true\n\n\n\n\nThe first command adds the current directory to the Hadoop classpath, and the second command tells Hadoop/Yarn to prioritize user's classpath. Then you can run the compaction jar:\n\n\nhadoop jar gobblin-compaction.jar \nglobal-config-file\n\n\nor\n\n\nyarn jar gobblin-compaction.jar \nglobal-config-file\n\n\nThe merged data will be written to the HDFS directory specified in \noutput.datalocation\n, as one or more Avro files. The schema of the output data will be the same as the schema of the last delta (which is the last pulled data and thus has the latest schema).\n\n\nThe provided log4j config file (\nlog4j.xml\n) prints logs from Gobblin compaction classes to the console, and writes logs from other classes (e.g., Hive classes) to logs/gobblin-compaction.log. Note that for drop table queries (\nDROP TABLE IF EXISTS \ntablename\n), the Hive JDBC client will throw \nNoSuchObjectException\n if the table doesn't exist. This is normal and such exceptions should be ignored.\n\n\nGlobal Config Properties (example: compaction.properties)\n\n\n(1) Required:\n\n\n\n\ncompaction.config.dir\n\n\n\n\nThis is the the compaction jobconfig directory. Each file in this directory should be a jobconfig file (described in the next section).\n\n\n(2) Optional:\n\n\n\n\nhadoop.configfile.\n*\n\n\n\n\nHadoop configuration files that should be loaded\n(e.g., hadoop.configfile.coresite.xml=/export/apps/hadoop/latest/etc/hadoop/core-site.xml)\n\n\n\n\nhdfs.uri\n\n\n\n\nIf property \nfs.defaultFS\n (or \nfs.default.name\n) is specified in the hadoop config file, then this property is not needed. However, if it is specified, it will override \nfs.defaultFS\n (or \nfs.default.name\n).\n\n\nIf \nfs.defaultFS\n or \nfs.default.name\n is not specified in the hadoop config file, and this property is also not specified, then the default value \"hdfs://localhost:9000\" will be used.\n\n\n\n\nhiveserver.version\n (default: 2)\n\n\n\n\nEither 1 or 2.\n\n\n\n\n\n\nhiveserver.connection.string\n\n\n\n\n\n\nhiveserver.url\n\n\n\n\n\n\nhiveserver.user\n (default: \"\")\n\n\n\n\n\n\nhiveserver.password\n (default: \"\")\n\n\n\n\n\n\nIf \nhiveserver.connection.string\n is specified, it will be used to connect to hiveserver.\n\n\nIf \nhiveserver.connection.string\n is not specified but \nhiveserver.url\n is specified, then it uses (\nhiveserver.url\n, \nhiveserver.user\n, \nhiveserver.password\n) to connect to hiveserver.\n\n\nIf neither \nhiveserver.connection.string\n nor \nhiveserver.url\n is specified, then embedded hiveserver will be used (i.e., \njdbc:hive://\n if \nhiveserver.version=1\n, \njdbc:hive2://\n if \nhiveserver.version=2\n)\n\n\n\n\nhivesite.dir\n\n\n\n\nDirectory that contains hive-site.xml, if hive-site.xml should be loaded.\n\n\n\n\nhive.\n*\n\n\n\n\nAny hive config property. (e.g., \nhive.join.cache.size\n). If specified, it will override the corresponding property in hive-site.xml.\n\n\nJob Config Properties (example: jobconf/task1.conf)\n\n\n(1) Required:\n\n\n\n\nsnapshot.pkey\n\n\n\n\ncomma separated primary key attributes of the snapshot table\n\n\n\n\nsnapshot.datalocation\n\n\n\n\nsnapshot data directory in HDFS\n\n\n\n\ndelta.i.pkey\n (i = 1, 2...)\n\n\n\n\nthe primary key of ith delta table\n(the primary key of snapshot and all deltas should be the same)\n\n\n\n\ndelta.i.datalocation\n (i = 1, 2...)\n\n\n\n\nith delta table's data directory in HDFS\n\n\n\n\noutput.datalocation\n\n\n\n\nthe HDFS data directory for the output\n(make sure you have write permission on this directory)\n\n\n(2) Optional:\n\n\n\n\nsnapshot.name\n (default: randomly generated name)\n\n\n\n\nprefix name of the snapshot table. The table name will be snapshot.name + random suffix\n\n\n\n\nsnapshot.schemalocation\n\n\n\n\nsnapshot table's schema location in HDFS. If not specified, schema will be extracted from the data.\n\n\n\n\ndelta.i.name\n (default: randomly generated name)\n\n\n\n\nprefix name of the ith delta table. The table name will be delta.i.name + random suffix\n\n\n\n\ndelta.i.schemalocation\n\n\n\n\nith delta table's schema location in HDFS. If not specified, schema will be extracted from the data.\n\n\n\n\noutput.name\n (default: randomly generated name)\n\n\n\n\nprefix name of the output table. The table name will be output.name + random suffix\n\n\n\n\nhive.db.name\n (default: default)\n\n\n\n\nthe database name to be used. This database should already exist, and you should have write permission on it.\n\n\n\n\nhive.queue.name\n (default: default)\n\n\n\n\nqueue name to be used.\n\n\n\n\nhive.use.mapjoin\n (default: if not specified in the global config file, then false)\n\n\n\n\nwhether map-side join should be turned on. If specified both in this property and in the global config file (hive.*), this property takes precedences. \n\n\n\n\nhive.mapjoin.smalltable.filesize\n (default: if not specified in the global config file, then use Hive's default value)\n\n\n\n\nif hive.use.mapjoin = true, mapjoin will be used if the small table size is smaller than hive.mapjoin.smalltable.filesize (in bytes).\nIf specified both in this property and in the global config file (hive.*), this property takes precedences. \n\n\n\n\nhive.tmpschema.dir\n (default: the parent dir of the data location dir where the data is used to extract the schema)\n\n\n\n\nIf we need to extract schema from data, this dir is for the extracted schema.\nNote that if you do not have write permission on the default dir, you must specify this property as a dir where you do have write permission.\n\n\n\n\nsnapshot.copydata\n (default: false)\n\n\n\n\nSet to true if you don't want to (or are unable to) create external table on snapshot.datalocation. A copy of the snapshot data will be created in \nhive.tmpdata.dir\n, and will be removed after the compaction.\n\n\nThis property should be set to true if either of the following two situations applies:\n\n\n(i) You don't have write permission to \nsnapshot.datalocation\n. If so, once you create an external table on \nsnapshot.datalocation\n, you may not be able to drop it. This is a Hive bug and for more information, see \nthis page\n, which includes a Hive patch for the bug.\n\n\n(ii) You want to use a certain subset of files in \nsnapshot.datalocation\n (e.g., \nsnapshot.datalocation\n contains both .csv and .avro files but you only want to use .avro files)\n\n\n\n\ndelta.i.copydata\n (i = 1, 2...) (default: false)\n\n\n\n\nSimilar as \nsnapshot.copydata\n\n\n\n\nhive.tmpdata.dir\n (default: \"/\")\n\n\n\n\nIf \nsnapshot.copydata\n = true or \ndelta.i.copydata\n = true, the data will be copied to this dir. You should have write permission to this dir.\n\n\n\n\nsnapshot.dataformat.extension.name\n (default: \"\")\n\n\n\n\nIf \nsnapshot.copydata\n = true, then only those data files whose extension is \nsnapshot.dataformat\n will be moved to \nhive.tmpdata.dir\n.\n\n\n\n\ndelta.i.dataformat.extension.name\n (default: \"\")\n\n\n\n\nSimilar as \nsnapshot.dataformat.extension.name\n. \n\n\n\n\nmapreduce.job.num.reducers\n\n\n\n\nNumber of reducers for the job.\n\n\n\n\ntiming.file\n (default: time.txt)\n\n\n\n\nA file where the running time of each compaction job is printed.", 
            "title": "Compaction"
        }, 
        {
            "location": "/user-guide/Compaction/#table-of-contents", 
            "text": "Table of Contents  MapReduce Compactor  Example Use Case  Basic Usage  Non-deduping Compaction via Map-only Jobs  Handling Late Records  Verifying Data Completeness Before Compaction    Hive Compactor  Usage  Global Config Properties (example: compaction.properties)  Job Config Properties (example: jobconf/task1.conf)        Compaction can be used to post-process files pulled by Gobblin with certain semantics. Deduplication is one of the common reasons to do compaction, e.g., you may want to   deduplicate on all fields of the records.  deduplicate on key fields of the records, keep the one with the latest timestamp for records with the same key.   This is because duplicates can be generated for multiple reasons including both intended and unintended:   For ingestion from data sources with mutable records (e.g., relational databases), instead of ingesting a full snapshot of a table every time, one may wish to ingest only the records that were changed since the previous run (i.e., delta records), and merge these delta records with previously generated snapshots in a compaction. In this case, for records with the same primary key, the one with the latest timestamp should be kept.  The data source you ingest from may have duplicate records, e.g., if you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during the replication. In some data sources duplicate records may also be produced by the data producer.  In rare circumstances, Gobblin may pull the same data twice, thus creating duplicate records. This may happen if Gobblin publishes the data successfully, but for some reason fails to persist the checkpoints (watermarks) into the state store.   Gobblin provides two compactors out-of-the-box, a MapReduce compactor and a Hive compactor.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Compaction/#mapreduce-compactor", 
            "text": "The MapReduce compactor can be used to deduplicate on all or certain fields of the records. For duplicate records, one of them will be preserved; there is no guarantee which one will be preserved.  A use case of MapReduce Compactor is for Kafka records deduplication. We will use the following example use case to explain the MapReduce Compactor.", 
            "title": "MapReduce Compactor"
        }, 
        {
            "location": "/user-guide/Compaction/#example-use-case", 
            "text": "Suppose we ingest data from a Kafka broker, and we would like to publish the data by hour and by day, both of which are deduplicated:   Data in the Kafka broker is first ingested into an  hourly_staging  folder, e.g.,  /data/kafka_topics/NewUserEvent/hourly_staging/2015/10/29/08...  A compaction with deduplication runs hourly, consumes data in  hourly_staging  and publish data into  hourly , e.g.,  /data/kafka_topics/NewUserEvent/hourly/2015/10/29/08...  A non-deduping compaction runs daily, consumes data in  hourly  and publish data into  daily , e.g.,  /data/kafka_topics/NewUserEvent/daily/2015/10/29...", 
            "title": "Example Use Case"
        }, 
        {
            "location": "/user-guide/Compaction/#basic-usage", 
            "text": "MRCompactor.compact()  is the entry point for MapReduce-based compaction. The input data to be compacted is specified by  compaction.input.dir . Each subdir under  compaction.input.dir  is considered a  topic . Each topic may contain multiple  datasets , each of which is a unit for compaction. It is up to  MRCompactorJobPropCreator  to determine what is a dataset under each topic. If a topic has multiple levels of folders, subsequent levels can be specified using  compaction.input.subdir .  In the above example use case, for hourly compaction, each dataset contains an hour's data in the  hourly_staging  folder, e.g.,  /data/kafka_topics/NewUserEvent/hourly_staging/2015/10/29/08 ; for daily compaction, each dataset contains 24 hourly folder of a day, e.g.,  /data/kafka_topics/NewUserEvent/hourly/2015/10/29 . In hourly compaction, you may use the following config properties:  compaction.input.dir=/data/kafka_topics\ncompaction.dest.dir=/data/kafka_topics\ncompaction.input.subdir=hourly_staging\ncompaction.dest.subdir=hourly\ncompaction.folder.pattern=YYYY/MM/dd\ncompaction.timebased.max.time.ago=3h\ncompaction.timebased.min.time.ago=1h\ncompaction.jobprops.creator.class=gobblin.compaction.mapreduce.MRCompactorTimeBasedJobPropCreator\ncompaction.job.runner.class=gobblin.compaction.mapreduce.avro.MRCompactorAvroKeyDedupJobRunner (if your data is Avro)  If your data format is not Avro, you can implement a different job runner class for deduplicating your data format.  compaction.timebased.max.time.ago  and  compaction.timebased.min.time.ago  are used to control the earliest and latest folders to be processed, e.g., if there values are 3h and 1h, respectively, and suppose the current time is 10/07 9:20am, it will not process folders on 10/07/06 or before (since they are more than 3 hours ago) or folders on 10/07/09 (since they are less than 1 hour ago).", 
            "title": "Basic Usage"
        }, 
        {
            "location": "/user-guide/Compaction/#non-deduping-compaction-via-map-only-jobs", 
            "text": "There are two types of Non-deduping compaction.   Type 1 : deduplication is not needed, for example you simply want to consolidate files in 24 hourly folders into a single daily folder.  Type 2 : deduplication is needed, i.e., the published data should not contain duplicates, but the input data are already deduplicated. The daily compaction in the above example use case is of this type.   Property  compaction.input.deduplicated  specifies whether the input data are deduplicated (default is false), and property  compaction.output.deduplicated  specifies whether the output data should be deduplicated (default is true). For type 1 deduplication, set both to false. For type 2 deduplication, set both to true.  The reason these two types of compaction need to be separated is because of late data handling, which we will explain next.", 
            "title": "Non-deduping Compaction via Map-only Jobs"
        }, 
        {
            "location": "/user-guide/Compaction/#handling-late-records", 
            "text": "Late records are records that arrived at a folder after compaction on this folder has started. We explain how Gobblin handles late records using the following example.  In this use case, both hourly compaction and daily compaction need a mechanism to handle late records. For hourly compaction, late records are records that arrived at an  hourly_staging  folder after the hourly compaction of that folder has started. It is similar for daily compaction.  Compaction with Deduplication  For a compaction with deduplication (i.e., hourly compaction in the above use case), there are two options to deal with late data:   Option 1 : if there are late data, re-do the compaction. For example, you may run the hourly compaction multiple times per hour. The first run will do the normal compaction, and in each subsequent run, if it detects late data in a folder, it will re-do compaction for that folder.   To do so, set  compaction.job.overwrite.output.dir=true  and  compaction.recompact.from.input.for.late.data=true .  Please note the following when you use this option: (1) this means that your already-published data will be re-published if late data are detected; (2) this is potentially dangerous if your input folders have short retention periods. For example, suppose  hourly_staging  folders have a 2-day retention period, i.e., folder  /data/kafka_topics/NewUserEvent/hourly_staging/2015/10/29  will be deleted on 2015/10/31. If, after 2015/10/31, new data arrived at this folder and you re-compact this folder and publish the data to  hourly , all original data will be gone. To avoid this problem you may set  compaction.timebased.max.time.ago=2d  so that compaction will not be performed on a folder more than 2 days ago. However, this means that if a late record is late for more than 2 days, it will never be published into  hourly .   Option 2 : (this is the default option) if there are late data, copy the late data into a  [output_subdir]/_late  folder, e.g., for hourly compaction, late data in  hourly_staging  will be copied to  hourly_late  folders, e.g.,  /data/kafka_topics/NewUserEvent/hourly_late/2015/10/29... .    If re-compaction is not necessary, this is all you need to do. If re-compaction is needed, you may schedule or manually invoke a re-compaction job which will re-compact by consuming data in both  hourly  and  hourly_late . For this job, you need to set  compaction.job.overwrite.output.dir=true  and  compaction.recompact.from.dest.paths=true .  Note that this re-compaction is different from the re-compaction in Option 1: this re-compaction consumes data in output folders (i.e.,  hourly ) whereas the re-compaction in Option 1 consumes data in input folders (i.e.,  hourly_staging ).  Compaction without Deduplication  For a compaction without deduplication, if it is type 2, the same two options above apply. If it is type 1, late data will simply be copied to the output folder.  How to Determine if a Data File is Late  Every time a compaction finishes (except the case below), Gobblin will create a file named  _COMPACTION_COMPLETE  in the compaction output folder. This file contains the timestamp of when the compaction job starts. All files in the input folder with earlier modification timestamps have been compacted. Next time the compaction runs, files in the input folder with later timestamps are considered late data.  The  _COMPACTION_COMPLETE  file will be only be created if it is a regular compaction that consumes input data (including compaction jobs that just copy late data to the output folder or the  [output_subdir]/_late  folder without launching an MR job). It will not be created if it is a re-compaction that consumes output data. This is because whether a file in the input folder is a late file depends on whether it has been compacted or moved into the output folder, which is not affected by a re-compaction that consumes output data.  One way of reducing the chance of seeing late records is to verify data completeness before running compaction, which will be explained next.", 
            "title": "Handling Late Records"
        }, 
        {
            "location": "/user-guide/Compaction/#verifying-data-completeness-before-compaction", 
            "text": "Besides aborting the compaction job for a dataset if new data in the input folder is found, another way to reduce the chance of seeing late events is to verify the completeness of input data before running compaction. To do so, set  compaction.completeness.verification.enabled=true , extend  DataCompletenessVerifier.AbstractRunner  and put in your verification logic, and pass it via  compaction.completeness.verification.class .  When data completeness verification is enabled,  MRCompactor  will verify data completeness for the input datasets, and meanwhile speculatively start the compaction MR jobs. When the compaction MR job for a dataset finishes, if the completeness of the dataset is verified, its compacted data will be published, otherwise it is discarded, and the compaction MR job for this dataset will be launched again with a reduced priority.  It is possible to control which topics should or should not be verified via  compaction.completeness.verification.whitelist  and  compaction.completeness.verification.blacklist . It is also possible to set a timeout for data completeness verification via  compaction.completeness.verification.timeout.minutes . A dataset whose completeness verification timed out can be configured to be either compacted anyway or not compacted.", 
            "title": "Verifying Data Completeness Before Compaction"
        }, 
        {
            "location": "/user-guide/Compaction/#hive-compactor", 
            "text": "The Hive compactor can be used to merge a snapshot with one or multiple deltas. It assumes the snapshot and the deltas meet the following requirements:   Snapshot and all deltas are in Avro format.  Snapshot and all deltas have the same primary key attributes (they do not need to have the same schema).  Snapshot is pulled earlier than all deltas. Therefore if a key appears in both snapshot and deltas, the one in the snapshot should be discarded.  The deltas are pulled one after another, and ordered in ascending order of pull time. If a key appears in both the ith delta and the jth delta (i   j), the one in the jth delta survives.   In the near future we also plan to support selecting records by timestamps (rather than which file they appear). This is useful if the snapshot and the deltas are pulled in parallel, where if a key has multiple occurrences we should keep the one with the latest timestamp.  Note that since delta tables don't have information of deleted records, such information is only available the next time the full snapshot is pulled.", 
            "title": "Hive Compactor"
        }, 
        {
            "location": "/user-guide/Compaction/#usage", 
            "text": "After building Gobblin (i.e.,  ./gradlew clean build ), a zipped file  build/gobblin-compaction/distributions/gobblin-compaction.tar.gz  should be created. It contains a jar file ( gobblin-compaction.jar ), a folder of dependencies ( gobblin-compaction_lib ), and a log4j config file ( log4j.xml ).  To run compaction, extract it into a folder, go to that folder and run   java -jar gobblin-compaction.jar  global-config-file  If for whatever reason (e.g., your Hadoop cluster is in secure mode) you need to run the jar using Hadoop or Yarn, then you first need to make sure the correct log4j config file is used, since there is another log4j config file in the Hadoop classpath. To do so, run the following two commands:  export HADOOP_CLASSPATH=.\nexport HADOOP_USER_CLASSPATH_FIRST=true  The first command adds the current directory to the Hadoop classpath, and the second command tells Hadoop/Yarn to prioritize user's classpath. Then you can run the compaction jar:  hadoop jar gobblin-compaction.jar  global-config-file  or  yarn jar gobblin-compaction.jar  global-config-file  The merged data will be written to the HDFS directory specified in  output.datalocation , as one or more Avro files. The schema of the output data will be the same as the schema of the last delta (which is the last pulled data and thus has the latest schema).  The provided log4j config file ( log4j.xml ) prints logs from Gobblin compaction classes to the console, and writes logs from other classes (e.g., Hive classes) to logs/gobblin-compaction.log. Note that for drop table queries ( DROP TABLE IF EXISTS  tablename ), the Hive JDBC client will throw  NoSuchObjectException  if the table doesn't exist. This is normal and such exceptions should be ignored.", 
            "title": "Usage"
        }, 
        {
            "location": "/user-guide/Compaction/#global-config-properties-example-compactionproperties", 
            "text": "(1) Required:   compaction.config.dir   This is the the compaction jobconfig directory. Each file in this directory should be a jobconfig file (described in the next section).  (2) Optional:   hadoop.configfile. *   Hadoop configuration files that should be loaded\n(e.g., hadoop.configfile.coresite.xml=/export/apps/hadoop/latest/etc/hadoop/core-site.xml)   hdfs.uri   If property  fs.defaultFS  (or  fs.default.name ) is specified in the hadoop config file, then this property is not needed. However, if it is specified, it will override  fs.defaultFS  (or  fs.default.name ).  If  fs.defaultFS  or  fs.default.name  is not specified in the hadoop config file, and this property is also not specified, then the default value \"hdfs://localhost:9000\" will be used.   hiveserver.version  (default: 2)   Either 1 or 2.    hiveserver.connection.string    hiveserver.url    hiveserver.user  (default: \"\")    hiveserver.password  (default: \"\")    If  hiveserver.connection.string  is specified, it will be used to connect to hiveserver.  If  hiveserver.connection.string  is not specified but  hiveserver.url  is specified, then it uses ( hiveserver.url ,  hiveserver.user ,  hiveserver.password ) to connect to hiveserver.  If neither  hiveserver.connection.string  nor  hiveserver.url  is specified, then embedded hiveserver will be used (i.e.,  jdbc:hive://  if  hiveserver.version=1 ,  jdbc:hive2://  if  hiveserver.version=2 )   hivesite.dir   Directory that contains hive-site.xml, if hive-site.xml should be loaded.   hive. *   Any hive config property. (e.g.,  hive.join.cache.size ). If specified, it will override the corresponding property in hive-site.xml.", 
            "title": "Global Config Properties (example: compaction.properties)"
        }, 
        {
            "location": "/user-guide/Compaction/#job-config-properties-example-jobconftask1conf", 
            "text": "(1) Required:   snapshot.pkey   comma separated primary key attributes of the snapshot table   snapshot.datalocation   snapshot data directory in HDFS   delta.i.pkey  (i = 1, 2...)   the primary key of ith delta table\n(the primary key of snapshot and all deltas should be the same)   delta.i.datalocation  (i = 1, 2...)   ith delta table's data directory in HDFS   output.datalocation   the HDFS data directory for the output\n(make sure you have write permission on this directory)  (2) Optional:   snapshot.name  (default: randomly generated name)   prefix name of the snapshot table. The table name will be snapshot.name + random suffix   snapshot.schemalocation   snapshot table's schema location in HDFS. If not specified, schema will be extracted from the data.   delta.i.name  (default: randomly generated name)   prefix name of the ith delta table. The table name will be delta.i.name + random suffix   delta.i.schemalocation   ith delta table's schema location in HDFS. If not specified, schema will be extracted from the data.   output.name  (default: randomly generated name)   prefix name of the output table. The table name will be output.name + random suffix   hive.db.name  (default: default)   the database name to be used. This database should already exist, and you should have write permission on it.   hive.queue.name  (default: default)   queue name to be used.   hive.use.mapjoin  (default: if not specified in the global config file, then false)   whether map-side join should be turned on. If specified both in this property and in the global config file (hive.*), this property takes precedences.    hive.mapjoin.smalltable.filesize  (default: if not specified in the global config file, then use Hive's default value)   if hive.use.mapjoin = true, mapjoin will be used if the small table size is smaller than hive.mapjoin.smalltable.filesize (in bytes).\nIf specified both in this property and in the global config file (hive.*), this property takes precedences.    hive.tmpschema.dir  (default: the parent dir of the data location dir where the data is used to extract the schema)   If we need to extract schema from data, this dir is for the extracted schema.\nNote that if you do not have write permission on the default dir, you must specify this property as a dir where you do have write permission.   snapshot.copydata  (default: false)   Set to true if you don't want to (or are unable to) create external table on snapshot.datalocation. A copy of the snapshot data will be created in  hive.tmpdata.dir , and will be removed after the compaction.  This property should be set to true if either of the following two situations applies:  (i) You don't have write permission to  snapshot.datalocation . If so, once you create an external table on  snapshot.datalocation , you may not be able to drop it. This is a Hive bug and for more information, see  this page , which includes a Hive patch for the bug.  (ii) You want to use a certain subset of files in  snapshot.datalocation  (e.g.,  snapshot.datalocation  contains both .csv and .avro files but you only want to use .avro files)   delta.i.copydata  (i = 1, 2...) (default: false)   Similar as  snapshot.copydata   hive.tmpdata.dir  (default: \"/\")   If  snapshot.copydata  = true or  delta.i.copydata  = true, the data will be copied to this dir. You should have write permission to this dir.   snapshot.dataformat.extension.name  (default: \"\")   If  snapshot.copydata  = true, then only those data files whose extension is  snapshot.dataformat  will be moved to  hive.tmpdata.dir .   delta.i.dataformat.extension.name  (default: \"\")   Similar as  snapshot.dataformat.extension.name .    mapreduce.job.num.reducers   Number of reducers for the job.   timing.file  (default: time.txt)   A file where the running time of each compaction job is printed.", 
            "title": "Job Config Properties (example: jobconf/task1.conf)"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nManaging Watermarks in a Job\n\n\nBasics\n\n\nTask Failures\n\n\nMulti-Dataset Jobs\n\n\n\n\n\n\nGobblin State Deep Dive\n\n\nState class hierarchy\n\n\nHow States are Used in a Gobblin Job\n\n\n\n\n\n\n\n\n\n\nThis page has two parts. Section 1 is an instruction on how to carry over checkpoints between two runs of a scheduled batch ingestion job, so that each run can start at where the previous run left off. Section 2 is a deep dive of different types of states in Gobblin and how they are used in a typical job run.\n\n\nManaging Watermarks in a Job\n\n\nWhen scheduling a Gobblin job to run in batches and pull data incrementally, each run, upon finishing its tasks, should check in the state of its work into the state store, so that the next run can continue the work based on the previous run. This is done through a concept called Watermark.\n\n\nBasics\n\n\nlow watermark and expected high watermark\n\n\nWhen the \nSource\n creates \nWorkUnit\ns, each \nWorkUnit\n should generally contain a low watermark and an expected high watermark. They are the start and finish points for the corresponding task, and the task is expected to pull the data from the low watermark to the expected high watermark. \n\n\nactual high watermark\n\n\nWhen a task finishes extracting data, it should write the actual high watermark into its \nWorkUnitState\n. To do so, the \nExtractor\n may maintain a \nnextWatermark\n field, and in \nExtractor.close()\n, call \nthis.workUnitState.setActualHighWatermark(this.nextWatermark)\n. The actual high Watermark is normally the same as the expected high Watermark if the task completes successfully, and may be smaller than the expected high Watermark if the task failed or timed-out. In some cases, the expected high watermark may not be available so the actual high watermark is the only stat available that tells where the previous run left off. \n\n\nIn the next run, the \nSource\n will call \nSourceState.getPreviousWorkUnitStates()\n which should contain the actual high watermarks the last run checked in, to be used as the low watermarks of the new run.\n\n\nwatermark type\n\n\nA watermark can be of any custom type by implementing the \nWatermark\n interface. For example, for Kafka-HDFS ingestion, if each \nWorkUnit\n is responsible for pulling a single Kafka topic partition, a watermark is a single \nlong\n value representing a Kafka offset. If each \nWorkUnit\n is responsible for pulling multiple Kafka topic partitions, a watermark can be a list of \nlong\n values, such as \nMultiLongWatermark\n.\n\n\nTask Failures\n\n\nA task may pull some data and then fail. If a task fails and job commit policy specified by configuration property \njob.commit.policy\n is set to \nfull\n, the data it pulled won't be published. In this case, it doesn't matter what value \nExtractor.nextWatermark\n is, the actual high watermark will be automatically rolled back to the low watermark by Gobblin internally. On the other hand, if the commit policy is set to \npartial\n, the failed task may get committed and the data may get published. In this case the \nExtractor\n is responsible for setting the correct actual high watermark in \nExtractor.close()\n. Therefore, it is recommended that the \nExtractor\n update \nnextWatermark\n every time it pulls a record, so that \nnextWatermark\n is always up to date (unless you are OK with the next run re-doing the work which may cause some data to be published twice).\n\n\nMulti-Dataset Jobs\n\n\nCurrently the only state store implementation Gobblin provides is \nFsStateStore\n which uses Hadoop SequenceFiles to store the states. By default, each job run reads the SequenceFile created by the previous run, and generates a new SequenceFile. This creates a pitfall when a job pulls data from multiple datasets: if a data set is skipped in a job run for whatever reason (e.g., it is blacklisted), its watermark will be unavailable for the next run.\n\n\nExample\n: suppose we schedule a Gobblin job to pull a Kafka topic from a Kafka broker, which has 10 partitions. In this case each partition is a dataset. In one of the job runs, a partition is skipped due to either being blacklisted or some failure. If no \nWorkUnit\n is created for this partition, this partition's watermark will not be checked in to the state store, and will not be available for the next run.\n\n\nThe are two solutions to the above problem (three if you count the one that implements a different state store that behaves differently and doesn't have this problem).\n\n\nSolution 1\n: make sure to create a \nWorkUnit\n for every dataset. Even if a dataset should be skipped, an empty \nWorkUnit\n should still be created for the dataset ('empty' means low watermark = expected high watermark).\n\n\nSolution 2\n: use Dataset URNs. When a job pulls multiple datasets, the \nSource\n class may define a URN for each dataset, e.g., we may use \nPageViewEvent.5\n as the URN of the 5th partition of topic \nPageViewEvent\n. When the \nSource\n creates the \nWorkUnit\n for this partition, it should set property \ndataset.urn\n in this \nWorkUnit\n with value \nPageViewEvent.5\n. This is the solution gobblin current uses to support jobs pulling data for multiple datasets.\n\n\nIf different \nWorkUnit\ns have different values of \ndataset.urn\n, the job will create one state store SequenceFile for each \ndataset.urn\n. In the next run, instead of calling \nSourceState.getPreviousWorkUnitStates()\n, one should use \nSourceState.getPreviousWorkUnitStatesByDatasetUrns()\n. In this way, each run will look for the most recent state store SequenceFile for each dataset, and therefore, even if a dataset is not processed by a job run, its watermark won't be lost.\n\n\nNote that when using Dataset URNs, \neach \nWorkUnit\n can only have one \ndataset.urn\n, which means, for example, in the Kafka ingestion case, each \nWorkUnit\n can only process one partition. This is usually not a big problem except that it may output too many small files (as explained in \nKafka HDFS ingestion\n, by having a \nWorkUnit\n pull multiple partitions of the same topic, these partitions can share output files). On the other hand, different \nWorkUnit\ns may have the same \ndataset.urn\n.\n\n\nGobblin State Deep Dive\n\n\nGobblin involves several types of states during a job run, such as \nJobState\n, \nTaskState\n, \nWorkUnit\n, etc. They all extend the \nState\n class, which is a wrapper around \nProperties\n and provides some useful utility functions. \n\n\nState\n class hierarchy\n\n\n\n  \n    \n    \n\n  \n\n\n\n\n\n\n\nSourceState\n, \nJobState\n and \nDatasetState\n: \nSourceState\n contains properties that define the current job run. It contains properties in the job config file, and the states the previous run persisted in the state store. It is passed to Source to create \nWorkUnit\ns.\n\n\n\n\nCompared to \nSourceState\n, a \nJobState\n also contains properties of a job run such as job ID, starting time, end time, etc., as well as status of a job run, e.g, \nPENDING\n, \nRUNNING\n, \nCOMMITTED\n, \nFAILED\n, etc.\n\n\nWhen the data pulled by a job is separated into different datasets (by using \ndataset.urn\n explained above), each dataset will have a \nDatasetState\n object in the JobState, and each dataset will persist its states separately.\n\n\n\n\nWorkUnit\n and \nMultiWorkUnit\n: A \nWorkUnit\n defines a unit of work. It may contain properties such as which data set to be pulled, where to start (low watermark), where to finish (expected high watermark), among others. A \nMultiWorkUnit\n contains one or more \nWorkUnit\ns. All \nWorkUnit\ns in a \nMultiWorkUnit\n will be run by a single Task.\n\n\n\n\nThe \nMultiWorkUnit\n is useful for finer-grained control and load balancing. Without \nMultiWorkUnit\ns, if the number of \nWorkUnit\ns exceeds the number of mappers in the MR mode, the job launcher can only balance the number of \nWorkUnit\ns in the mappers. If different \nWorkUnit\ns have very different workloads (e.g., some pull from very large partitions and others pull from small partitions), this may lead to mapper skew. With \nMultiWorkUnit\n, if the \nSource\n class knows or can estimate the workload of the \nWorkUnit\ns, it can pack a large number of \nWorkUnit\ns into a smaller number of \nMultiWorkUnit\ns using its own logic, achieving better load balancing.\n\n\n\n\n\n\nWorkUnitState\n and \nTaskState\n: A \nWorkUnitState\n contains the runtime properties of a \nWorkUnit\n, e.g., actual high watermark, as well as the status of a WorkUnit, e.g., \nPENDING\n, \nRUNNING\n, \nCOMMITTED\n, \nFAILED\n, etc. A \nTaskState\n additionally contains properties of a Task that runs a \nWorkUnit\n, e.g., task ID, start time, end time, etc.\n\n\n\n\n\n\nExtract\n: \nExtract\n is mainly used for ingesting from databases. It contains properties such as job type (snapshot-only, append-only, snapshot-append), primary keys, delta fields, etc.\n\n\n\n\n\n\nHow States are Used in a Gobblin Job\n\n\n\n\n\n\nWhen a job run starts, the job launcher first creates a \nJobState\n, which contains (1) all properties specified in the job config file, and (2) the \nJobState\n / \nDatasetState\n of the previous run, which contains, among other properties, the actual high watermark the previous run checked in for each of its tasks / datasets.\n\n\n\n\n\n\nThe job launcher then passes the \nJobState\n (as a \nSourceState\n object) to the \nSource\n, based on which the \nSource\n will create a set of \nWorkUnit\ns. Note that when creating \nWorkUnit\ns, the \nSource\n should not add properties in \nSourceState\n into the \nWorkUnit\ns, which will be done when each \nWorkUnit\n is executed in a \nTask\n. The reason is that since the job launcher runs in a single JVM, creating a large number of \nWorkUnit\ns, each containing a copy of the \nSourceState\n, may cause OOM.\n\n\n\n\n\n\nThe job launcher prepares to run the \nWorkUnit\ns.\n\n\n\n\nIn standalone mode, the job launcher will add properties in the \nJobState\n into each \nWorkUnit\n (if a property in \nJobState\n already exists in the \nWorkUnit\n, it will NOT be overwritten, i.e., the value in the \nWorkUnit\n takes precedence). Then for each \nWorkUnit\n it creates a \nTask\n to run the \nWorkUnit\n, and submits all these Tasks to a \nTaskExecutor\n which will run these \nTask\ns in a thread pool.\n\n\nIn MR mode, the job launcher will serialize the \nJobState\n and each \nWorkUnit\n into a file, which will be picked up by the mappers. It then creates, configures and submits a Hadoop job.\n\n\n\n\nAfter this step, the job launcher will be waiting till all tasks finish.\n\n\n\n\n\n\nEach \nTask\n corresponding to a \nWorkUnit\n contains a \nTaskState\n. The \nTaskState\n initially contains all properties in \nJobState\n and the corresponding \nWorkUnit\n, and during the Task run, more runtime properties can be added to \nTaskState\n by \nExtractor\n, \nConverter\n and \nWriter\n, such as the actual high watermark explained in Section 1.\n\n\n\n\n\n\nAfter all \nTask\ns finish, \nDatasetState\ns will be created from all \nTaskState\ns based on the \ndataset.urn\n specified in the \nWorkUnit\ns. For each dataset whose data is committed, the job launcher will persist its \nDatasetState\n. If no \ndataset.urn\n is specified, there will be a single DatasetState, and thus the DatasetState will be persisted if either all \nTask\ns successfully committed, or some task failed but the commit policy is set to \npartial\n, in which case the watermarks of these failed tasks will be rolled back, as explained in Section 1.", 
            "title": "State Management and Watermarks"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/#table-of-contents", 
            "text": "Table of Contents  Managing Watermarks in a Job  Basics  Task Failures  Multi-Dataset Jobs    Gobblin State Deep Dive  State class hierarchy  How States are Used in a Gobblin Job      This page has two parts. Section 1 is an instruction on how to carry over checkpoints between two runs of a scheduled batch ingestion job, so that each run can start at where the previous run left off. Section 2 is a deep dive of different types of states in Gobblin and how they are used in a typical job run.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/#managing-watermarks-in-a-job", 
            "text": "When scheduling a Gobblin job to run in batches and pull data incrementally, each run, upon finishing its tasks, should check in the state of its work into the state store, so that the next run can continue the work based on the previous run. This is done through a concept called Watermark.", 
            "title": "Managing Watermarks in a Job"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/#basics", 
            "text": "low watermark and expected high watermark  When the  Source  creates  WorkUnit s, each  WorkUnit  should generally contain a low watermark and an expected high watermark. They are the start and finish points for the corresponding task, and the task is expected to pull the data from the low watermark to the expected high watermark.   actual high watermark  When a task finishes extracting data, it should write the actual high watermark into its  WorkUnitState . To do so, the  Extractor  may maintain a  nextWatermark  field, and in  Extractor.close() , call  this.workUnitState.setActualHighWatermark(this.nextWatermark) . The actual high Watermark is normally the same as the expected high Watermark if the task completes successfully, and may be smaller than the expected high Watermark if the task failed or timed-out. In some cases, the expected high watermark may not be available so the actual high watermark is the only stat available that tells where the previous run left off.   In the next run, the  Source  will call  SourceState.getPreviousWorkUnitStates()  which should contain the actual high watermarks the last run checked in, to be used as the low watermarks of the new run.  watermark type  A watermark can be of any custom type by implementing the  Watermark  interface. For example, for Kafka-HDFS ingestion, if each  WorkUnit  is responsible for pulling a single Kafka topic partition, a watermark is a single  long  value representing a Kafka offset. If each  WorkUnit  is responsible for pulling multiple Kafka topic partitions, a watermark can be a list of  long  values, such as  MultiLongWatermark .", 
            "title": "Basics"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/#task-failures", 
            "text": "A task may pull some data and then fail. If a task fails and job commit policy specified by configuration property  job.commit.policy  is set to  full , the data it pulled won't be published. In this case, it doesn't matter what value  Extractor.nextWatermark  is, the actual high watermark will be automatically rolled back to the low watermark by Gobblin internally. On the other hand, if the commit policy is set to  partial , the failed task may get committed and the data may get published. In this case the  Extractor  is responsible for setting the correct actual high watermark in  Extractor.close() . Therefore, it is recommended that the  Extractor  update  nextWatermark  every time it pulls a record, so that  nextWatermark  is always up to date (unless you are OK with the next run re-doing the work which may cause some data to be published twice).", 
            "title": "Task Failures"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/#multi-dataset-jobs", 
            "text": "Currently the only state store implementation Gobblin provides is  FsStateStore  which uses Hadoop SequenceFiles to store the states. By default, each job run reads the SequenceFile created by the previous run, and generates a new SequenceFile. This creates a pitfall when a job pulls data from multiple datasets: if a data set is skipped in a job run for whatever reason (e.g., it is blacklisted), its watermark will be unavailable for the next run.  Example : suppose we schedule a Gobblin job to pull a Kafka topic from a Kafka broker, which has 10 partitions. In this case each partition is a dataset. In one of the job runs, a partition is skipped due to either being blacklisted or some failure. If no  WorkUnit  is created for this partition, this partition's watermark will not be checked in to the state store, and will not be available for the next run.  The are two solutions to the above problem (three if you count the one that implements a different state store that behaves differently and doesn't have this problem).  Solution 1 : make sure to create a  WorkUnit  for every dataset. Even if a dataset should be skipped, an empty  WorkUnit  should still be created for the dataset ('empty' means low watermark = expected high watermark).  Solution 2 : use Dataset URNs. When a job pulls multiple datasets, the  Source  class may define a URN for each dataset, e.g., we may use  PageViewEvent.5  as the URN of the 5th partition of topic  PageViewEvent . When the  Source  creates the  WorkUnit  for this partition, it should set property  dataset.urn  in this  WorkUnit  with value  PageViewEvent.5 . This is the solution gobblin current uses to support jobs pulling data for multiple datasets.  If different  WorkUnit s have different values of  dataset.urn , the job will create one state store SequenceFile for each  dataset.urn . In the next run, instead of calling  SourceState.getPreviousWorkUnitStates() , one should use  SourceState.getPreviousWorkUnitStatesByDatasetUrns() . In this way, each run will look for the most recent state store SequenceFile for each dataset, and therefore, even if a dataset is not processed by a job run, its watermark won't be lost.  Note that when using Dataset URNs,  each  WorkUnit  can only have one  dataset.urn , which means, for example, in the Kafka ingestion case, each  WorkUnit  can only process one partition. This is usually not a big problem except that it may output too many small files (as explained in  Kafka HDFS ingestion , by having a  WorkUnit  pull multiple partitions of the same topic, these partitions can share output files). On the other hand, different  WorkUnit s may have the same  dataset.urn .", 
            "title": "Multi-Dataset Jobs"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/#gobblin-state-deep-dive", 
            "text": "Gobblin involves several types of states during a job run, such as  JobState ,  TaskState ,  WorkUnit , etc. They all extend the  State  class, which is a wrapper around  Properties  and provides some useful utility functions.", 
            "title": "Gobblin State Deep Dive"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/#state-class-hierarchy", 
            "text": "SourceState ,  JobState  and  DatasetState :  SourceState  contains properties that define the current job run. It contains properties in the job config file, and the states the previous run persisted in the state store. It is passed to Source to create  WorkUnit s.   Compared to  SourceState , a  JobState  also contains properties of a job run such as job ID, starting time, end time, etc., as well as status of a job run, e.g,  PENDING ,  RUNNING ,  COMMITTED ,  FAILED , etc.  When the data pulled by a job is separated into different datasets (by using  dataset.urn  explained above), each dataset will have a  DatasetState  object in the JobState, and each dataset will persist its states separately.   WorkUnit  and  MultiWorkUnit : A  WorkUnit  defines a unit of work. It may contain properties such as which data set to be pulled, where to start (low watermark), where to finish (expected high watermark), among others. A  MultiWorkUnit  contains one or more  WorkUnit s. All  WorkUnit s in a  MultiWorkUnit  will be run by a single Task.   The  MultiWorkUnit  is useful for finer-grained control and load balancing. Without  MultiWorkUnit s, if the number of  WorkUnit s exceeds the number of mappers in the MR mode, the job launcher can only balance the number of  WorkUnit s in the mappers. If different  WorkUnit s have very different workloads (e.g., some pull from very large partitions and others pull from small partitions), this may lead to mapper skew. With  MultiWorkUnit , if the  Source  class knows or can estimate the workload of the  WorkUnit s, it can pack a large number of  WorkUnit s into a smaller number of  MultiWorkUnit s using its own logic, achieving better load balancing.    WorkUnitState  and  TaskState : A  WorkUnitState  contains the runtime properties of a  WorkUnit , e.g., actual high watermark, as well as the status of a WorkUnit, e.g.,  PENDING ,  RUNNING ,  COMMITTED ,  FAILED , etc. A  TaskState  additionally contains properties of a Task that runs a  WorkUnit , e.g., task ID, start time, end time, etc.    Extract :  Extract  is mainly used for ingesting from databases. It contains properties such as job type (snapshot-only, append-only, snapshot-append), primary keys, delta fields, etc.", 
            "title": "State class hierarchy"
        }, 
        {
            "location": "/user-guide/State-Management-and-Watermarks/#how-states-are-used-in-a-gobblin-job", 
            "text": "When a job run starts, the job launcher first creates a  JobState , which contains (1) all properties specified in the job config file, and (2) the  JobState  /  DatasetState  of the previous run, which contains, among other properties, the actual high watermark the previous run checked in for each of its tasks / datasets.    The job launcher then passes the  JobState  (as a  SourceState  object) to the  Source , based on which the  Source  will create a set of  WorkUnit s. Note that when creating  WorkUnit s, the  Source  should not add properties in  SourceState  into the  WorkUnit s, which will be done when each  WorkUnit  is executed in a  Task . The reason is that since the job launcher runs in a single JVM, creating a large number of  WorkUnit s, each containing a copy of the  SourceState , may cause OOM.    The job launcher prepares to run the  WorkUnit s.   In standalone mode, the job launcher will add properties in the  JobState  into each  WorkUnit  (if a property in  JobState  already exists in the  WorkUnit , it will NOT be overwritten, i.e., the value in the  WorkUnit  takes precedence). Then for each  WorkUnit  it creates a  Task  to run the  WorkUnit , and submits all these Tasks to a  TaskExecutor  which will run these  Task s in a thread pool.  In MR mode, the job launcher will serialize the  JobState  and each  WorkUnit  into a file, which will be picked up by the mappers. It then creates, configures and submits a Hadoop job.   After this step, the job launcher will be waiting till all tasks finish.    Each  Task  corresponding to a  WorkUnit  contains a  TaskState . The  TaskState  initially contains all properties in  JobState  and the corresponding  WorkUnit , and during the Task run, more runtime properties can be added to  TaskState  by  Extractor ,  Converter  and  Writer , such as the actual high watermark explained in Section 1.    After all  Task s finish,  DatasetState s will be created from all  TaskState s based on the  dataset.urn  specified in the  WorkUnit s. For each dataset whose data is committed, the job launcher will persist its  DatasetState . If no  dataset.urn  is specified, there will be a single DatasetState, and thus the DatasetState will be persisted if either all  Task s successfully committed, or some task failed but the commit policy is set to  partial , in which case the watermarks of these failed tasks will be rolled back, as explained in Section 1.", 
            "title": "How States are Used in a Gobblin Job"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nOverview of the ForkOperator\n\n\nUsing the ForkOperator\n\n\nBasics of Usage\n\n\nPer-Fork Configuration\n\n\nFailure Semantics\n\n\nPerformance Tuning\n\n\nComparison with PartitionedDataWriter\n\n\n\n\n\n\nWriting your Own ForkOperator\n\n\nBest Practices\n\n\nExample\n\n\n\n\n\n\nOverview of the ForkOperator\n\n\nThe \nForkOperator\n is a type of control operators that allow a task flow to branch into multiple streams (or forked branches) as represented by a \nFork\n, each of which goes to a separately configured sink with its own data writer. The \nForkOperator\n gives users more flexibility in terms of controlling where and how ingested data should be output. This is useful for situations, e.g., that data records need to be written into multiple different storages, or that data records need to be written out to the same storage (say, HDFS) but in different forms for different downstream consumers. The best practices of using the \nForkOperator\n that we recommend, though, are discussed below. The diagram below illustrates how the \nForkOperator\n in a Gobblin task flow allows an input stream to be forked into multiple output streams, each of which can have its own converters, quality checkers, and writers.\n\n\n\n  \n\n    \n\n    \nGobblin Task Flow\n\n  \n\n\n\n\n\nUsing the ForkOperator\n\n\nBasics of Usage\n\n\nThe \nForkOperator\n, like most other operators in a Gobblin task flow, is pluggable through the configuration, or more specifically , the configuration property \nfork.operator.class\n that points to a class that implements the \nForkOperator\n interface. For instance:\n\n\nfork.operator.class=gobblin.fork.IdentityForkOperator\n\n\n\n\nBy default, if no \nForkOperator\n class is specified, internally Gobblin uses the default implementation \nIdentityForkOperator\n with a single forked branch (although it does supports multiple forked branches). The \nIdentityForkOperator\n simply unconditionally forwards the schema and ingested data records to all the forked branches, the number of which is specified through the configuration property \nfork.branches\n with a default value of 1. When an \nIdentityForkOperator\n instance is initialized, it will read the value of \nfork.branches\n and use that as the return value of \ngetBranches\n.   \n\n\nThe \nexpected\n number of forked branches is given by the method \ngetBranches\n of the \nForkOperator\n. This number must match the size of the list of \nBoolean\ns returned by \nforkSchema\n as well as the size of the list of \nBoolean\ns returned by \nforkDataRecords\n. Otherwise, \nForkBranchMismatchException\n will be thrown. Note that the \nForkOperator\n itself \nis not making and returning a copy\n for the input schema and data records, but rather just providing a \nBoolean\n for each forked branch telling if the schema or data records should be in each particular branch. Each forked branch has a branch index starting at 0. So if there are three forked branches, the branches will have indices 0, 1, and 2, respectively. Branch indices are useful to tell which branch the Gobblin task flow is in. Each branch also has a name associated with it that can be specified using the configuration property \nfork.branch.name.\nbranch index\n. Note that the branch index is added as a suffix to the property name in this case. More on this later. If the user does not specify a name for the branches, the names in the form \nfork_\nbranch index\n will be used.   \n\n\nThe use of the \nForkOperator\n with \nthe possibility that the schema and/or data records may be forwarded to more than one forked branches\n has some special requirement on the input schema and data records to the \nForkOperator\n. Specifically, because the same schema or data records may be forwarded to more than branches that may alter the schema or data records in place, it is necessary for the Gobblin task flow to make a copy of the input schema or data records for each forked branch so any modification within one branch won't affect any other branches. \n\n\nTo guarantee that it is always able to make a copy in such a case, Gobblin requires the input schema and data records to be of type \nCopyable\n when there are more than one forked branch. \nCopyable\n is an interface that defines a method \ncopy\n for making a copy of an instance of a given type. The Gobblin task flow will check if the input schema and data records are instances of \nCopyable\n and throw a \nCopyNotSupportedException\n if not. This check is performed independently on the schema first and data records subsequently. Note that this requirement is enforced \nif and only if the schema or data records are to be forwarded to more than one branches\n, which is the case if \nforkSchema\n or \nforkDataRecord\n returns a list containing more than one \nTRUE\n. Having more than one branch does not necessarily mean the schema and/or data records need to be \nCopyable\n.\n\n\nGobblin ships with some built-in \nCopyable\n implementations, e.g., \nCopyableSchema\n and \nCopyableGenericRecord\n for Avro's \nSchema\n and \nGenericRecord\n.   \n\n\nPer-Fork Configuration\n\n\nSince each forked branch may have it's own converters, quality checkers, and writers, in addition to the ones in the pre-fork stream (which does not have a writer apparently), there must be a way to tell the converter, quality checker, and writer classes of one branch from another and from the pre-fork stream. Gobblin uses a pretty straightforward approach: if a configuration property is used to specify something for a branch in a multi-branch use case, \nthe branch index should be appended as a suffix\n to the property name. The original configuration name without the suffix is \ngenerally reserved for the pre-fork stream\n. For example, \nconverter.classes.0\n and \nconverter.classes.1\n are used to specify the list of converter classes for branch 0 and 1, respectively, whereas \nconverter.classes\n is reserved for the pre-fork stream. If there's only a single branch (the default case), then the index suffix is not applicable. Without being a comprehensive list, the following groups of built-in configuration properties may be used with branch indices as suffices to specify things for forked branches:\n\n\n\n\nConverter configuration properties: configuration properties whose names start with \nconverter\n.\n\n\nQuality checker configuration properties: configuration properties whose names start with \nqualitychecker\n.\n\n\nWriter configuration properties: configuration properties whose names start with \nwriter\n.\n\n\n\n\nFailure Semantics\n\n\nIn a normal task flow where the default \nIdentityForkOperator\n with a single branch is used, the failure of the single branch also means the failure of the task flow. When there are more than one forked branch, however, the failure semantics are more involved. Gobblin uses the following failure semantics in this case: \n\n\n\n\nThe failure of any forked branch means the failure of the whole task flow, i.e., the task succeeds if and only if all the forked branches succeed.\n\n\nA forked branch stops processing any outstanding incoming data records in the queue if it fails in the middle of processing the data.   \n\n\nThe failure and subsequent stop/completion of any forked branch does not prevent other branches from processing their copies of the ingested data records. The task will wait until all the branches to finish, regardless if they succeed or fail.   \n\n\nThe commit of output data of forks is determined by the job commit policy (see \nJobCommitPolicy\n) specified. If \nJobCommitPolicy.COMMIT_ON_FULL_SUCCESS\n (or \nfull\n in short) is used, the output data of the entire job will be discarded if any forked branch fails, which will fail the task and consequently the job. If instead \nJobCommitPolicy.COMMIT_SUCCESSFUL_TASKS\n (or \nsuccessful\n in short) is used, output data of tasks whose forked branches all succeed will be committed. Output data of any task that has \nat least one failed forked branch\n will not be committed since the the task is considered failed in this case. This also means output data of the successful forked branches of the task won't be committed either.\n\n\n\n\nPerformance Tuning\n\n\nInternally, each forked branch as represented by a \nFork\n maintains a bounded record queue (implemented by \nBoundedBlockingRecordQueue\n), which serves as a buffer between the pre-fork stream and the forked stream of the particular branch. The size if this bounded record queue can be configured through the property \nfork.record.queue.capacity\n. A larger queue allows for more data records to be buffered therefore giving the producer (the pre-fork stream) more head room to move forward. On the other hand, a larger queue requires more memory. The bounded record queue imposes a timeout time on all blocking operations such as putting a new record to the tail and polling a record off the head of the queue. Tuning the queue size and timeout time together offers a lot of flexibility and a tradeoff between queuing performance vs. memory consumption.\n\n\nIn terms of the number of forked branches, we have seen use cases with a half dozen forked branches, and we are anticipating uses cases with much larger numbers. Again, when using a large number of forked branches, the size of the record queues and the timeout time need to be carefully tuned. \n\n\nThe \nBoundedBlockingRecordQueue\n in each \nFork\n keeps trach of the following queue statistics that can be output to the logs if the \nDEBUG\n logging level is turned on. Those statistics provide good indications on the performance of the forks.\n\n\n\n\nQueue size, i.e., the number of records in queue.\n\n\nQueue fill ratio, i.e., a ratio of the number of records in queue over the queue capacity.\n\n\nPut attempt rate (per second).\n\n\nTotal put attempt count.\n\n\nGet attempt rate (per second).\n\n\nTotal get attempt count. \n\n\n\n\nComparison with PartitionedDataWriter\n\n\nGobblin ships with a special type of \nDataWriter\ns called \nPartitionedDataWriter\n that allow ingested records to be written in a partitioned fashion using a \nWriterPartitioner\n into different locations in the same sink. The \nWriterPartitioner\n determines the specific partition for each data record. So there's certain overlap in terms of functionality between the \nForkOperator\n and \nPartitionedDataWriter\n. The question is which one should be used under which circumstances? Below is a summary of the major differences between the two operators.\n\n\n\n\nThe \nForkOperator\n requires the number of forked branches to be known and returned through \ngetBranches\n before the task starts, whereas the \nPartitionedDataWriter\n does not have this requirement.\n\n\nThe \nPartitionedDataWriter\n writes each data record to a single partition, whereas the \nForkOperator\n allows data records to be forwarded to any number of forked branches.\n\n\nThe \nForkOperator\n allows the use of additional converters and quality checkers in any forked branches before data gets written out. The \nPartitionedDataWriter\n is the last operator in a task flow.\n\n\nUse of the \nForkOperator\n allows data records to be written to different sinks, whereas the \nPartitionedDataWriter\n is not capable of doing this.\n\n\nThe \nPartitionedDataWriter\n writes data records sequentially in a single thread, whereas use of the \nForkOperator\n allows forked branches to write independently in parallel since \nFork\ns are executed in a thread pool.  \n\n\n\n\nWriting your Own ForkOperator\n\n\nSince the built-in default implementation \nIdentityForkOperator\n simply blindly forks the input schema and data records to every branches, it's often necessary to have a custom implementation of the \nForkOperator\n interface for more fine-grained control on the actual branching. Checkout the interface \nForkOperator\n for the methods that need to be implemented. You will also find the \nForkOperatorUtils\n to be handy when writing your own \nForkOperator\n implementations.\n\n\nBest Practices\n\n\nThe \nForkOperator\n can have many potential use cases and we have seen the following common ones:\n\n\n\n\nUsing a \nForkOperator\n to write the same ingested data to multiple sinks, e.g., HDFS and S3, possibly in different formats. This kind of use cases is often referred to as \"dual writes\", which are \ngenerally NOT recommended\n as \"dual writes\" may lead to data inconsistency between the sinks in case of write failures. However, with the failure semantics discussed above, data inconsistency generally should not happen with the job commit policy \nJobCommitPolicy.COMMIT_ON_FULL_SUCCESS\n or \nJobCommitPolicy.COMMIT_SUCCESSFUL_TASKS\n. This is because a failure of any forked branch means the failure of the task and none of the forked branches of the task will have its output data committed, making inconsistent output data between different sinks impossible.  \n\n\nUsing a \nForkOperator\n to process ingested data records in different ways conditionally. For example, a \nForkOperator\n may be used to classify and write ingested data records to different places on HDFS depending on some field in the data that serves as a classifier.\n\n\nUsing a \nForkOperator\n to group ingested data records of a certain schema type in case the incoming stream mixes data records of different schema types. For example, we have seen a use case in which a single Kafka topic is used for records of various schema types and when data gets ingested to HDFS, the records need to be written to different paths according to their schema types.\n\n\n\n\nGenerally, a common use case of the \nForkOperator\n is to route ingested data records so they get written to different output locations \nconditionally\n. The \nForkOperator\n also finds common usage for \"dual writes\" to different sinks potentially in different formats if the job commit policy \nJobCommitPolicy.COMMIT_ON_FULL_SUCCESS\n (or \nfull\n in short) or \nJobCommitPolicy.COMMIT_SUCCESSFUL_TASKS\n (or \nsuccessful\n in short) is used, as explained above. \n\n\nExample\n\n\nLet's take a look at one example that shows how to work with the \nForkOperator\n for a real use case. Say you have a Gobblin job that ingests Avro data from a data source that may have some sensitive data in some of the fields that need to be purged. Depending on if data records have sensitive data, they need to be written to different locations on the same sink, which we assume is HDFS. So essentially the tasks of the job need a mechanism to conditionally write ingested data records to different locations depending if they have sensitive data. The \nForkOperator\n offers a way of implementing this mechanism. \n\n\nIn this particular use case, we need a \nForkOperator\n implementation of two branches that forwards the schema to both branches but each data record to only one of the two branches. The default \nIdentityForkOperator\n cannot be used since it simply forwards every data records to every branches. So we need a custom implementation of the \nForkOperator\n and let's simply call it \nSensitiveDataAwareForkOperator\n under the package \ngobblin.example.fork\n. Let's also assume that branch 0 is for data records with sensitive data, whereas branch 1 is for data records without. Below is a brief sketch of how the implementation looks like:\n\n\npublic class SensitiveDataAwareForkOperator implements ForkOperator\nSchema, GenericRecord\n {\n\n  private static final int NUM_BRANCHES = 2;\n\n  @Override\n  public void init(WorkUnitState workUnitState) {\n  }\n\n  @Override\n  public int getBranches(WorkUnitState workUnitState) {\n    return NUM_BRANCHES;\n  }\n\n  @Override\n  public List\nBoolean\n forkSchema(WorkUnitState workUnitState, Schema schema) {\n    // The schema goes to both branches.\n    return ImmutableList.of(Boolean.TRUE, Boolean.TRUE);\n  }\n\n  @Override\n  public List\nBoolean\n forkDataRecord(WorkUnitState workUnitState, GenericRecord record) {\n    // Data records only go to one of the two branches depending on if they have sensitive data.\n    // Branch 0 is for data records with sensitive data and branch 1 is for data records without.\n    // hasSensitiveData checks the record and returns true of the record has sensitive data and false otherwise.\n    if (hasSensitiveData(record)) {\n      return ImmutableList.of(Boolean.TRUE, Boolean.FALSE)      \n    }\n\n    return ImmutableList.of(Boolean.FALSE, Boolean.TRUE);\n  }\n\n  @Override\n  public void close() throws IOException {\n  }\n}\n\n\n\n\nTo make the example more concrete, let's assume that the job uses some converters and quality checkers before the schema and data records reach the \nSensitiveDataAwareForkOperator\n, and it also uses a converter to purge the sensitive fields and a quality checker that makes sure some mandatory fields exist for purged data records in branch 0. Both branches will be written to the same HDFS but into different locations.\n\n\nfork.operator.class=gobblin.example.fork.SensitiveDataAwareForkOperator\n\n# Pre-fork or non-fork-specific configuration properties\nconverter.classes=\nConverter classes used in the task flow prior to OutlierAwareForkOperator\n\nqualitychecker.task.policies=gobblin.policies.count.RowCountPolicy,gobblin.policies.schema.SchemaCompatibilityPolicy\nqualitychecker.task.policy.types=OPTIONAL,OPTIONAL\ndata.publisher.type=gobblin.publisher.BaseDataPublisher\n\n# Configuration properties for branch 0\nconverter.classes.0=gobblin.example.converter.PurgingConverter\nqualitychecker.task.policies.0=gobblin.example,policies.MandatoryFieldExistencePolicy\nqualitychecker.task.policy.types.0=FAILED\nwriter.fs.uri.0=hdfs://\nnamenode host\n:\nnamenode port\n/\nwriter.destination.type.0=HDFS\nwriter.output.format.0=AVRO\nwriter.staging.dir.0=/gobblin/example/task-staging/purged\nwriter.output.dir.0=/gobblin/example/task-output/purged\ndata.publisher.final.dir.0=/gobblin/example/job-output/purged\n\n# Configuration properties for branch 1\nwriter.fs.uri.1=hdfs://\nnamenode host\n:\nnamenode port\n/\nwriter.destination.type.1=HDFS\nwriter.output.format.1=AVRO\nwriter.staging.dir.1=/gobblin/example/task-staging/normal\nwriter.output.dir.1=/gobblin/example/task-output/normal\ndata.publisher.final.dir.1=/gobblin/example/job-output/normal", 
            "title": "Fork Operator"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#table-of-contents", 
            "text": "Table of Contents  Overview of the ForkOperator  Using the ForkOperator  Basics of Usage  Per-Fork Configuration  Failure Semantics  Performance Tuning  Comparison with PartitionedDataWriter    Writing your Own ForkOperator  Best Practices  Example", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#overview-of-the-forkoperator", 
            "text": "The  ForkOperator  is a type of control operators that allow a task flow to branch into multiple streams (or forked branches) as represented by a  Fork , each of which goes to a separately configured sink with its own data writer. The  ForkOperator  gives users more flexibility in terms of controlling where and how ingested data should be output. This is useful for situations, e.g., that data records need to be written into multiple different storages, or that data records need to be written out to the same storage (say, HDFS) but in different forms for different downstream consumers. The best practices of using the  ForkOperator  that we recommend, though, are discussed below. The diagram below illustrates how the  ForkOperator  in a Gobblin task flow allows an input stream to be forked into multiple output streams, each of which can have its own converters, quality checkers, and writers.  \n   \n     \n     Gobblin Task Flow", 
            "title": "Overview of the ForkOperator"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#using-the-forkoperator", 
            "text": "", 
            "title": "Using the ForkOperator"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#basics-of-usage", 
            "text": "The  ForkOperator , like most other operators in a Gobblin task flow, is pluggable through the configuration, or more specifically , the configuration property  fork.operator.class  that points to a class that implements the  ForkOperator  interface. For instance:  fork.operator.class=gobblin.fork.IdentityForkOperator  By default, if no  ForkOperator  class is specified, internally Gobblin uses the default implementation  IdentityForkOperator  with a single forked branch (although it does supports multiple forked branches). The  IdentityForkOperator  simply unconditionally forwards the schema and ingested data records to all the forked branches, the number of which is specified through the configuration property  fork.branches  with a default value of 1. When an  IdentityForkOperator  instance is initialized, it will read the value of  fork.branches  and use that as the return value of  getBranches .     The  expected  number of forked branches is given by the method  getBranches  of the  ForkOperator . This number must match the size of the list of  Boolean s returned by  forkSchema  as well as the size of the list of  Boolean s returned by  forkDataRecords . Otherwise,  ForkBranchMismatchException  will be thrown. Note that the  ForkOperator  itself  is not making and returning a copy  for the input schema and data records, but rather just providing a  Boolean  for each forked branch telling if the schema or data records should be in each particular branch. Each forked branch has a branch index starting at 0. So if there are three forked branches, the branches will have indices 0, 1, and 2, respectively. Branch indices are useful to tell which branch the Gobblin task flow is in. Each branch also has a name associated with it that can be specified using the configuration property  fork.branch.name. branch index . Note that the branch index is added as a suffix to the property name in this case. More on this later. If the user does not specify a name for the branches, the names in the form  fork_ branch index  will be used.     The use of the  ForkOperator  with  the possibility that the schema and/or data records may be forwarded to more than one forked branches  has some special requirement on the input schema and data records to the  ForkOperator . Specifically, because the same schema or data records may be forwarded to more than branches that may alter the schema or data records in place, it is necessary for the Gobblin task flow to make a copy of the input schema or data records for each forked branch so any modification within one branch won't affect any other branches.   To guarantee that it is always able to make a copy in such a case, Gobblin requires the input schema and data records to be of type  Copyable  when there are more than one forked branch.  Copyable  is an interface that defines a method  copy  for making a copy of an instance of a given type. The Gobblin task flow will check if the input schema and data records are instances of  Copyable  and throw a  CopyNotSupportedException  if not. This check is performed independently on the schema first and data records subsequently. Note that this requirement is enforced  if and only if the schema or data records are to be forwarded to more than one branches , which is the case if  forkSchema  or  forkDataRecord  returns a list containing more than one  TRUE . Having more than one branch does not necessarily mean the schema and/or data records need to be  Copyable .  Gobblin ships with some built-in  Copyable  implementations, e.g.,  CopyableSchema  and  CopyableGenericRecord  for Avro's  Schema  and  GenericRecord .", 
            "title": "Basics of Usage"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#per-fork-configuration", 
            "text": "Since each forked branch may have it's own converters, quality checkers, and writers, in addition to the ones in the pre-fork stream (which does not have a writer apparently), there must be a way to tell the converter, quality checker, and writer classes of one branch from another and from the pre-fork stream. Gobblin uses a pretty straightforward approach: if a configuration property is used to specify something for a branch in a multi-branch use case,  the branch index should be appended as a suffix  to the property name. The original configuration name without the suffix is  generally reserved for the pre-fork stream . For example,  converter.classes.0  and  converter.classes.1  are used to specify the list of converter classes for branch 0 and 1, respectively, whereas  converter.classes  is reserved for the pre-fork stream. If there's only a single branch (the default case), then the index suffix is not applicable. Without being a comprehensive list, the following groups of built-in configuration properties may be used with branch indices as suffices to specify things for forked branches:   Converter configuration properties: configuration properties whose names start with  converter .  Quality checker configuration properties: configuration properties whose names start with  qualitychecker .  Writer configuration properties: configuration properties whose names start with  writer .", 
            "title": "Per-Fork Configuration"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#failure-semantics", 
            "text": "In a normal task flow where the default  IdentityForkOperator  with a single branch is used, the failure of the single branch also means the failure of the task flow. When there are more than one forked branch, however, the failure semantics are more involved. Gobblin uses the following failure semantics in this case:    The failure of any forked branch means the failure of the whole task flow, i.e., the task succeeds if and only if all the forked branches succeed.  A forked branch stops processing any outstanding incoming data records in the queue if it fails in the middle of processing the data.     The failure and subsequent stop/completion of any forked branch does not prevent other branches from processing their copies of the ingested data records. The task will wait until all the branches to finish, regardless if they succeed or fail.     The commit of output data of forks is determined by the job commit policy (see  JobCommitPolicy ) specified. If  JobCommitPolicy.COMMIT_ON_FULL_SUCCESS  (or  full  in short) is used, the output data of the entire job will be discarded if any forked branch fails, which will fail the task and consequently the job. If instead  JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS  (or  successful  in short) is used, output data of tasks whose forked branches all succeed will be committed. Output data of any task that has  at least one failed forked branch  will not be committed since the the task is considered failed in this case. This also means output data of the successful forked branches of the task won't be committed either.", 
            "title": "Failure Semantics"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#performance-tuning", 
            "text": "Internally, each forked branch as represented by a  Fork  maintains a bounded record queue (implemented by  BoundedBlockingRecordQueue ), which serves as a buffer between the pre-fork stream and the forked stream of the particular branch. The size if this bounded record queue can be configured through the property  fork.record.queue.capacity . A larger queue allows for more data records to be buffered therefore giving the producer (the pre-fork stream) more head room to move forward. On the other hand, a larger queue requires more memory. The bounded record queue imposes a timeout time on all blocking operations such as putting a new record to the tail and polling a record off the head of the queue. Tuning the queue size and timeout time together offers a lot of flexibility and a tradeoff between queuing performance vs. memory consumption.  In terms of the number of forked branches, we have seen use cases with a half dozen forked branches, and we are anticipating uses cases with much larger numbers. Again, when using a large number of forked branches, the size of the record queues and the timeout time need to be carefully tuned.   The  BoundedBlockingRecordQueue  in each  Fork  keeps trach of the following queue statistics that can be output to the logs if the  DEBUG  logging level is turned on. Those statistics provide good indications on the performance of the forks.   Queue size, i.e., the number of records in queue.  Queue fill ratio, i.e., a ratio of the number of records in queue over the queue capacity.  Put attempt rate (per second).  Total put attempt count.  Get attempt rate (per second).  Total get attempt count.", 
            "title": "Performance Tuning"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#comparison-with-partitioneddatawriter", 
            "text": "Gobblin ships with a special type of  DataWriter s called  PartitionedDataWriter  that allow ingested records to be written in a partitioned fashion using a  WriterPartitioner  into different locations in the same sink. The  WriterPartitioner  determines the specific partition for each data record. So there's certain overlap in terms of functionality between the  ForkOperator  and  PartitionedDataWriter . The question is which one should be used under which circumstances? Below is a summary of the major differences between the two operators.   The  ForkOperator  requires the number of forked branches to be known and returned through  getBranches  before the task starts, whereas the  PartitionedDataWriter  does not have this requirement.  The  PartitionedDataWriter  writes each data record to a single partition, whereas the  ForkOperator  allows data records to be forwarded to any number of forked branches.  The  ForkOperator  allows the use of additional converters and quality checkers in any forked branches before data gets written out. The  PartitionedDataWriter  is the last operator in a task flow.  Use of the  ForkOperator  allows data records to be written to different sinks, whereas the  PartitionedDataWriter  is not capable of doing this.  The  PartitionedDataWriter  writes data records sequentially in a single thread, whereas use of the  ForkOperator  allows forked branches to write independently in parallel since  Fork s are executed in a thread pool.", 
            "title": "Comparison with PartitionedDataWriter"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#writing-your-own-forkoperator", 
            "text": "Since the built-in default implementation  IdentityForkOperator  simply blindly forks the input schema and data records to every branches, it's often necessary to have a custom implementation of the  ForkOperator  interface for more fine-grained control on the actual branching. Checkout the interface  ForkOperator  for the methods that need to be implemented. You will also find the  ForkOperatorUtils  to be handy when writing your own  ForkOperator  implementations.", 
            "title": "Writing your Own ForkOperator"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#best-practices", 
            "text": "The  ForkOperator  can have many potential use cases and we have seen the following common ones:   Using a  ForkOperator  to write the same ingested data to multiple sinks, e.g., HDFS and S3, possibly in different formats. This kind of use cases is often referred to as \"dual writes\", which are  generally NOT recommended  as \"dual writes\" may lead to data inconsistency between the sinks in case of write failures. However, with the failure semantics discussed above, data inconsistency generally should not happen with the job commit policy  JobCommitPolicy.COMMIT_ON_FULL_SUCCESS  or  JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS . This is because a failure of any forked branch means the failure of the task and none of the forked branches of the task will have its output data committed, making inconsistent output data between different sinks impossible.    Using a  ForkOperator  to process ingested data records in different ways conditionally. For example, a  ForkOperator  may be used to classify and write ingested data records to different places on HDFS depending on some field in the data that serves as a classifier.  Using a  ForkOperator  to group ingested data records of a certain schema type in case the incoming stream mixes data records of different schema types. For example, we have seen a use case in which a single Kafka topic is used for records of various schema types and when data gets ingested to HDFS, the records need to be written to different paths according to their schema types.   Generally, a common use case of the  ForkOperator  is to route ingested data records so they get written to different output locations  conditionally . The  ForkOperator  also finds common usage for \"dual writes\" to different sinks potentially in different formats if the job commit policy  JobCommitPolicy.COMMIT_ON_FULL_SUCCESS  (or  full  in short) or  JobCommitPolicy.COMMIT_SUCCESSFUL_TASKS  (or  successful  in short) is used, as explained above.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/user-guide/Working-with-the-ForkOperator/#example", 
            "text": "Let's take a look at one example that shows how to work with the  ForkOperator  for a real use case. Say you have a Gobblin job that ingests Avro data from a data source that may have some sensitive data in some of the fields that need to be purged. Depending on if data records have sensitive data, they need to be written to different locations on the same sink, which we assume is HDFS. So essentially the tasks of the job need a mechanism to conditionally write ingested data records to different locations depending if they have sensitive data. The  ForkOperator  offers a way of implementing this mechanism.   In this particular use case, we need a  ForkOperator  implementation of two branches that forwards the schema to both branches but each data record to only one of the two branches. The default  IdentityForkOperator  cannot be used since it simply forwards every data records to every branches. So we need a custom implementation of the  ForkOperator  and let's simply call it  SensitiveDataAwareForkOperator  under the package  gobblin.example.fork . Let's also assume that branch 0 is for data records with sensitive data, whereas branch 1 is for data records without. Below is a brief sketch of how the implementation looks like:  public class SensitiveDataAwareForkOperator implements ForkOperator Schema, GenericRecord  {\n\n  private static final int NUM_BRANCHES = 2;\n\n  @Override\n  public void init(WorkUnitState workUnitState) {\n  }\n\n  @Override\n  public int getBranches(WorkUnitState workUnitState) {\n    return NUM_BRANCHES;\n  }\n\n  @Override\n  public List Boolean  forkSchema(WorkUnitState workUnitState, Schema schema) {\n    // The schema goes to both branches.\n    return ImmutableList.of(Boolean.TRUE, Boolean.TRUE);\n  }\n\n  @Override\n  public List Boolean  forkDataRecord(WorkUnitState workUnitState, GenericRecord record) {\n    // Data records only go to one of the two branches depending on if they have sensitive data.\n    // Branch 0 is for data records with sensitive data and branch 1 is for data records without.\n    // hasSensitiveData checks the record and returns true of the record has sensitive data and false otherwise.\n    if (hasSensitiveData(record)) {\n      return ImmutableList.of(Boolean.TRUE, Boolean.FALSE)      \n    }\n\n    return ImmutableList.of(Boolean.FALSE, Boolean.TRUE);\n  }\n\n  @Override\n  public void close() throws IOException {\n  }\n}  To make the example more concrete, let's assume that the job uses some converters and quality checkers before the schema and data records reach the  SensitiveDataAwareForkOperator , and it also uses a converter to purge the sensitive fields and a quality checker that makes sure some mandatory fields exist for purged data records in branch 0. Both branches will be written to the same HDFS but into different locations.  fork.operator.class=gobblin.example.fork.SensitiveDataAwareForkOperator\n\n# Pre-fork or non-fork-specific configuration properties\nconverter.classes= Converter classes used in the task flow prior to OutlierAwareForkOperator \nqualitychecker.task.policies=gobblin.policies.count.RowCountPolicy,gobblin.policies.schema.SchemaCompatibilityPolicy\nqualitychecker.task.policy.types=OPTIONAL,OPTIONAL\ndata.publisher.type=gobblin.publisher.BaseDataPublisher\n\n# Configuration properties for branch 0\nconverter.classes.0=gobblin.example.converter.PurgingConverter\nqualitychecker.task.policies.0=gobblin.example,policies.MandatoryFieldExistencePolicy\nqualitychecker.task.policy.types.0=FAILED\nwriter.fs.uri.0=hdfs:// namenode host : namenode port /\nwriter.destination.type.0=HDFS\nwriter.output.format.0=AVRO\nwriter.staging.dir.0=/gobblin/example/task-staging/purged\nwriter.output.dir.0=/gobblin/example/task-output/purged\ndata.publisher.final.dir.0=/gobblin/example/job-output/purged\n\n# Configuration properties for branch 1\nwriter.fs.uri.1=hdfs:// namenode host : namenode port /\nwriter.destination.type.1=HDFS\nwriter.output.format.1=AVRO\nwriter.staging.dir.1=/gobblin/example/task-staging/normal\nwriter.output.dir.1=/gobblin/example/task-output/normal\ndata.publisher.final.dir.1=/gobblin/example/job-output/normal", 
            "title": "Example"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/", 
            "text": "Configuration properties are key/value pairs that are set in text files. They include system properties that control how Gobblin will pull data, and control what source Gobblin will pull the data from. Configuration files end in some user-specified suffix (by default text files ending in \n.pull\n or \n.job\n are recognized as configs files, although this is configurable). Each file represents some unit of work that needs to be done in Gobblin. For example, there will typically be a separate configuration file for each table that needs to be pulled from a database.  \n\n\nThe first section of this document contains all the required properties needed to run a basic Gobblin job. The rest of the document is dedicated to other properties that can be used to configure Gobbin jobs. The description of each configuration parameter will often refer to core Gobblin concepts and terms. If any of these terms are confusing, check out the \nGobblin Archiecture\n page for a more detailed explanation of how Gobblin works. The GitHub repo also contains sample config files for specific sources. For example, there are sample config files to connect to MySQL databases and \nSFTP servers\n.  \n\n\nGobblin also allows you to specify a global configuration file that contains common properties that are shared across all jobs. The \nJob Launcher Properties\n section has more information on how to specify a global properties file.  \n\n\nTable of Contents\n\n\n\n\nProperties File Format\n\n\nCreating a Basic Properties File\n   \n\n\nJob Launcher Properties\n  \n\n\nCommon Job Launcher Properties\n  \n\n\nSchedulerDaemon Properties\n  \n\n\nCliMRJobLauncher Properties\n  \n\n\nAzkabanJobLauncher Properties\n  \n\n\nJob Type Properties\n  \n\n\nCommon Job Type Properties\n  \n\n\nLocalJobLauncher Properties\n  \n\n\nMRJobLauncher Properties\n  \n\n\nTask Execution Properties\n  \n\n\nState Store Properties\n  \n\n\nMetrics Properties\n  \n\n\nEmail Alert Properties\n  \n\n\nSource Properties\n  \n\n\nCommon Source Properties\n  \n\n\nQueryBasedExtractor Properties\n \n\n\nJdbcExtractor Properties\n  \n\n\n\n\n\n\nFileBasedExtractor Properties\n  \n\n\nSftpExtractor Properties\n  \n\n\n\n\n\n\nConverter Properties\n\n\nCsvToJsonConverter Properties\n    \n\n\nJsonIntermediateToAvroConverter Properties\n  \n\n\nAvroFilterConverter Properties\n  \n\n\nAvroFieldRetrieverConverter Properties\n  \n\n\nQuality Checker Properties\n  \n\n\nWriter Properties\n  \n\n\nData Publisher Properties\n  \n\n\nGeneric Properties\n  \n\n\n\n\nProperties File Format \n\n\nConfiguration properties files follow the \nJava Properties text file format\n. Further, file includes and variable expansion/interpolation as defined in \nApache Commons Configuration\n are also supported.\n\n\nExample:\n\n\n\n\ncommon.properties\n\n\n\n\n    writer.staging.dir=/path/to/staging/dir/\n    writer.output.dir=/path/to/output/dir/\n\n\n\n\n\n\nmy-job.properties\n\n\n\n\n    include=common.properties\n\n    job.name=MyFirstJob\n\n\n\n\nCreating a Basic Properties File \n\n\nIn order to create a basic configuration property there is a small set of required properties that need to be set. The following properties are required to run any Gobblin job:\n\n \njob.name\n - Name of the job\n\n\n \nsource.class\n - Fully qualified path to the Source class responsible for connecting to the data source\n\n\n \nwriter.staging.dir\n - The directory each task will write staging data to\n\n\n \nwriter.output.dir\n - The directory each task will commit data to\n\n\n \ndata.publisher.final.dir\n - The final directory where all the data will be published\n\n \nstate.store.dir\n - The directory where state-store files will be written  \n\n\nFor more information on each property, check out the comprehensive list below.  \n\n\nIf only these properties are set, then by default, Gobblin will run in Local mode, as opposed to running on Hadoop M/R. This means Gobblin will write Avro data to the local filesystem. In order to write to HDFS, set the \nwriter.fs.uri\n property to the URI of the HDFS NameNode that data should be written to. Since the default version of Gobblin writes data in Avro format, the writer expects Avro records to be passed to it. Thus, any data pulled from an external source must be converted to Avro before it can be written out to the filesystem.  \n\n\nThe \nsource.class\n property is one of the most important properties in Gobblin. It specifies what Source class to use. The Source class is responsible for determining what work needs to be done during each run of the job, and specifies what Extractor to use in order to read over each sub-unit of data. Examples of Source classes are \nWikipediaSource\n and \nSimpleJsonSource\n, which can be found in the GitHub repository. For more information on Sources and Extractors, check out the \nArchitecture\n page.  \n\n\nTypically, Gobblin jobs will be launched using the launch scripts in the \nbin\n folder. These scripts allow jobs to be launched on the local machine (e.g. SchedulerDaemon) or on Hadoop (e.g. CliMRJobLauncher). Check out the Job Launcher section below, to see the configuration difference between each launch mode. The \nDeployment\n page also has more information on the different ways a job can be launched.  \n\n\nJob Launcher Properties \n\n\nGobblin jobs can be launched and scheduled in a variety of ways. They can be scheduled via a Quartz scheduler or through \nAzkaban\n. Jobs can also be run without a scheduler via the Command Line. For more information on launching Gobblin jobs, check out the \nDeployment\n page.\n\n\nCommon Job Launcher Properties \n\n\nThese properties are common to both the Job Launcher and the Command Line.\n\n\njob.name\n\n\nDescription\n\n\nThe name of the job to run. This name must be unique within a single Gobblin instance.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\njob.group\n\n\nDescription\n\n\nA way to group logically similar jobs together.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\njob.description\n\n\nDescription\n\n\nA description of what the jobs does.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\njob.lock.dir\n\n\nDescription\n\n\nDirectory where job locks are stored. Job locks are used by the scheduler to ensure two executions of a job do not run at the same time. If a job is scheduled to run, Gobblin will first check this directory to see if there is a lock file for the job. If there is one, it will not run the job, if there isn't one then it will run the job.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\njob.lock.enabled\n\n\nDescription\n\n\nIf set to true job locks are enabled, if set to false they are disabled\n\n\nDefault Value\n\n\nTrue\n\n\nRequired\n\n\nNo\n\n\njob.runonce\n\n\nDescription\n\n\nA boolean specifying whether the job will be only once, or multiple times. If set to true the job will only be run once even if a job.schedule is specified. If set to false and a job.schedule is specified then it will run according to the schedule. If set false and a job.schedule is not specified, it will run only once.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\njob.disabled\n\n\nDescription\n\n\nWhether the job is disabled or not. If set to true, then Gobblin will not run this job.\n\n\nDefault Value\n\n\nFalse\n\n\nRequired\n\n\nNo\n\n\nSchedulerDaemon Properties \n\n\nThis class is used to schedule Gobblin jobs on Quartz. The job can be launched via the command line, and takes in the location of a global configuration file as a parameter. This configuration file should have the property \njobconf.dir\n in order to specify the location of all the \n.job\n or \n.pull\n files. Another core difference, is that the global configuration file for the SchedulerDaemon must specify the following properties:\n\n\n\n\nwriter.staging.dir\n  \n\n\nwriter.output.dir\n  \n\n\ndata.publisher.final.dir\n  \n\n\nstate.store.dir\n  \n\n\n\n\nThey should not be set in individual job files, as they are system-level parameters.\nFor more information on how to set the configuration parameters for jobs launched through the SchedulerDaemon, check out the \nDeployment\n page.\n\n\njob.schedule\n\n\nDescription\n\n\nCron-Based job schedule. This schedule only applies to jobs that run using Quartz.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\njobconf.dir\n\n\nDescription\n\n\nWhen running in local mode, Gobblin will check this directory for any configuration files. Each configuration file should correspond to a separate Gobblin job, and each one should in a suffix specified by the jobconf.extensions parameter.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\njobconf.extensions\n\n\nDescription\n\n\nComma-separated list of supported job configuration file extensions. When running in local mode, Gobblin will only pick up job files ending in these suffixes.\n\n\nDefault Value\n\n\npull,job\n\n\nRequired\n\n\nNo\n\n\njobconf.monitor.interval\n\n\nDescription\n\n\nControls how often Gobblin checks the jobconf.dir for new configuration files, or for configuration file updates. The parameter is measured in milliseconds.\n\n\nDefault Value\n\n\n300000\n\n\nRequired\n\n\nNo\n\n\nCliMRJobLauncher Properties \n\n\nThere are no configuration parameters specific to CliMRJobLauncher. This class is used to launch Gobblin jobs on Hadoop from the command line, the jobs are not scheduled. Common properties are set using the \n--sysconfig\n option when launching jobs via the command line. For more information on how to set the configuration parameters for jobs launched through the command line, check out the \nDeployment\n page.  \n\n\nAzkabanJobLauncher Properties \n\n\nThere are no configuration parameters specific to AzkabanJobLauncher. This class is used to schedule Gobblin jobs on Azkaban. Common properties can be set through Azkaban by creating a \n.properties\n file, check out the \nAzkaban Documentation\n for more information. For more information on how to set the configuration parameters for jobs scheduled through the Azkaban, check out the \nDeployment\n page.\n\n\nJob Type Properties \n\n\nCommon Job Type Properties \n\n\nlauncher.type\n\n\nDescription\n\n\nJob launcher type; one of LOCAL, MAPREDUCE, YARN. LOCAL mode runs on a single machine (LocalJobLauncher), MAPREDUCE runs on a Hadoop cluster (MRJobLauncher), and YARN runs on a YARN cluster (not implemented yet).\n\n\nDefault Value\n\n\nLOCAL \n\n\nRequired\n\n\nNo\n\n\nLocalJobLauncher Properties \n\n\nThere are no configuration parameters specific to LocalJobLauncher. The LocalJobLauncher will launch a Hadoop job on a single machine. If launcher.type is set to LOCAL then this class will be used to launch the job.\nProperties required by the MRJobLauncher class.\n\n\nframework.jars\n\n\nDescription\n\n\nComma-separated list of jars the Gobblin framework depends on. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\njob.jars\n\n\nDescription\n\n\nComma-separated list of jar files the job depends on. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\njob.local.files\n\n\nDescription\n\n\nComma-separated list of local files the job depends on. These files will be available to any map tasks that get launched via the DistributedCache.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\njob.hdfs.files\n\n\nDescription\n\n\nComma-separated list of files on HDFS the job depends on. These files will be available to any map tasks that get launched via the DistributedCache.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nMRJobLauncher Properties \n\n\nmr.job.root.dir\n\n\nDescription\n\n\nWorking directory for a Gobblin Hadoop MR job. Gobblin uses this to write intermediate data, such as the workunit state files that are used by each map task. This has to be a path on HDFS.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nmr.job.max.mappers\n\n\nDescription\n\n\nMaximum number of mappers to use in a Gobblin Hadoop MR job. If no explicit limit is set then a map task for each workunit will be launched. If the value of this properties is less than the number of workunits created, then each map task will run multiple tasks.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nmr.include.task.counters\n\n\nDescription\n\n\nWhether to include task-level counters in the set of counters reported as Hadoop counters. Hadoop imposes a system-level limit (default to 120) on the number of counters, so a Gobblin MR job may easily go beyond that limit if the job has a large number of tasks and each task has a few counters. This property gives users an option to not include task-level counters to avoid going over that limit.\n\n\nDefault Value\n\n\nFalse\n\n\nRequired\n\n\nNo\n\n\nRetry Properties \n\n\nProperties that control how tasks and jobs get retried on failure.\n\n\nworkunit.retry.enabled\n\n\nDescription\n\n\nWhether retries of failed work units across job runs are enabled or not.\n\n\nDefault Value\n\n\nTrue \n\n\nRequired\n\n\nNo\n\n\nworkunit.retry.policy\n\n\nDescription\n\n\nWork unit retry policy, can be one of {always, never, onfull, onpartial}.\n\n\nDefault Value\n\n\nalways \n\n\nRequired\n\n\nNo\n\n\ntask.maxretries\n\n\nDescription\n\n\nMaximum number of task retries. A task will be re-tried this many times before it is considered a failure.\n\n\nDefault Value\n\n\n5 \n\n\nRequired\n\n\nNo\n\n\ntask.retry.intervalinsec\n\n\nDescription\n\n\nInterval in seconds between task retries. The interval increases linearly with each retry. For example, if the first interval is 300 seconds, then the second one is 600 seconds, etc.\n\n\nDefault Value\n\n\n300 \n\n\nRequired\n\n\nNo\n\n\njob.max.failures\n\n\nDescription\n\n\nMaximum number of failures before an alert email is triggered.\n\n\nDefault Value\n\n\n1 \n\n\nTask Execution Properties \n\n\nThese properties control how tasks get executed for a job. Gobblin uses thread pools in order to executes the tasks for a specific job. In local mode there is a single thread pool per job that executes all the tasks for a job. In MR mode there is a thread pool for each map task (or container), and all Gobblin tasks assigned to that mapper are executed in that thread pool.\n\n\ntaskexecutor.threadpool.size\n\n\nDescription\n\n\nSize of the thread pool used by task executor for task execution. Each task executor will spawn this many threads to execute any Tasks that is has been allocated.\n\n\nDefault Value\n\n\n10 \n\n\nRequired\n\n\nNo\n\n\ntasktracker.threadpool.coresize\n\n\nDescription\n\n\nCore size of the thread pool used by task tracker for task state tracking and reporting.\n\n\nDefault Value\n\n\n10 \n\n\nRequired\n\n\nNo\n\n\ntasktracker.threadpool.maxsize\n\n\nDescription\n\n\nMaximum size of the thread pool used by task tracker for task state tracking and reporting.\n\n\nDefault Value\n\n\n10 \n\n\nRequired\n\n\nNo\n\n\ntaskretry.threadpool.coresize\n\n\nDescription\n\n\nCore size of the thread pool used by the task executor for task retries.\n\n\nDefault Value\n\n\n2 \n\n\nRequired\n\n\nNo\n\n\ntaskretry.threadpool.maxsize\n\n\nDescription\n\n\nMaximum size of the thread pool used by the task executor for task retries.\n\n\nDefault Value\n\n\n2 \n\n\nRequired\n\n\nNo\n\n\ntask.status.reportintervalinms\n\n\nDescription\n\n\nTask status reporting interval in milliseconds.\n\n\nDefault Value\n\n\n30000 \n\n\nRequired\n\n\nNo\n\n\nState Store Properties \n\n\nstate.store.dir\n\n\nDescription\n\n\nRoot directory where job and task state files are stored. The state-store is used by Gobblin to track state between different executions of a job. All state-store files will be written to this directory.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nstate.store.fs.uri\n\n\nDescription\n\n\nFile system URI for file-system-based state stores.\n\n\nDefault Value\n\n\nfile:///\n\n\nRequired\n\n\nNo\n\n\nMetrics Properties \n\n\nmetrics.enabled\n\n\nDescription\n\n\nWhether metrics collecting and reporting are enabled or not.\n\n\nDefault Value\n\n\nTrue\n\n\nRequired\n\n\nNo\n\n\nmetrics.report.interval\n\n\nDescription\n\n\nMetrics reporting interval in milliseconds.\n\n\nDefault Value\n\n\n60000\n\n\nRequired\n\n\nNo\n\n\nmetrics.log.dir\n\n\nDescription\n\n\nThe directory where metric files will be written to.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nmetrics.reporting.file.enabled\n\n\nDescription\n\n\nA boolean indicating whether or not metrics should be reported to a file.\n\n\nDefault Value\n\n\nTrue\n\n\nRequired\n\n\nNo\n\n\nmetrics.reporting.jmx.enabled\n\n\nDescription\n\n\nA boolean indicating whether or not metrics should be exposed via JMX.\n\n\nDefault Value\n\n\nFalse\n\n\nRequired\n\n\nNo\n\n\nEmail Alert Properties \n\n\nemail.alert.enabled\n\n\nDescription\n\n\nWhether alert emails are enabled or not. Email alerts are only sent out when jobs fail consecutively job.max.failures number of times.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nemail.notification.enabled\n\n\nDescription\n\n\nWhether job completion notification emails are enabled or not. Notification emails are sent whenever the job completes, regardless of whether it failed or not.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nemail.host\n\n\nDescription\n\n\nHost name of the email server.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes, if email notifications or alerts are enabled.\n\n\nemail.smtp.port\n\n\nDescription\n\n\nSMTP port number.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes, if email notifications or alerts are enabled.\n\n\nemail.user\n\n\nDescription\n\n\nUser name of the sender email account.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nemail.password\n\n\nDescription\n\n\nUser password of the sender email account.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nemail.from\n\n\nDescription\n\n\nSender email address.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes, if email notifications or alerts are enabled.\n\n\nemail.tos\n\n\nDescription\n\n\nComma-separated list of recipient email addresses.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes, if email notifications or alerts are enabled.\n\n\nSource Properties \n\n\nCommon Source Properties \n\n\nThese properties are common properties that are used among different Source implementations. Depending on what source class is being used, these parameters may or may not be necessary. These parameters are not tied to a specific source, and thus can be used in new source classes.\n\n\nsource.class\n\n\nDescription\n\n\nFully qualified name of the Source class. For example, com.linkedin.gobblin.example.wikipedia\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nsource.entity\n\n\nDescription\n\n\nName of the source entity that needs to be pulled from the source. The parameter represents a logical grouping of data that needs to be pulled from the source. Often this logical grouping comes in the form a database table, a source topic, etc. In many situations, such as when using the QueryBasedExtractor, it will be the name of the table that needs to pulled from the source.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nRequired for QueryBasedExtractors, FileBasedExtractors.\n\n\nsource.timezone\n\n\nDescription\n\n\nTimezone of the data being pulled in by the extractor. Examples include \"PST\" or \"UTC\".\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nRequired for QueryBasedExtractors\n\n\nsource.max.number.of.partitions\n\n\nDescription\n\n\nMaximum number of partitions to split this current run across. Only used by the QueryBasedSource and FileBasedSource.\n\n\nDefault Value\n\n\n20 \n\n\nRequired\n\n\nNo\n\n\nsource.skip.first.record\n\n\nDescription\n\n\nTrue if you want to skip the first record of each data partition. Only used by the FileBasedExtractor.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nextract.namespace\n\n\nDescription\n\n\nNamespace for the extract data. The namespace will be included in the default file name of the outputted data.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.conn.use.proxy.url\n\n\nDescription\n\n\nThe URL of the proxy to connect to when connecting to the source. This parameter is only used for SFTP and REST sources.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.conn.use.proxy.port\n\n\nDescription\n\n\nThe port of the proxy to connect to when connecting to the source. This parameter is only used for SFTP and REST sources.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.conn.username\n\n\nDescription\n\n\nThe username to authenticate with the source. This is parameter is only used for SFTP and JDBC sources.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.conn.password\n\n\nDescription\n\n\nThe password to use when authenticating with the source. This is parameter is only used for JDBC sources.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.conn.host\n\n\nDescription\n\n\nThe name of the host to connect to.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nRequired for SftpExtractor, MySQLExtractor, and SQLServerExtractor.\n\n\nsource.conn.rest.url\n\n\nDescription\n\n\nURL to connect to for REST requests. This parameter is only used for the Salesforce source.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.conn.version\n\n\nDescription\n\n\nVersion number of communication protocol. This parameter is only used for the Salesforce source.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.conn.timeout\n\n\nDescription\n\n\nThe timeout set for connecting to the source in milliseconds.\n\n\nDefault Value\n\n\n500000\n\n\nRequired\n\n\nNo\n\n\nsource.conn.port\n\n\nDescription\n\n\nThe value of the port to connect to.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nRequired for SftpExtractor, MySQLExtractor, SqlServerExtractor.\n\n\nextract.table.name\n\n\nDescription\n\n\nTable name in Hadoop which is different table name in source.\n\n\nDefault Value\n\n\nSource table name \n\n\nRequired\n\n\nNo\n\n\nextract.is.full\n\n\nDescription\n\n\nTrue if this pull should treat the data as a full dump of table from the source, false otherwise\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nextract.delta.fields\n\n\nDescription\n\n\nList of columns that will be used as the delta field for the data.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nextract.primary.key.fields\n\n\nDescription\n\n\nList of columns that will be used as the primary key for the data.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nextract.pull.limit\n\n\nDescription\n\n\nThis limits the number of records read by Gobblin. In Gobblin's extractor the readRecord() method is expected to return records until there are no more to pull, in which case it runs null. This parameter limits the number of times readRecord() is executed. This parameter is useful for pulling a limited sample of the source data for testing purposes.\n\n\nDefault Value\n\n\nUnbounded\n\n\nRequired\n\n\nNo\n\n\nextract.full.run.time\n\n\nDescription\n\n\nDefault Value\n\n\nRequired\n\n\nQueryBasedExtractor Properties \n\n\nThe following table lists the query based extractor configuration properties.\n\n\nsource.querybased.watermark.type\n\n\nDescription\n\n\nThe format of the watermark that is used when extracting data from the source. Possible types are timestamp, date, hour, simple.\n\n\nDefault Value\n\n\ntimestamp \n\n\nRequired\n\n\nYes\n\n\nsource.querybased.start.value\n\n\nDescription\n\n\nValue for the watermark to start pulling data from, also the default watermark if the previous watermark cannot be found in the old task states.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nsource.querybased.partition.interval\n\n\nDescription\n\n\nNumber of hours to pull in each partition.\n\n\nDefault Value\n\n\n1 \n\n\nRequired\n\n\nNo\n\n\nsource.querybased.hour.column\n\n\nDescription\n\n\nDelta column with hour for hourly extracts (Ex: hour_sk)\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.querybased.skip.high.watermark.calc\n\n\nDescription\n\n\nIf it is true, skips high watermark calculation in the source and it will use partition higher range as high watermark instead of getting it from source.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nsource.querybased.query\n\n\nDescription\n\n\nThe query that the extractor should execute to pull data.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.querybased.hourly.extract\n\n\nDescription\n\n\nTrue if hourly extract is required.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nsource.querybased.extract.type\n\n\nDescription\n\n\n\"snapshot\" for the incremental dimension pulls. \"append_daily\", \"append_hourly\" and \"append_batch\" for the append data append_batch for the data with sequence numbers as watermarks\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.querybased.end.value\n\n\nDescription\n\n\nThe high watermark which this entire job should pull up to. If this is not specified, pull entire data from the table\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.querybased.append.max.watermark.limit\n\n\nDescription\n\n\nmax limit of the high watermark for the append data.  CURRENT_DATE - X CURRENT_HOUR - X where X\n=1\n\n\nDefault Value\n\n\nCURRENT_DATE for daily extract CURRENT_HOUR for hourly extract \n\n\nRequired\n\n\nNo\n\n\nsource.querybased.is.watermark.override\n\n\nDescription\n\n\nTrue if this pull should override previous watermark with start.value and end.value. False otherwise.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nsource.querybased.low.watermark.backup.secs\n\n\nDescription\n\n\nNumber of seconds that needs to be backup from the previous high watermark. This is to cover late data.  Ex: Set to 3600 to cover 1 hour late data.\n\n\nDefault Value\n\n\n0 \n\n\nRequired\n\n\nNo\n\n\nsource.querybased.schema\n\n\nDescription\n\n\nDatabase name\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nsource.querybased.is.specific.api.active\n\n\nDescription\n\n\nTrue if this pull needs to use source specific apis instead of standard protocols.  Ex: Use salesforce bulk api instead of rest api\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nsource.querybased.skip.count.calc\n\n\nDescription\n\n\nA boolean, if true then the QueryBasedExtractor will skip the source count calculation.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nsource.querybased.fetch.size\n\n\nDescription\n\n\nThis parameter is currently only used in JDBCExtractor. The JDBCExtractor will process this many number of records from the JDBC ResultSet at a time. It will then take these records and return them to the rest of the Gobblin flow so that they can get processed by the rest of the Gobblin components. \n\n\nDefault Value\n\n\n1000\n\n\nRequired\n\n\nNo\n\n\nsource.querybased.is.metadata.column.check.enabled\n\n\nDescription\n\n\nWhen a query is specified in the configuration file, it is possible a user accidentally adds in a column name that does not exist on the source side. By default, this parameter is set to false, which means that if a column is specified in the query and it does not exist in the source data set, Gobblin will just skip over that column. If it is set to true, Gobblin will actually take the config specified column and check to see if it exists in the source data set. If it doesn't exist then the job will fail.\n\n\nDefault Value\n\n\nFalse\n\n\nRequired\n\n\nNo\n\n\nsource.querybased.is.compression.enabled\n\n\nDescription\n\n\nA boolean specifying whether or not compression should be enabled when pulling data from the source. This parameter is only used for MySQL sources. If set to true, the MySQL will send compressed data back to the source.\n\n\nDefault Value\n\n\nFalse\n\n\nRequired\n\n\nNo\n\n\nsource.querybased.jdbc.resultset.fetch.size\n\n\nDescription\n\n\nThe number of rows to pull through JDBC at a time. This is useful when the JDBC ResultSet is too big to fit into memory, so only \"x\" number of records will be fetched at a time.\n\n\nDefault Value\n\n\n1000\n\n\nRequired\n\n\nNo\n\n\nJdbcExtractor Properties \n\n\nThe following table lists the jdbc based extractor configuration properties.\n\n\nsource.conn.driver\n\n\nDescription\n\n\nThe fully qualified path of the JDBC driver used to connect to the external source.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nsource.column.name.case\n\n\nDescription\n\n\nA enum specifying whether or not to convert the column names to a specific case before performing a query. Possible values are TOUPPER or TOLOWER.\n\n\nDefault Value\n\n\nNOCHANGE \n\n\nRequired\n\n\nNo\n\n\nFileBasedExtractor Properties \n\n\nThe following table lists the file based extractor configuration properties.\n\n\nsource.filebased.data.directory\n\n\nDescription\n\n\nThe data directory from which to pull data from.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nsource.filebased.files.to.pull\n\n\nDescription\n\n\nA list of files to pull - this should be set in the Source class and the extractor will pull the specified files.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes  \n\n\nfilebased.report.status.on.count\n\n\nDescription\n\n\nThe FileBasedExtractor will report it's status every time it processes the number of records specified by this parameter. The way it reports status is by logging out how many records it has seen.\n\n\nDefault Value\n\n\n10000\n\n\nRequired\n\n\nNo  \n\n\nsource.filebased.fs.uri\n\n\nDescription\n\n\nThe URI of the filesystem to connect to.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nRequired for HadoopExtractor.\n\n\nsource.filebased.preserve.file.name\n\n\nDescription\n\n\nA boolean, if true then the original file names will be preserved when they are are written to the source.\n\n\nDefault Value\n\n\nFalse\n\n\nRequired\n\n\nNo\n\n\nsource.schema\n\n\nDescription\n\n\nThe schema of the data that will be pulled by the source.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nSftpExtractor Properties \n\n\nsource.conn.private.key\n\n\nDescription\n\n\nFile location of the private key used for key based authentication. This parameter is only used for the SFTP source.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nsource.conn.known.hosts\n\n\nDescription\n\n\nFile location of the known hosts file used for key based authentication.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nConverter Properties \n\n\nProperties for Gobblin converters.\n\n\nconverter.classes\n\n\nDescription\n\n\nComma-separated list of fully qualified names of the Converter classes. The order is important as the converters will be applied in this order.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nCsvToJsonConverter Properties \n\n\nThis converter takes in text data separated by a delimiter (converter.csv.to.json.delimiter), and splits the data into a JSON format recognized by JsonIntermediateToAvroConverter.\n\n\nconverter.csv.to.json.delimiter\n\n\nDescription\n\n\nThe regex delimiter between CSV based files, only necessary when using the CsvToJsonConverter - e.g. \",\", \"/t\" or some other regex\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nJsonIntermediateToAvroConverter Properties \n\n\nThis converter takes in JSON data in a specific schema, and converts it to Avro data.\n\n\nconverter.avro.date.format\n\n\nDescription\n\n\nSource format of the date columns for Avro-related converters.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nconverter.avro.timestamp.format\n\n\nDescription\n\n\nSource format of the timestamp columns for Avro-related converters.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nconverter.avro.time.format\n\n\nDescription\n\n\nSource format of the time columns for Avro-related converters.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nconverter.avro.binary.charset\n\n\nDescription\n\n\nSource format of the time columns for Avro-related converters.\n\n\nDefault Value\n\n\nUTF-8\n\n\nRequired\n\n\nNo\n\n\nconverter.is.epoch.time.in.seconds\n\n\nDescription\n\n\nA boolean specifying whether or not a epoch time field in the JSON object is in seconds or not.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nconverter.avro.max.conversion.failures\n\n\nDescription\n\n\nThis converter is will fail for this many number of records before throwing an exception.\n\n\nDefault Value\n\n\n0\n\n\nRequired\n\n\nNo\n\n\nAvroFilterConverter Properties \n\n\nThis converter takes in an Avro record, and filters out records by performing an equality operation on the value of the field specified by converter.filter.field and the value specified in converter.filter.value. It returns the record unmodified if the equality operation evaluates to true, false otherwise.\n\n\nconverter.filter.field\n\n\nDescription\n\n\nThe name of the field in the Avro record, for which the converter will filter records on.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nconverter.filter.value\n\n\nDescription\n\n\nThe value that will be used in the equality operation to filter out records.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nAvroFieldRetrieverConverter Properties \n\n\nThis converter takes a specific field from an Avro record and returns its value.\n\n\nconverter.avro.extractor.field.path\n\n\nDescription\n\n\nThe field in the Avro record to retrieve. If it is a nested field, then each level must be separated by a period.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nFork Properties \n\n\nProperties for Gobblin's fork operator.\n\n\nfork.operator.class\n\n\nDescription\n\n\nFully qualified name of the ForkOperator class.\n\n\nDefault Value\n\n\ncom.linkedin.uif.fork.IdentityForkOperator \n\n\nRequired\n\n\nNo\n\n\nfork.branches\n\n\nDescription\n\n\nNumber of fork branches.\n\n\nDefault Value\n\n\n1 \n\n\nRequired\n\n\nNo\n\n\nfork.branch.name.${branch index}\n\n\nDescription\n\n\nName of a fork branch with the given index, e.g., 0 and 1.\n\n\nDefault Value\n\n\nfork_${branch index}, e.g., fork_0 and fork_1. \n\n\nRequired\n\n\nNo\n\n\nQuality Checker Properties \n\n\nqualitychecker.task.policies\n\n\nDescription\n\n\nComma-separted list of fully qualified names of the TaskLevelPolicy classes that will run at the end of each Task.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nqualitychecker.task.policy.types\n\n\nDescription\n\n\nOPTIONAL implies the corresponding class in qualitychecker.task.policies is optional and if it fails the Task will still succeed, FAIL implies that if the corresponding class fails then the Task will fail too.\n\n\nDefault Value\n\n\nOPTIONAL \n\n\nRequired\n\n\nNo\n\n\nqualitychecker.row.policies\n\n\nDescription\n\n\nComma-separted list of fully qualified names of the RowLevelPolicy classes that will run on each record.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nqualitychecker.row.policy.types\n\n\nDescription\n\n\nOPTIONAL implies the corresponding class in qualitychecker.row.policies is optional and if it fails the Task will still succeed, FAIL implies that if the corresponding class fails then the Task will fail too, ERR_FILE implies that if the record does not pass the test then the record will be written to an error file.\n\n\nDefault Value\n\n\nOPTIONAL \n\n\nRequired\n\n\nNo\n\n\nqualitychecker.row.err.file\n\n\nDescription\n\n\nThe quality checker will write the current record to the location specified by this parameter, if the current record fails to pass the quality checkers specified by qualitychecker.row.policies; this file will only be written to if the quality checker policy type is ERR_FILE.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nNo\n\n\nWriter Properties \n\n\nwriter.destination.type\n\n\nDescription\n\n\nWriter destination type; currently only writing to HDFS is supported.\n\n\nDefault Value\n\n\nHDFS \n\n\nRequired\n\n\nNo\n\n\nwriter.output.format\n\n\nDescription\n\n\nWriter output format; currently only Avro is supported.\n\n\nDefault Value\n\n\nAVRO \n\n\nRequired\n\n\nNo\n\n\nwriter.fs.uri\n\n\nDescription\n\n\nFile system URI for writer output.\n\n\nDefault Value\n\n\nfile:/// \n\n\nRequired\n\n\nNo\n\n\nwriter.staging.dir\n\n\nDescription\n\n\nStaging directory of writer output. All staging data that the writer produces will be placed in this directory, but all the data will be eventually moved to the writer.output.dir.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nwriter.output.dir\n\n\nDescription\n\n\nOutput directory of writer output. All output data that the writer produces will be placed in this directory, but all the data will be eventually moved to the final directory by the publisher.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nwriter.builder.class\n\n\nDescription\n\n\nFully qualified name of the writer builder class.\n\n\nDefault Value\n\n\ncom.linkedin.uif.writer.AvroDataWriterBuilder\n\n\nRequired\n\n\nNo\n\n\nwriter.file.path\n\n\nDescription\n\n\nThe Path where the writer will write it's data. Data in this directory will be copied to it's final output directory by the DataPublisher.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\nwriter.file.name\n\n\nDescription\n\n\nThe name of the file the writer writes to.\n\n\nDefault Value\n\n\npart \n\n\nRequired\n\n\nYes\n\n\nwriter.partitioner.class\n\n\nDescription\n\n\nPartitioner used for distributing records into multiple output files. \nwriter.builder.class\n must be a subclass of \nPartitionAwareDataWriterBuilder\n, otherwise Gobblin will throw an error. \n\n\nDefault Value\n\n\nNone (will not use partitioner)\n\n\nRequired\n\n\nNo\n\n\nwriter.buffer.size\n\n\nDescription\n\n\nWriter buffer size in bytes. This parameter is only applicable for the AvroHdfsDataWriter.\n\n\nDefault Value\n\n\n4096 \n\n\nRequired\n\n\nNo\n\n\nwriter.deflate.level\n\n\nDescription\n\n\nWriter deflate level. Deflate is a type of compression for Avro data.\n\n\nDefault Value\n\n\n9 \n\n\nRequired\n\n\nNo\n\n\nwriter.codec.type\n\n\nDescription\n\n\nThis is used to specify the type of compression used when writing data out. Possible values are NOCOMPRESSION, DEFLATE, SNAPPY.\n\n\nDefault Value\n\n\nDEFLATE \n\n\nRequired\n\n\nNo\n\n\nwriter.eager.initialization\n\n\nDescription\n\n\nThis is used to control the writer creation. If the value is set to true, writer is created before records are read. This means an empty file will be created even if no records were read.\n\n\nDefault Value\n\n\nFalse \n\n\nRequired\n\n\nNo\n\n\nData Publisher Properties \n\n\ndata.publisher.type\n\n\nDescription\n\n\nThe fully qualified name of the DataPublisher class to run. The DataPublisher is responsible for publishing task data once all Tasks have been completed.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\ndata.publisher.final.dir\n\n\nDescription\n\n\nThe final output directory where the data should be published.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\ndata.publisher.replace.final.dir\n\n\nDescription\n\n\nA boolean, if true and the the final output directory already exists, then the data will not be committed. If false and the final output directory already exists then it will be overwritten.\n\n\nDefault Value\n\n\nNone\n\n\nRequired\n\n\nYes\n\n\ndata.publisher.final.name\n\n\nDescription\n\n\nThe final name of the file that is produced by Gobblin. By default, Gobblin already assigns a unique name to each file it produces. If that default name needs to be overridden then this parameter can be used. Typically, this parameter should be set on a per workunit basis so that file names don't collide.\n\n\nDefault Value\n\n\nRequired\n\n\nNo\n\n\nGeneric Properties \n\n\nThese properties are used throughout multiple Gobblin components.\n\n\nfs.uri\n\n\nDescription\n\n\nDefault file system URI for all file storage; over-writable by more specific configuration properties.\n\n\nDefault Value\n\n\nfile:///\n\n\nRequired\n\n\nNo", 
            "title": "Configuration Glossary"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#table-of-contents", 
            "text": "Properties File Format  Creating a Basic Properties File      Job Launcher Properties     Common Job Launcher Properties     SchedulerDaemon Properties     CliMRJobLauncher Properties     AzkabanJobLauncher Properties     Job Type Properties     Common Job Type Properties     LocalJobLauncher Properties     MRJobLauncher Properties     Task Execution Properties     State Store Properties     Metrics Properties     Email Alert Properties     Source Properties     Common Source Properties     QueryBasedExtractor Properties    JdbcExtractor Properties       FileBasedExtractor Properties     SftpExtractor Properties       Converter Properties  CsvToJsonConverter Properties       JsonIntermediateToAvroConverter Properties     AvroFilterConverter Properties     AvroFieldRetrieverConverter Properties     Quality Checker Properties     Writer Properties     Data Publisher Properties     Generic Properties", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#properties-file-format", 
            "text": "Configuration properties files follow the  Java Properties text file format . Further, file includes and variable expansion/interpolation as defined in  Apache Commons Configuration  are also supported.  Example:   common.properties       writer.staging.dir=/path/to/staging/dir/\n    writer.output.dir=/path/to/output/dir/   my-job.properties       include=common.properties\n\n    job.name=MyFirstJob", 
            "title": "Properties File Format "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#creating-a-basic-properties-file", 
            "text": "In order to create a basic configuration property there is a small set of required properties that need to be set. The following properties are required to run any Gobblin job:   job.name  - Name of the job    source.class  - Fully qualified path to the Source class responsible for connecting to the data source    writer.staging.dir  - The directory each task will write staging data to    writer.output.dir  - The directory each task will commit data to    data.publisher.final.dir  - The final directory where all the data will be published   state.store.dir  - The directory where state-store files will be written    For more information on each property, check out the comprehensive list below.    If only these properties are set, then by default, Gobblin will run in Local mode, as opposed to running on Hadoop M/R. This means Gobblin will write Avro data to the local filesystem. In order to write to HDFS, set the  writer.fs.uri  property to the URI of the HDFS NameNode that data should be written to. Since the default version of Gobblin writes data in Avro format, the writer expects Avro records to be passed to it. Thus, any data pulled from an external source must be converted to Avro before it can be written out to the filesystem.    The  source.class  property is one of the most important properties in Gobblin. It specifies what Source class to use. The Source class is responsible for determining what work needs to be done during each run of the job, and specifies what Extractor to use in order to read over each sub-unit of data. Examples of Source classes are  WikipediaSource  and  SimpleJsonSource , which can be found in the GitHub repository. For more information on Sources and Extractors, check out the  Architecture  page.    Typically, Gobblin jobs will be launched using the launch scripts in the  bin  folder. These scripts allow jobs to be launched on the local machine (e.g. SchedulerDaemon) or on Hadoop (e.g. CliMRJobLauncher). Check out the Job Launcher section below, to see the configuration difference between each launch mode. The  Deployment  page also has more information on the different ways a job can be launched.", 
            "title": "Creating a Basic Properties File "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#job-launcher-properties", 
            "text": "Gobblin jobs can be launched and scheduled in a variety of ways. They can be scheduled via a Quartz scheduler or through  Azkaban . Jobs can also be run without a scheduler via the Command Line. For more information on launching Gobblin jobs, check out the  Deployment  page.", 
            "title": "Job Launcher Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#common-job-launcher-properties", 
            "text": "These properties are common to both the Job Launcher and the Command Line.", 
            "title": "Common Job Launcher Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobname", 
            "text": "", 
            "title": "job.name"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description", 
            "text": "The name of the job to run. This name must be unique within a single Gobblin instance.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobgroup", 
            "text": "", 
            "title": "job.group"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_1", 
            "text": "A way to group logically similar jobs together.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_1", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_1", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobdescription", 
            "text": "", 
            "title": "job.description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_2", 
            "text": "A description of what the jobs does.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_2", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_2", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#joblockdir", 
            "text": "", 
            "title": "job.lock.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_3", 
            "text": "Directory where job locks are stored. Job locks are used by the scheduler to ensure two executions of a job do not run at the same time. If a job is scheduled to run, Gobblin will first check this directory to see if there is a lock file for the job. If there is one, it will not run the job, if there isn't one then it will run the job.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_3", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_3", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#joblockenabled", 
            "text": "", 
            "title": "job.lock.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_4", 
            "text": "If set to true job locks are enabled, if set to false they are disabled", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_4", 
            "text": "True", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_4", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobrunonce", 
            "text": "", 
            "title": "job.runonce"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_5", 
            "text": "A boolean specifying whether the job will be only once, or multiple times. If set to true the job will only be run once even if a job.schedule is specified. If set to false and a job.schedule is specified then it will run according to the schedule. If set false and a job.schedule is not specified, it will run only once.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_5", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_5", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobdisabled", 
            "text": "", 
            "title": "job.disabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_6", 
            "text": "Whether the job is disabled or not. If set to true, then Gobblin will not run this job.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_6", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_6", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#schedulerdaemon-properties", 
            "text": "This class is used to schedule Gobblin jobs on Quartz. The job can be launched via the command line, and takes in the location of a global configuration file as a parameter. This configuration file should have the property  jobconf.dir  in order to specify the location of all the  .job  or  .pull  files. Another core difference, is that the global configuration file for the SchedulerDaemon must specify the following properties:   writer.staging.dir     writer.output.dir     data.publisher.final.dir     state.store.dir      They should not be set in individual job files, as they are system-level parameters.\nFor more information on how to set the configuration parameters for jobs launched through the SchedulerDaemon, check out the  Deployment  page.", 
            "title": "SchedulerDaemon Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobschedule", 
            "text": "", 
            "title": "job.schedule"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_7", 
            "text": "Cron-Based job schedule. This schedule only applies to jobs that run using Quartz.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_7", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_7", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobconfdir", 
            "text": "", 
            "title": "jobconf.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_8", 
            "text": "When running in local mode, Gobblin will check this directory for any configuration files. Each configuration file should correspond to a separate Gobblin job, and each one should in a suffix specified by the jobconf.extensions parameter.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_8", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_8", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobconfextensions", 
            "text": "", 
            "title": "jobconf.extensions"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_9", 
            "text": "Comma-separated list of supported job configuration file extensions. When running in local mode, Gobblin will only pick up job files ending in these suffixes.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_9", 
            "text": "pull,job", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_9", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobconfmonitorinterval", 
            "text": "", 
            "title": "jobconf.monitor.interval"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_10", 
            "text": "Controls how often Gobblin checks the jobconf.dir for new configuration files, or for configuration file updates. The parameter is measured in milliseconds.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_10", 
            "text": "300000", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_10", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#climrjoblauncher-properties", 
            "text": "There are no configuration parameters specific to CliMRJobLauncher. This class is used to launch Gobblin jobs on Hadoop from the command line, the jobs are not scheduled. Common properties are set using the  --sysconfig  option when launching jobs via the command line. For more information on how to set the configuration parameters for jobs launched through the command line, check out the  Deployment  page.", 
            "title": "CliMRJobLauncher Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#azkabanjoblauncher-properties", 
            "text": "There are no configuration parameters specific to AzkabanJobLauncher. This class is used to schedule Gobblin jobs on Azkaban. Common properties can be set through Azkaban by creating a  .properties  file, check out the  Azkaban Documentation  for more information. For more information on how to set the configuration parameters for jobs scheduled through the Azkaban, check out the  Deployment  page.", 
            "title": "AzkabanJobLauncher Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#job-type-properties", 
            "text": "", 
            "title": "Job Type Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#common-job-type-properties", 
            "text": "", 
            "title": "Common Job Type Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#launchertype", 
            "text": "", 
            "title": "launcher.type"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_11", 
            "text": "Job launcher type; one of LOCAL, MAPREDUCE, YARN. LOCAL mode runs on a single machine (LocalJobLauncher), MAPREDUCE runs on a Hadoop cluster (MRJobLauncher), and YARN runs on a YARN cluster (not implemented yet).", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_11", 
            "text": "LOCAL", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_11", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#localjoblauncher-properties", 
            "text": "There are no configuration parameters specific to LocalJobLauncher. The LocalJobLauncher will launch a Hadoop job on a single machine. If launcher.type is set to LOCAL then this class will be used to launch the job.\nProperties required by the MRJobLauncher class.", 
            "title": "LocalJobLauncher Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#frameworkjars", 
            "text": "", 
            "title": "framework.jars"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_12", 
            "text": "Comma-separated list of jars the Gobblin framework depends on. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_12", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_12", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobjars", 
            "text": "", 
            "title": "job.jars"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_13", 
            "text": "Comma-separated list of jar files the job depends on. These jars will be added to the classpath of the job, and to the classpath of any containers the job launches.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_13", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_13", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#joblocalfiles", 
            "text": "", 
            "title": "job.local.files"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_14", 
            "text": "Comma-separated list of local files the job depends on. These files will be available to any map tasks that get launched via the DistributedCache.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_14", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_14", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobhdfsfiles", 
            "text": "", 
            "title": "job.hdfs.files"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_15", 
            "text": "Comma-separated list of files on HDFS the job depends on. These files will be available to any map tasks that get launched via the DistributedCache.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_15", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_15", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#mrjoblauncher-properties", 
            "text": "", 
            "title": "MRJobLauncher Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#mrjobrootdir", 
            "text": "", 
            "title": "mr.job.root.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_16", 
            "text": "Working directory for a Gobblin Hadoop MR job. Gobblin uses this to write intermediate data, such as the workunit state files that are used by each map task. This has to be a path on HDFS.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_16", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_16", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#mrjobmaxmappers", 
            "text": "", 
            "title": "mr.job.max.mappers"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_17", 
            "text": "Maximum number of mappers to use in a Gobblin Hadoop MR job. If no explicit limit is set then a map task for each workunit will be launched. If the value of this properties is less than the number of workunits created, then each map task will run multiple tasks.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_17", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_17", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#mrincludetaskcounters", 
            "text": "", 
            "title": "mr.include.task.counters"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_18", 
            "text": "Whether to include task-level counters in the set of counters reported as Hadoop counters. Hadoop imposes a system-level limit (default to 120) on the number of counters, so a Gobblin MR job may easily go beyond that limit if the job has a large number of tasks and each task has a few counters. This property gives users an option to not include task-level counters to avoid going over that limit.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_18", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_18", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#retry-properties", 
            "text": "Properties that control how tasks and jobs get retried on failure.", 
            "title": "Retry Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#workunitretryenabled", 
            "text": "", 
            "title": "workunit.retry.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_19", 
            "text": "Whether retries of failed work units across job runs are enabled or not.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_19", 
            "text": "True", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_19", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#workunitretrypolicy", 
            "text": "", 
            "title": "workunit.retry.policy"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_20", 
            "text": "Work unit retry policy, can be one of {always, never, onfull, onpartial}.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_20", 
            "text": "always", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_20", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#taskmaxretries", 
            "text": "", 
            "title": "task.maxretries"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_21", 
            "text": "Maximum number of task retries. A task will be re-tried this many times before it is considered a failure.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_21", 
            "text": "5", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_21", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#taskretryintervalinsec", 
            "text": "", 
            "title": "task.retry.intervalinsec"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_22", 
            "text": "Interval in seconds between task retries. The interval increases linearly with each retry. For example, if the first interval is 300 seconds, then the second one is 600 seconds, etc.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_22", 
            "text": "300", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_22", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jobmaxfailures", 
            "text": "", 
            "title": "job.max.failures"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_23", 
            "text": "Maximum number of failures before an alert email is triggered.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_23", 
            "text": "1", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#task-execution-properties", 
            "text": "These properties control how tasks get executed for a job. Gobblin uses thread pools in order to executes the tasks for a specific job. In local mode there is a single thread pool per job that executes all the tasks for a job. In MR mode there is a thread pool for each map task (or container), and all Gobblin tasks assigned to that mapper are executed in that thread pool.", 
            "title": "Task Execution Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#taskexecutorthreadpoolsize", 
            "text": "", 
            "title": "taskexecutor.threadpool.size"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_24", 
            "text": "Size of the thread pool used by task executor for task execution. Each task executor will spawn this many threads to execute any Tasks that is has been allocated.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_24", 
            "text": "10", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_23", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#tasktrackerthreadpoolcoresize", 
            "text": "", 
            "title": "tasktracker.threadpool.coresize"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_25", 
            "text": "Core size of the thread pool used by task tracker for task state tracking and reporting.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_25", 
            "text": "10", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_24", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#tasktrackerthreadpoolmaxsize", 
            "text": "", 
            "title": "tasktracker.threadpool.maxsize"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_26", 
            "text": "Maximum size of the thread pool used by task tracker for task state tracking and reporting.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_26", 
            "text": "10", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_25", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#taskretrythreadpoolcoresize", 
            "text": "", 
            "title": "taskretry.threadpool.coresize"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_27", 
            "text": "Core size of the thread pool used by the task executor for task retries.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_27", 
            "text": "2", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_26", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#taskretrythreadpoolmaxsize", 
            "text": "", 
            "title": "taskretry.threadpool.maxsize"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_28", 
            "text": "Maximum size of the thread pool used by the task executor for task retries.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_28", 
            "text": "2", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_27", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#taskstatusreportintervalinms", 
            "text": "", 
            "title": "task.status.reportintervalinms"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_29", 
            "text": "Task status reporting interval in milliseconds.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_29", 
            "text": "30000", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_28", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#state-store-properties", 
            "text": "", 
            "title": "State Store Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#statestoredir", 
            "text": "", 
            "title": "state.store.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_30", 
            "text": "Root directory where job and task state files are stored. The state-store is used by Gobblin to track state between different executions of a job. All state-store files will be written to this directory.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_30", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_29", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#statestorefsuri", 
            "text": "", 
            "title": "state.store.fs.uri"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_31", 
            "text": "File system URI for file-system-based state stores.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_31", 
            "text": "file:///", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_30", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#metrics-properties", 
            "text": "", 
            "title": "Metrics Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#metricsenabled", 
            "text": "", 
            "title": "metrics.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_32", 
            "text": "Whether metrics collecting and reporting are enabled or not.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_32", 
            "text": "True", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_31", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#metricsreportinterval", 
            "text": "", 
            "title": "metrics.report.interval"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_33", 
            "text": "Metrics reporting interval in milliseconds.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_33", 
            "text": "60000", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_32", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#metricslogdir", 
            "text": "", 
            "title": "metrics.log.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_34", 
            "text": "The directory where metric files will be written to.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_34", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_33", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#metricsreportingfileenabled", 
            "text": "", 
            "title": "metrics.reporting.file.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_35", 
            "text": "A boolean indicating whether or not metrics should be reported to a file.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_35", 
            "text": "True", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_34", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#metricsreportingjmxenabled", 
            "text": "", 
            "title": "metrics.reporting.jmx.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_36", 
            "text": "A boolean indicating whether or not metrics should be exposed via JMX.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_36", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_35", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#email-alert-properties", 
            "text": "", 
            "title": "Email Alert Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#emailalertenabled", 
            "text": "", 
            "title": "email.alert.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_37", 
            "text": "Whether alert emails are enabled or not. Email alerts are only sent out when jobs fail consecutively job.max.failures number of times.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_37", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_36", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#emailnotificationenabled", 
            "text": "", 
            "title": "email.notification.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_38", 
            "text": "Whether job completion notification emails are enabled or not. Notification emails are sent whenever the job completes, regardless of whether it failed or not.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_38", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_37", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#emailhost", 
            "text": "", 
            "title": "email.host"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_39", 
            "text": "Host name of the email server.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_39", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_38", 
            "text": "Yes, if email notifications or alerts are enabled.", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#emailsmtpport", 
            "text": "", 
            "title": "email.smtp.port"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_40", 
            "text": "SMTP port number.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_40", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_39", 
            "text": "Yes, if email notifications or alerts are enabled.", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#emailuser", 
            "text": "", 
            "title": "email.user"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_41", 
            "text": "User name of the sender email account.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_41", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_40", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#emailpassword", 
            "text": "", 
            "title": "email.password"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_42", 
            "text": "User password of the sender email account.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_42", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_41", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#emailfrom", 
            "text": "", 
            "title": "email.from"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_43", 
            "text": "Sender email address.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_43", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_42", 
            "text": "Yes, if email notifications or alerts are enabled.", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#emailtos", 
            "text": "", 
            "title": "email.tos"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_44", 
            "text": "Comma-separated list of recipient email addresses.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_44", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_43", 
            "text": "Yes, if email notifications or alerts are enabled.", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#source-properties", 
            "text": "", 
            "title": "Source Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#common-source-properties", 
            "text": "These properties are common properties that are used among different Source implementations. Depending on what source class is being used, these parameters may or may not be necessary. These parameters are not tied to a specific source, and thus can be used in new source classes.", 
            "title": "Common Source Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceclass", 
            "text": "", 
            "title": "source.class"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_45", 
            "text": "Fully qualified name of the Source class. For example, com.linkedin.gobblin.example.wikipedia", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_45", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_44", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceentity", 
            "text": "", 
            "title": "source.entity"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_46", 
            "text": "Name of the source entity that needs to be pulled from the source. The parameter represents a logical grouping of data that needs to be pulled from the source. Often this logical grouping comes in the form a database table, a source topic, etc. In many situations, such as when using the QueryBasedExtractor, it will be the name of the table that needs to pulled from the source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_46", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_45", 
            "text": "Required for QueryBasedExtractors, FileBasedExtractors.", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcetimezone", 
            "text": "", 
            "title": "source.timezone"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_47", 
            "text": "Timezone of the data being pulled in by the extractor. Examples include \"PST\" or \"UTC\".", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_47", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_46", 
            "text": "Required for QueryBasedExtractors", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcemaxnumberofpartitions", 
            "text": "", 
            "title": "source.max.number.of.partitions"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_48", 
            "text": "Maximum number of partitions to split this current run across. Only used by the QueryBasedSource and FileBasedSource.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_48", 
            "text": "20", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_47", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceskipfirstrecord", 
            "text": "", 
            "title": "source.skip.first.record"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_49", 
            "text": "True if you want to skip the first record of each data partition. Only used by the FileBasedExtractor.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_49", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_48", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#extractnamespace", 
            "text": "", 
            "title": "extract.namespace"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_50", 
            "text": "Namespace for the extract data. The namespace will be included in the default file name of the outputted data.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_50", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_49", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnuseproxyurl", 
            "text": "", 
            "title": "source.conn.use.proxy.url"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_51", 
            "text": "The URL of the proxy to connect to when connecting to the source. This parameter is only used for SFTP and REST sources.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_51", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_50", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnuseproxyport", 
            "text": "", 
            "title": "source.conn.use.proxy.port"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_52", 
            "text": "The port of the proxy to connect to when connecting to the source. This parameter is only used for SFTP and REST sources.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_52", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_51", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnusername", 
            "text": "", 
            "title": "source.conn.username"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_53", 
            "text": "The username to authenticate with the source. This is parameter is only used for SFTP and JDBC sources.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_53", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_52", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnpassword", 
            "text": "", 
            "title": "source.conn.password"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_54", 
            "text": "The password to use when authenticating with the source. This is parameter is only used for JDBC sources.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_54", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_53", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnhost", 
            "text": "", 
            "title": "source.conn.host"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_55", 
            "text": "The name of the host to connect to.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_55", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_54", 
            "text": "Required for SftpExtractor, MySQLExtractor, and SQLServerExtractor.", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnresturl", 
            "text": "", 
            "title": "source.conn.rest.url"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_56", 
            "text": "URL to connect to for REST requests. This parameter is only used for the Salesforce source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_56", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_55", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnversion", 
            "text": "", 
            "title": "source.conn.version"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_57", 
            "text": "Version number of communication protocol. This parameter is only used for the Salesforce source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_57", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_56", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconntimeout", 
            "text": "", 
            "title": "source.conn.timeout"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_58", 
            "text": "The timeout set for connecting to the source in milliseconds.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_58", 
            "text": "500000", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_57", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnport", 
            "text": "", 
            "title": "source.conn.port"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_59", 
            "text": "The value of the port to connect to.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_59", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_58", 
            "text": "Required for SftpExtractor, MySQLExtractor, SqlServerExtractor.", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#extracttablename", 
            "text": "", 
            "title": "extract.table.name"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_60", 
            "text": "Table name in Hadoop which is different table name in source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_60", 
            "text": "Source table name", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_59", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#extractisfull", 
            "text": "", 
            "title": "extract.is.full"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_61", 
            "text": "True if this pull should treat the data as a full dump of table from the source, false otherwise", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_61", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_60", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#extractdeltafields", 
            "text": "", 
            "title": "extract.delta.fields"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_62", 
            "text": "List of columns that will be used as the delta field for the data.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_62", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_61", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#extractprimarykeyfields", 
            "text": "", 
            "title": "extract.primary.key.fields"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_63", 
            "text": "List of columns that will be used as the primary key for the data.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_63", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_62", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#extractpulllimit", 
            "text": "", 
            "title": "extract.pull.limit"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_64", 
            "text": "This limits the number of records read by Gobblin. In Gobblin's extractor the readRecord() method is expected to return records until there are no more to pull, in which case it runs null. This parameter limits the number of times readRecord() is executed. This parameter is useful for pulling a limited sample of the source data for testing purposes.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_64", 
            "text": "Unbounded", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_63", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#extractfullruntime", 
            "text": "", 
            "title": "extract.full.run.time"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_65", 
            "text": "", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_65", 
            "text": "", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_64", 
            "text": "", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#querybasedextractor-properties", 
            "text": "The following table lists the query based extractor configuration properties.", 
            "title": "QueryBasedExtractor Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedwatermarktype", 
            "text": "", 
            "title": "source.querybased.watermark.type"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_66", 
            "text": "The format of the watermark that is used when extracting data from the source. Possible types are timestamp, date, hour, simple.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_66", 
            "text": "timestamp", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_65", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedstartvalue", 
            "text": "", 
            "title": "source.querybased.start.value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_67", 
            "text": "Value for the watermark to start pulling data from, also the default watermark if the previous watermark cannot be found in the old task states.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_67", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_66", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedpartitioninterval", 
            "text": "", 
            "title": "source.querybased.partition.interval"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_68", 
            "text": "Number of hours to pull in each partition.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_68", 
            "text": "1", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_67", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedhourcolumn", 
            "text": "", 
            "title": "source.querybased.hour.column"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_69", 
            "text": "Delta column with hour for hourly extracts (Ex: hour_sk)", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_69", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_68", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedskiphighwatermarkcalc", 
            "text": "", 
            "title": "source.querybased.skip.high.watermark.calc"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_70", 
            "text": "If it is true, skips high watermark calculation in the source and it will use partition higher range as high watermark instead of getting it from source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_70", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_69", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedquery", 
            "text": "", 
            "title": "source.querybased.query"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_71", 
            "text": "The query that the extractor should execute to pull data.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_71", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_70", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedhourlyextract", 
            "text": "", 
            "title": "source.querybased.hourly.extract"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_72", 
            "text": "True if hourly extract is required.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_72", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_71", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedextracttype", 
            "text": "", 
            "title": "source.querybased.extract.type"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_73", 
            "text": "\"snapshot\" for the incremental dimension pulls. \"append_daily\", \"append_hourly\" and \"append_batch\" for the append data append_batch for the data with sequence numbers as watermarks", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_73", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_72", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedendvalue", 
            "text": "", 
            "title": "source.querybased.end.value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_74", 
            "text": "The high watermark which this entire job should pull up to. If this is not specified, pull entire data from the table", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_74", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_73", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedappendmaxwatermarklimit", 
            "text": "", 
            "title": "source.querybased.append.max.watermark.limit"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_75", 
            "text": "max limit of the high watermark for the append data.  CURRENT_DATE - X CURRENT_HOUR - X where X =1", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_75", 
            "text": "CURRENT_DATE for daily extract CURRENT_HOUR for hourly extract", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_74", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasediswatermarkoverride", 
            "text": "", 
            "title": "source.querybased.is.watermark.override"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_76", 
            "text": "True if this pull should override previous watermark with start.value and end.value. False otherwise.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_76", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_75", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedlowwatermarkbackupsecs", 
            "text": "", 
            "title": "source.querybased.low.watermark.backup.secs"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_77", 
            "text": "Number of seconds that needs to be backup from the previous high watermark. This is to cover late data.  Ex: Set to 3600 to cover 1 hour late data.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_77", 
            "text": "0", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_76", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedschema", 
            "text": "", 
            "title": "source.querybased.schema"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_78", 
            "text": "Database name", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_78", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_77", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedisspecificapiactive", 
            "text": "", 
            "title": "source.querybased.is.specific.api.active"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_79", 
            "text": "True if this pull needs to use source specific apis instead of standard protocols.  Ex: Use salesforce bulk api instead of rest api", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_79", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_78", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedskipcountcalc", 
            "text": "", 
            "title": "source.querybased.skip.count.calc"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_80", 
            "text": "A boolean, if true then the QueryBasedExtractor will skip the source count calculation.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_80", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_79", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedfetchsize", 
            "text": "", 
            "title": "source.querybased.fetch.size"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_81", 
            "text": "This parameter is currently only used in JDBCExtractor. The JDBCExtractor will process this many number of records from the JDBC ResultSet at a time. It will then take these records and return them to the rest of the Gobblin flow so that they can get processed by the rest of the Gobblin components.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_81", 
            "text": "1000", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_80", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedismetadatacolumncheckenabled", 
            "text": "", 
            "title": "source.querybased.is.metadata.column.check.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_82", 
            "text": "When a query is specified in the configuration file, it is possible a user accidentally adds in a column name that does not exist on the source side. By default, this parameter is set to false, which means that if a column is specified in the query and it does not exist in the source data set, Gobblin will just skip over that column. If it is set to true, Gobblin will actually take the config specified column and check to see if it exists in the source data set. If it doesn't exist then the job will fail.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_82", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_81", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasediscompressionenabled", 
            "text": "", 
            "title": "source.querybased.is.compression.enabled"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_83", 
            "text": "A boolean specifying whether or not compression should be enabled when pulling data from the source. This parameter is only used for MySQL sources. If set to true, the MySQL will send compressed data back to the source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_83", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_82", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcequerybasedjdbcresultsetfetchsize", 
            "text": "", 
            "title": "source.querybased.jdbc.resultset.fetch.size"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_84", 
            "text": "The number of rows to pull through JDBC at a time. This is useful when the JDBC ResultSet is too big to fit into memory, so only \"x\" number of records will be fetched at a time.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_84", 
            "text": "1000", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_83", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jdbcextractor-properties", 
            "text": "The following table lists the jdbc based extractor configuration properties.", 
            "title": "JdbcExtractor Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconndriver", 
            "text": "", 
            "title": "source.conn.driver"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_85", 
            "text": "The fully qualified path of the JDBC driver used to connect to the external source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_85", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_84", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcecolumnnamecase", 
            "text": "", 
            "title": "source.column.name.case"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_86", 
            "text": "A enum specifying whether or not to convert the column names to a specific case before performing a query. Possible values are TOUPPER or TOLOWER.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_86", 
            "text": "NOCHANGE", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_85", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#filebasedextractor-properties", 
            "text": "The following table lists the file based extractor configuration properties.", 
            "title": "FileBasedExtractor Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcefilebaseddatadirectory", 
            "text": "", 
            "title": "source.filebased.data.directory"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_87", 
            "text": "The data directory from which to pull data from.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_87", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_86", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcefilebasedfilestopull", 
            "text": "", 
            "title": "source.filebased.files.to.pull"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_88", 
            "text": "A list of files to pull - this should be set in the Source class and the extractor will pull the specified files.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_88", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_87", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#filebasedreportstatusoncount", 
            "text": "", 
            "title": "filebased.report.status.on.count"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_89", 
            "text": "The FileBasedExtractor will report it's status every time it processes the number of records specified by this parameter. The way it reports status is by logging out how many records it has seen.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_89", 
            "text": "10000", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_88", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcefilebasedfsuri", 
            "text": "", 
            "title": "source.filebased.fs.uri"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_90", 
            "text": "The URI of the filesystem to connect to.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_90", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_89", 
            "text": "Required for HadoopExtractor.", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourcefilebasedpreservefilename", 
            "text": "", 
            "title": "source.filebased.preserve.file.name"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_91", 
            "text": "A boolean, if true then the original file names will be preserved when they are are written to the source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_91", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_90", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceschema", 
            "text": "", 
            "title": "source.schema"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_92", 
            "text": "The schema of the data that will be pulled by the source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_92", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_91", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sftpextractor-properties", 
            "text": "", 
            "title": "SftpExtractor Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnprivatekey", 
            "text": "", 
            "title": "source.conn.private.key"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_93", 
            "text": "File location of the private key used for key based authentication. This parameter is only used for the SFTP source.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_93", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_92", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#sourceconnknownhosts", 
            "text": "", 
            "title": "source.conn.known.hosts"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_94", 
            "text": "File location of the known hosts file used for key based authentication.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_94", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_93", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converter-properties", 
            "text": "Properties for Gobblin converters.", 
            "title": "Converter Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converterclasses", 
            "text": "", 
            "title": "converter.classes"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_95", 
            "text": "Comma-separated list of fully qualified names of the Converter classes. The order is important as the converters will be applied in this order.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_95", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_94", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#csvtojsonconverter-properties", 
            "text": "This converter takes in text data separated by a delimiter (converter.csv.to.json.delimiter), and splits the data into a JSON format recognized by JsonIntermediateToAvroConverter.", 
            "title": "CsvToJsonConverter Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#convertercsvtojsondelimiter", 
            "text": "", 
            "title": "converter.csv.to.json.delimiter"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_96", 
            "text": "The regex delimiter between CSV based files, only necessary when using the CsvToJsonConverter - e.g. \",\", \"/t\" or some other regex", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_96", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_95", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#jsonintermediatetoavroconverter-properties", 
            "text": "This converter takes in JSON data in a specific schema, and converts it to Avro data.", 
            "title": "JsonIntermediateToAvroConverter Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converteravrodateformat", 
            "text": "", 
            "title": "converter.avro.date.format"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_97", 
            "text": "Source format of the date columns for Avro-related converters.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_97", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_96", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converteravrotimestampformat", 
            "text": "", 
            "title": "converter.avro.timestamp.format"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_98", 
            "text": "Source format of the timestamp columns for Avro-related converters.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_98", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_97", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converteravrotimeformat", 
            "text": "", 
            "title": "converter.avro.time.format"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_99", 
            "text": "Source format of the time columns for Avro-related converters.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_99", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_98", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converteravrobinarycharset", 
            "text": "", 
            "title": "converter.avro.binary.charset"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_100", 
            "text": "Source format of the time columns for Avro-related converters.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_100", 
            "text": "UTF-8", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_99", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converterisepochtimeinseconds", 
            "text": "", 
            "title": "converter.is.epoch.time.in.seconds"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_101", 
            "text": "A boolean specifying whether or not a epoch time field in the JSON object is in seconds or not.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_101", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_100", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converteravromaxconversionfailures", 
            "text": "", 
            "title": "converter.avro.max.conversion.failures"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_102", 
            "text": "This converter is will fail for this many number of records before throwing an exception.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_102", 
            "text": "0", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_101", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#avrofilterconverter-properties", 
            "text": "This converter takes in an Avro record, and filters out records by performing an equality operation on the value of the field specified by converter.filter.field and the value specified in converter.filter.value. It returns the record unmodified if the equality operation evaluates to true, false otherwise.", 
            "title": "AvroFilterConverter Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converterfilterfield", 
            "text": "", 
            "title": "converter.filter.field"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_103", 
            "text": "The name of the field in the Avro record, for which the converter will filter records on.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_103", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_102", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converterfiltervalue", 
            "text": "", 
            "title": "converter.filter.value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_104", 
            "text": "The value that will be used in the equality operation to filter out records.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_104", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_103", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#avrofieldretrieverconverter-properties", 
            "text": "This converter takes a specific field from an Avro record and returns its value.", 
            "title": "AvroFieldRetrieverConverter Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#converteravroextractorfieldpath", 
            "text": "", 
            "title": "converter.avro.extractor.field.path"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_105", 
            "text": "The field in the Avro record to retrieve. If it is a nested field, then each level must be separated by a period.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_105", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_104", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#fork-properties", 
            "text": "Properties for Gobblin's fork operator.", 
            "title": "Fork Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#forkoperatorclass", 
            "text": "", 
            "title": "fork.operator.class"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_106", 
            "text": "Fully qualified name of the ForkOperator class.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_106", 
            "text": "com.linkedin.uif.fork.IdentityForkOperator", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_105", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#forkbranches", 
            "text": "", 
            "title": "fork.branches"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_107", 
            "text": "Number of fork branches.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_107", 
            "text": "1", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_106", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#forkbranchnamebranch-index", 
            "text": "", 
            "title": "fork.branch.name.${branch index}"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_108", 
            "text": "Name of a fork branch with the given index, e.g., 0 and 1.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_108", 
            "text": "fork_${branch index}, e.g., fork_0 and fork_1.", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_107", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#quality-checker-properties", 
            "text": "", 
            "title": "Quality Checker Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#qualitycheckertaskpolicies", 
            "text": "", 
            "title": "qualitychecker.task.policies"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_109", 
            "text": "Comma-separted list of fully qualified names of the TaskLevelPolicy classes that will run at the end of each Task.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_109", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_108", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#qualitycheckertaskpolicytypes", 
            "text": "", 
            "title": "qualitychecker.task.policy.types"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_110", 
            "text": "OPTIONAL implies the corresponding class in qualitychecker.task.policies is optional and if it fails the Task will still succeed, FAIL implies that if the corresponding class fails then the Task will fail too.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_110", 
            "text": "OPTIONAL", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_109", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#qualitycheckerrowpolicies", 
            "text": "", 
            "title": "qualitychecker.row.policies"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_111", 
            "text": "Comma-separted list of fully qualified names of the RowLevelPolicy classes that will run on each record.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_111", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_110", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#qualitycheckerrowpolicytypes", 
            "text": "", 
            "title": "qualitychecker.row.policy.types"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_112", 
            "text": "OPTIONAL implies the corresponding class in qualitychecker.row.policies is optional and if it fails the Task will still succeed, FAIL implies that if the corresponding class fails then the Task will fail too, ERR_FILE implies that if the record does not pass the test then the record will be written to an error file.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_112", 
            "text": "OPTIONAL", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_111", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#qualitycheckerrowerrfile", 
            "text": "", 
            "title": "qualitychecker.row.err.file"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_113", 
            "text": "The quality checker will write the current record to the location specified by this parameter, if the current record fails to pass the quality checkers specified by qualitychecker.row.policies; this file will only be written to if the quality checker policy type is ERR_FILE.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_113", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_112", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writer-properties", 
            "text": "", 
            "title": "Writer Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerdestinationtype", 
            "text": "", 
            "title": "writer.destination.type"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_114", 
            "text": "Writer destination type; currently only writing to HDFS is supported.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_114", 
            "text": "HDFS", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_113", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writeroutputformat", 
            "text": "", 
            "title": "writer.output.format"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_115", 
            "text": "Writer output format; currently only Avro is supported.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_115", 
            "text": "AVRO", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_114", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerfsuri", 
            "text": "", 
            "title": "writer.fs.uri"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_116", 
            "text": "File system URI for writer output.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_116", 
            "text": "file:///", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_115", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerstagingdir", 
            "text": "", 
            "title": "writer.staging.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_117", 
            "text": "Staging directory of writer output. All staging data that the writer produces will be placed in this directory, but all the data will be eventually moved to the writer.output.dir.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_117", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_116", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writeroutputdir", 
            "text": "", 
            "title": "writer.output.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_118", 
            "text": "Output directory of writer output. All output data that the writer produces will be placed in this directory, but all the data will be eventually moved to the final directory by the publisher.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_118", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_117", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerbuilderclass", 
            "text": "", 
            "title": "writer.builder.class"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_119", 
            "text": "Fully qualified name of the writer builder class.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_119", 
            "text": "com.linkedin.uif.writer.AvroDataWriterBuilder", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_118", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerfilepath", 
            "text": "", 
            "title": "writer.file.path"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_120", 
            "text": "The Path where the writer will write it's data. Data in this directory will be copied to it's final output directory by the DataPublisher.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_120", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_119", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerfilename", 
            "text": "", 
            "title": "writer.file.name"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_121", 
            "text": "The name of the file the writer writes to.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_121", 
            "text": "part", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_120", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerpartitionerclass", 
            "text": "", 
            "title": "writer.partitioner.class"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_122", 
            "text": "Partitioner used for distributing records into multiple output files.  writer.builder.class  must be a subclass of  PartitionAwareDataWriterBuilder , otherwise Gobblin will throw an error.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_122", 
            "text": "None (will not use partitioner)", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_121", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerbuffersize", 
            "text": "", 
            "title": "writer.buffer.size"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_123", 
            "text": "Writer buffer size in bytes. This parameter is only applicable for the AvroHdfsDataWriter.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_123", 
            "text": "4096", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_122", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writerdeflatelevel", 
            "text": "", 
            "title": "writer.deflate.level"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_124", 
            "text": "Writer deflate level. Deflate is a type of compression for Avro data.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_124", 
            "text": "9", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_123", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writercodectype", 
            "text": "", 
            "title": "writer.codec.type"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_125", 
            "text": "This is used to specify the type of compression used when writing data out. Possible values are NOCOMPRESSION, DEFLATE, SNAPPY.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_125", 
            "text": "DEFLATE", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_124", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#writereagerinitialization", 
            "text": "", 
            "title": "writer.eager.initialization"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_126", 
            "text": "This is used to control the writer creation. If the value is set to true, writer is created before records are read. This means an empty file will be created even if no records were read.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_126", 
            "text": "False", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_125", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#data-publisher-properties", 
            "text": "", 
            "title": "Data Publisher Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#datapublishertype", 
            "text": "", 
            "title": "data.publisher.type"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_127", 
            "text": "The fully qualified name of the DataPublisher class to run. The DataPublisher is responsible for publishing task data once all Tasks have been completed.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_127", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_126", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#datapublisherfinaldir", 
            "text": "", 
            "title": "data.publisher.final.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_128", 
            "text": "The final output directory where the data should be published.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_128", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_127", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#datapublisherreplacefinaldir", 
            "text": "", 
            "title": "data.publisher.replace.final.dir"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_129", 
            "text": "A boolean, if true and the the final output directory already exists, then the data will not be committed. If false and the final output directory already exists then it will be overwritten.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_129", 
            "text": "None", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_128", 
            "text": "Yes", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#datapublisherfinalname", 
            "text": "", 
            "title": "data.publisher.final.name"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_130", 
            "text": "The final name of the file that is produced by Gobblin. By default, Gobblin already assigns a unique name to each file it produces. If that default name needs to be overridden then this parameter can be used. Typically, this parameter should be set on a per workunit basis so that file names don't collide.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_130", 
            "text": "", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_129", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#generic-properties", 
            "text": "These properties are used throughout multiple Gobblin components.", 
            "title": "Generic Properties "
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#fsuri", 
            "text": "", 
            "title": "fs.uri"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#description_131", 
            "text": "Default file system URI for all file storage; over-writable by more specific configuration properties.", 
            "title": "Description"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#default-value_131", 
            "text": "file:///", 
            "title": "Default Value"
        }, 
        {
            "location": "/user-guide/Configuration-Properties-Glossary/#required_130", 
            "text": "No", 
            "title": "Required"
        }, 
        {
            "location": "/user-guide/Partitioned-Writers/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nExisting Partition Aware Writers\n\n\nExisting Partitioners\n\n\nDesign\n\n\nImplementing a partitioner\n\n\nImplementing a Partition Aware Writer Builder\n\n\n\n\n\n\nGobblin allows partitioning output data using a writer partitioner. This allows, for example, to write timestamped records to a different file depending on the timestamp of the record.\n\n\nTo partition output records, two things are needed:\n\n\n\n\nSet \nwriter.builder.class\n to a class that implements \nPartitionAwareDataWriterBuilder\n.\n\n\nSet \nwriter.partitioner.class\n to the class of the desired partitioner, which must be subclass of \nWriterPartitioner\n. The partitioner will get all Gobblin configuration options, so some partitioners may require additional configurations.\n\n\n\n\nIf \nwriter.partitioner.class\n is set but \nwriter.builder.class\n is not a \nPartitionAwareDataWriterBuilder\n, Gobblin will throw an error. If \nwriter.builder.class\n is a \nPartitionAwareDataWriterBuilder\n, but no partitioner is set, Gobblin will attempt to still create the writer with no partition, however, the writer may not support unpartitioned data, in which case it will throw an error.\n\n\nWriterPartitioner\ns compute a partition key for each record. Some \nPartitionAwareDataWriterBuilder\n are unable to handle certain partition keys (for example, a writer that can only partition by date would expect a partition schema that only contains date information). If the writer cannot handle the partitioner key, Gobblin will throw an error. The Javadoc of partitioners should always include the schema it emits and the writer Javadoc should contain which schemas it accepts for ease of use.\n\n\nExisting Partition Aware Writers\n\n\n\n\ngobblin.writer.AvroDataWriterBuilder\n: If partition is present, creates directory structure based on partition. For example, if partition is \n{name=\"foo\", type=\"bar\"}\n, the record will be written to a file in directory \n/path/to/data/name=foo/type=bar/file.avro\n.  \n\n\n\n\nExisting Partitioners\n\n\n\n\ngobblin.example.wikipedia.WikipediaPartitioner\n: Sample partitioner for the Wikipedia example. Partitions record by article title.\n\n\n\n\nDesign\n\n\n\n\nGobblin always instantiates a \nPartitionedDataWriter\n for each fork. On construction, the partitioned writer:\n\n\n\n\nchecks whether a partitioner is present in the configuration. If no partitioner is present, then the instance of \nPartitionedDataWriter\n is simply a thin wrapper around a normal writer. \n\n\nIf a partitioner is present, the partitioned writer will check if the class configured at \nwriter.builder.class\n is an instance of \nPartitionAwareDataWriterBuilder\n, throwing an error in case this is not true.  \n\n\nThe partitioned writer instantiate the partitioner, runs \npartitionSchema()\n, and then checks whether the partition aware writer builder accepts that schema using \nvalidatePartitionSchema\n. If this returns false, Gobblin will throw an error.\n\n\n\n\nEvery time the partitioned writer gets a record, it uses the partitioner to get a partition key for that record. The partitioned writer keeps an internal map from partition key to instances of writers for each partition. If a writer is already created for this key, it will call write on that writer for the new record. If the writer is not present, the partitioned writer will instantiate a new writer with the computed partition, and then pass in the record.\n\n\nWriterPartitioner\n partitions records by returning a partition key for each record, which is of type \nGenericRecord\n. Each \nWriterPartitioner\n emits keys with a particular \nSchema\n which is available by using the method \nWriterPartitioner#partitionSchema()\n. Implementations of \nPartitionAwareDataWriterBuilder\n must check the partition schema to decide if they can understand and correctly handle that schema when the method \nPartitionAwareDataWriterBuilder#validateSchema\n is called (for example, a writer that can only partition by date would expect a partition schema that only contains date information). If the writer rejects the partition schema, then Gobblin will throw an error before writing anything.\n\n\nImplementing a partitioner\n\n\nThe interface for a partitioner is\n\n\n/**\n * Partitions records in the writer phase.\n */\npublic interface WriterPartitioner\nD\n {\n  /**\n   * @return The schema that {@link GenericRecord} returned by {@link #partitionForRecord} will have.\n   */\n  public Schema partitionSchema();\n\n  /**\n   * Returns the partition that the input record belongs to. If\n   * partitionFoRecord(record1).equals(partitionForRecord(record2)), then record1 and record2\n   * belong to the same partition.\n   * @param record input to compute partition for.\n   * @return {@link GenericRecord} representing partition record belongs to.\n   */\n  public GenericRecord partitionForRecord(D record);\n}\n\n\n\n\nFor an example of a partitioner implementation see \ngobblin.example.wikipedia.WikipediaPartitioner\n.\n\n\nEach class that implements \nWriterPartitioner\n is required to have a public constructor with signature \n(State state, int numBranches, int branchId)\n.\n\n\nImplementing a Partition Aware Writer Builder\n\n\nThis is very similar to a regular \nDataWriterBuilder\n, with two differences:\n\n\n\n\nYou must implement the method \nvalidatePartitionSchema(Schema)\n that must return false unless the builder can handle that schema.\n\n\nThe field \npartition\n is available, which is a \nGenericRecord\n that contains the partition key for the built writer. For any two different keys, Gobblin may create a writer for each key, so it is important that writers for different keys do not collide (e.g. do not try to use the same path).\n\n\n\n\nFor an example of a simple \nPartitionAwareWriterBuilder\n see \ngobblin.writer.AvroDataWriterBuilder\n.", 
            "title": "Partitioned Writers"
        }, 
        {
            "location": "/user-guide/Partitioned-Writers/#table-of-contents", 
            "text": "Table of Contents  Existing Partition Aware Writers  Existing Partitioners  Design  Implementing a partitioner  Implementing a Partition Aware Writer Builder    Gobblin allows partitioning output data using a writer partitioner. This allows, for example, to write timestamped records to a different file depending on the timestamp of the record.  To partition output records, two things are needed:   Set  writer.builder.class  to a class that implements  PartitionAwareDataWriterBuilder .  Set  writer.partitioner.class  to the class of the desired partitioner, which must be subclass of  WriterPartitioner . The partitioner will get all Gobblin configuration options, so some partitioners may require additional configurations.   If  writer.partitioner.class  is set but  writer.builder.class  is not a  PartitionAwareDataWriterBuilder , Gobblin will throw an error. If  writer.builder.class  is a  PartitionAwareDataWriterBuilder , but no partitioner is set, Gobblin will attempt to still create the writer with no partition, however, the writer may not support unpartitioned data, in which case it will throw an error.  WriterPartitioner s compute a partition key for each record. Some  PartitionAwareDataWriterBuilder  are unable to handle certain partition keys (for example, a writer that can only partition by date would expect a partition schema that only contains date information). If the writer cannot handle the partitioner key, Gobblin will throw an error. The Javadoc of partitioners should always include the schema it emits and the writer Javadoc should contain which schemas it accepts for ease of use.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Partitioned-Writers/#existing-partition-aware-writers", 
            "text": "gobblin.writer.AvroDataWriterBuilder : If partition is present, creates directory structure based on partition. For example, if partition is  {name=\"foo\", type=\"bar\"} , the record will be written to a file in directory  /path/to/data/name=foo/type=bar/file.avro .", 
            "title": "Existing Partition Aware Writers"
        }, 
        {
            "location": "/user-guide/Partitioned-Writers/#existing-partitioners", 
            "text": "gobblin.example.wikipedia.WikipediaPartitioner : Sample partitioner for the Wikipedia example. Partitions record by article title.", 
            "title": "Existing Partitioners"
        }, 
        {
            "location": "/user-guide/Partitioned-Writers/#design", 
            "text": "Gobblin always instantiates a  PartitionedDataWriter  for each fork. On construction, the partitioned writer:   checks whether a partitioner is present in the configuration. If no partitioner is present, then the instance of  PartitionedDataWriter  is simply a thin wrapper around a normal writer.   If a partitioner is present, the partitioned writer will check if the class configured at  writer.builder.class  is an instance of  PartitionAwareDataWriterBuilder , throwing an error in case this is not true.    The partitioned writer instantiate the partitioner, runs  partitionSchema() , and then checks whether the partition aware writer builder accepts that schema using  validatePartitionSchema . If this returns false, Gobblin will throw an error.   Every time the partitioned writer gets a record, it uses the partitioner to get a partition key for that record. The partitioned writer keeps an internal map from partition key to instances of writers for each partition. If a writer is already created for this key, it will call write on that writer for the new record. If the writer is not present, the partitioned writer will instantiate a new writer with the computed partition, and then pass in the record.  WriterPartitioner  partitions records by returning a partition key for each record, which is of type  GenericRecord . Each  WriterPartitioner  emits keys with a particular  Schema  which is available by using the method  WriterPartitioner#partitionSchema() . Implementations of  PartitionAwareDataWriterBuilder  must check the partition schema to decide if they can understand and correctly handle that schema when the method  PartitionAwareDataWriterBuilder#validateSchema  is called (for example, a writer that can only partition by date would expect a partition schema that only contains date information). If the writer rejects the partition schema, then Gobblin will throw an error before writing anything.", 
            "title": "Design"
        }, 
        {
            "location": "/user-guide/Partitioned-Writers/#implementing-a-partitioner", 
            "text": "The interface for a partitioner is  /**\n * Partitions records in the writer phase.\n */\npublic interface WriterPartitioner D  {\n  /**\n   * @return The schema that {@link GenericRecord} returned by {@link #partitionForRecord} will have.\n   */\n  public Schema partitionSchema();\n\n  /**\n   * Returns the partition that the input record belongs to. If\n   * partitionFoRecord(record1).equals(partitionForRecord(record2)), then record1 and record2\n   * belong to the same partition.\n   * @param record input to compute partition for.\n   * @return {@link GenericRecord} representing partition record belongs to.\n   */\n  public GenericRecord partitionForRecord(D record);\n}  For an example of a partitioner implementation see  gobblin.example.wikipedia.WikipediaPartitioner .  Each class that implements  WriterPartitioner  is required to have a public constructor with signature  (State state, int numBranches, int branchId) .", 
            "title": "Implementing a partitioner"
        }, 
        {
            "location": "/user-guide/Partitioned-Writers/#implementing-a-partition-aware-writer-builder", 
            "text": "This is very similar to a regular  DataWriterBuilder , with two differences:   You must implement the method  validatePartitionSchema(Schema)  that must return false unless the builder can handle that schema.  The field  partition  is available, which is a  GenericRecord  that contains the partition key for the built writer. For any two different keys, Gobblin may create a writer for each key, so it is important that writers for different keys do not collide (e.g. do not try to use the same path).   For an example of a simple  PartitionAwareWriterBuilder  see  gobblin.writer.AvroDataWriterBuilder .", 
            "title": "Implementing a Partition Aware Writer Builder"
        }, 
        {
            "location": "/user-guide/Monitoring/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nOverview\n\n\nMetrics Collecting and Reporting\n\n\nMetrics Reporting\n\n\nMetrics collection\n\n\nJVM Metrics\n\n\nPre-defined Job Execution Metrics\n\n\n\n\n\n\nJob Execution History Store\n\n\nEmail Notifications\n\n\n\n\n\n\nOverview\n\n\nAs a framework for ingesting potentially huge volume of data from many different sources, it's critical to monitor the health and status of the system and job executions. Gobblin employs a variety of approaches introduced below for this purpose. All the approaches are optional and can be configured to be turned on and off in different combinations through the framework and job configurations. \n\n\nMetrics Collecting and Reporting\n\n\nMetrics Reporting\n\n\nOut-of-the-box, Gobblin reports metrics though:\n\n\n\n\nJMX\n : used in the standalone deployment. Metrics reported to JMX can be checked using using tools such as \nVisualVM\n or JConsole. \n\n\nMetric log files\n: Files are stored in a root directory defined by the property \nmetrics.log.dir\n. Each Gobblin job has its own subdirectory under the root directory and each run of the job has its own metric log file named after the job ID as \n${job_id}.metrics.log\n.\n\n\nHadoop counters\n : used for M/R deployments. Gobblin-specific metrics are reported in the \"JOB\" or \"TASK\" groups for job- and task- level metrics. By default, task-level metrics are not reported through Hadoop counters as doing so may cause the number of Hadoop counters to go beyond the system-wide limit. However, users can choose to turn on reporting task-level metrics as Hadoop counters by setting \nmr.include.task.counters=true\n. \n\n\n\n\nMetrics collection\n\n\nJVM Metrics\n\n\nThe standalone deployment of Gobblin runs in a single JVM so it's important to monitor the health of the JVM, through a set of pre-defined JVM metrics in the following four categories. \n\n\n\n\njvm.gc\n: this covers metrics related to garbage collection, e.g., counts and time spent on garbage collection.\n\n\njvm.memory\n: this covers metrics related to memory usage, e.g., detailed heap usage.  \n\n\njvm.threads\n: this covers metrics related to thread states, e.g., thread count and thread deadlocks.\n\n\njvm.fileDescriptorRatio\n: this measures the ratio of open file descriptors.\n\n\n\n\nAll JVM metrics are reported via JMX and can be checked using tools such as \nVisualVM\n or JConsole. \n\n\nPre-defined Job Execution Metrics\n\n\nInternally, Gobblin pre-defines a minimum set of metrics listed below in two metric groups: \nJOB\n and \nTASK\n for job-level metrics and task-level metrics, respectively. Those metrics are useful in keeping track of the progress and performance of job executions.\n\n\n\n\n${metric_group}.${id}.records\n: this metric keeps track of the total number of data records extracted by the job or task depending on the \n${metric_group}\n. The \n${id}\n is either a job ID or a task ID depending on the \n${metric_group}\n. \n\n\n${metric_group}.${id}.recordsPerSecond\n: this metric keeps track of the rate of data extraction as data records extracted per second by the job or task depending on the \n${metric_group}\n.\n\n\n${metric_group}.${id}.bytes\n: this metric keeps track of the total number of bytes extracted by the job or task depending on the \n${metric_group}\n.\n\n\n${metric_group}.${id}.bytesPerSecond\n: this metric keeps track of the rate of data extraction as bytes extracted per second by the job or task depending on the \n${metric_group}\n.\n\n\n\n\nAmong the above metrics, \n${metric_group}.${id}.records\n and \n${metric_group}.${id}.bytes\n are reported as Hadoop MapReduce counters for Gobblin jobs running on Hadoop.\n\n\nJob Execution History Store\n\n\nGobblin also supports writing job execution information to a job execution history store backed by a database of choice. Gobblin uses MySQL by default and it ships with the SQL \nDDLs\n of the relevant MySQL tables, although  it still allows users to choose which database to use as long as the schema of the tables is compatible. Users can use the properties \njob.history.store.url\n and \njob.history.store.jdbc.driver\n to specify the database URL and the JDBC driver to work with the database of choice. The user name and password used to access the database can be specified using the properties \njob.history.store.user\n and \njob.history.store.password\n. An example configuration is shown below:\n\n\njob.history.store.url=jdbc:mysql://localhost/gobblin\njob.history.store.jdbc.driver=com.mysql.jdbc.Driver\njob.history.store.user=gobblin\njob.history.store.password=gobblin\n\n\n\n\nEmail Notifications\n\n\nIn addition to writing job execution information to the job execution history store, Gobblin also supports sending email notifications about job status. Job status notifications fall into two categories: alerts in case of job failures and normal notifications in case of successful job completions. Users can choose to enable or disable both categories using the properties \nemail.alert.enabled\n and \nemail.notification.enabled\n. \n\n\nThe main content of an email alert or notification is a job status report in Json format. Below is an example job status report:\n\n\n{\n    \njob name\n: \nGobblin_Demo_Job\n,\n    \njob id\n: \njob_Gobblin_Demo_Job_1417487480842\n,\n    \njob state\n: \nCOMMITTED\n,\n    \nstart time\n: 1417487480874,\n    \nend time\n: 1417490858913,\n    \nduration\n: 3378039,\n    \ntasks\n: 1,\n    \ncompleted tasks\n: 1,\n    \ntask states\n: [\n        {\n            \ntask id\n: \ntask_Gobblin_Demo_Job_1417487480842_0\n,\n            \ntask state\n: \nCOMMITTED\n,\n            \nstart time\n: 1417490795903,\n            \nend time\n: 1417490858908,\n            \nduration\n: 63005,\n            \nhigh watermark\n: -1,\n            \nexception\n: \n\n        }\n    ]\n}", 
            "title": "Monitoring"
        }, 
        {
            "location": "/user-guide/Monitoring/#table-of-contents", 
            "text": "Table of Contents  Overview  Metrics Collecting and Reporting  Metrics Reporting  Metrics collection  JVM Metrics  Pre-defined Job Execution Metrics    Job Execution History Store  Email Notifications", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Monitoring/#overview", 
            "text": "As a framework for ingesting potentially huge volume of data from many different sources, it's critical to monitor the health and status of the system and job executions. Gobblin employs a variety of approaches introduced below for this purpose. All the approaches are optional and can be configured to be turned on and off in different combinations through the framework and job configurations.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/Monitoring/#metrics-collecting-and-reporting", 
            "text": "", 
            "title": "Metrics Collecting and Reporting"
        }, 
        {
            "location": "/user-guide/Monitoring/#metrics-reporting", 
            "text": "Out-of-the-box, Gobblin reports metrics though:   JMX  : used in the standalone deployment. Metrics reported to JMX can be checked using using tools such as  VisualVM  or JConsole.   Metric log files : Files are stored in a root directory defined by the property  metrics.log.dir . Each Gobblin job has its own subdirectory under the root directory and each run of the job has its own metric log file named after the job ID as  ${job_id}.metrics.log .  Hadoop counters  : used for M/R deployments. Gobblin-specific metrics are reported in the \"JOB\" or \"TASK\" groups for job- and task- level metrics. By default, task-level metrics are not reported through Hadoop counters as doing so may cause the number of Hadoop counters to go beyond the system-wide limit. However, users can choose to turn on reporting task-level metrics as Hadoop counters by setting  mr.include.task.counters=true .", 
            "title": "Metrics Reporting"
        }, 
        {
            "location": "/user-guide/Monitoring/#metrics-collection", 
            "text": "", 
            "title": "Metrics collection"
        }, 
        {
            "location": "/user-guide/Monitoring/#jvm-metrics", 
            "text": "The standalone deployment of Gobblin runs in a single JVM so it's important to monitor the health of the JVM, through a set of pre-defined JVM metrics in the following four categories.    jvm.gc : this covers metrics related to garbage collection, e.g., counts and time spent on garbage collection.  jvm.memory : this covers metrics related to memory usage, e.g., detailed heap usage.    jvm.threads : this covers metrics related to thread states, e.g., thread count and thread deadlocks.  jvm.fileDescriptorRatio : this measures the ratio of open file descriptors.   All JVM metrics are reported via JMX and can be checked using tools such as  VisualVM  or JConsole.", 
            "title": "JVM Metrics"
        }, 
        {
            "location": "/user-guide/Monitoring/#pre-defined-job-execution-metrics", 
            "text": "Internally, Gobblin pre-defines a minimum set of metrics listed below in two metric groups:  JOB  and  TASK  for job-level metrics and task-level metrics, respectively. Those metrics are useful in keeping track of the progress and performance of job executions.   ${metric_group}.${id}.records : this metric keeps track of the total number of data records extracted by the job or task depending on the  ${metric_group} . The  ${id}  is either a job ID or a task ID depending on the  ${metric_group} .   ${metric_group}.${id}.recordsPerSecond : this metric keeps track of the rate of data extraction as data records extracted per second by the job or task depending on the  ${metric_group} .  ${metric_group}.${id}.bytes : this metric keeps track of the total number of bytes extracted by the job or task depending on the  ${metric_group} .  ${metric_group}.${id}.bytesPerSecond : this metric keeps track of the rate of data extraction as bytes extracted per second by the job or task depending on the  ${metric_group} .   Among the above metrics,  ${metric_group}.${id}.records  and  ${metric_group}.${id}.bytes  are reported as Hadoop MapReduce counters for Gobblin jobs running on Hadoop.", 
            "title": "Pre-defined Job Execution Metrics"
        }, 
        {
            "location": "/user-guide/Monitoring/#job-execution-history-store", 
            "text": "Gobblin also supports writing job execution information to a job execution history store backed by a database of choice. Gobblin uses MySQL by default and it ships with the SQL  DDLs  of the relevant MySQL tables, although  it still allows users to choose which database to use as long as the schema of the tables is compatible. Users can use the properties  job.history.store.url  and  job.history.store.jdbc.driver  to specify the database URL and the JDBC driver to work with the database of choice. The user name and password used to access the database can be specified using the properties  job.history.store.user  and  job.history.store.password . An example configuration is shown below:  job.history.store.url=jdbc:mysql://localhost/gobblin\njob.history.store.jdbc.driver=com.mysql.jdbc.Driver\njob.history.store.user=gobblin\njob.history.store.password=gobblin", 
            "title": "Job Execution History Store"
        }, 
        {
            "location": "/user-guide/Monitoring/#email-notifications", 
            "text": "In addition to writing job execution information to the job execution history store, Gobblin also supports sending email notifications about job status. Job status notifications fall into two categories: alerts in case of job failures and normal notifications in case of successful job completions. Users can choose to enable or disable both categories using the properties  email.alert.enabled  and  email.notification.enabled .   The main content of an email alert or notification is a job status report in Json format. Below is an example job status report:  {\n     job name :  Gobblin_Demo_Job ,\n     job id :  job_Gobblin_Demo_Job_1417487480842 ,\n     job state :  COMMITTED ,\n     start time : 1417487480874,\n     end time : 1417490858913,\n     duration : 3378039,\n     tasks : 1,\n     completed tasks : 1,\n     task states : [\n        {\n             task id :  task_Gobblin_Demo_Job_1417487480842_0 ,\n             task state :  COMMITTED ,\n             start time : 1417490795903,\n             end time : 1417490858908,\n             duration : 63005,\n             high watermark : -1,\n             exception :  \n        }\n    ]\n}", 
            "title": "Email Notifications"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nQuartz\n\n\nAzkaban\n\n\nOozie\n\n\nLaunching Gobblin in Local Mode\n\n\nExample Config Files\n\n\nUploading Files to HDFS\n\n\nAdding Gobblin jar Dependencies\n\n\n\n\n\n\nLaunching the Job\n\n\nDebugging Tips\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nGobblin jobs can be scheduled on a recurring basis using a few different tools. Gobblin ships with a built in \nQuartz Scheduler\n. Gobblin also integrates with a few other third party tools.\n\n\nQuartz\n\n\nGobblin has a built in Quartz scheduler as part of the \nJobScheduler\n class. This class integrates with the Gobblin \nSchedulerDaemon\n, which can be run using the Gobblin \n`bin/gobblin-standalone.sh\n script.\n\n\nSo in order to take advantage of the Quartz scheduler two steps need to be taken:\n\n\n\n\nUse the \nbin/gobblin-standalone.sh\n script\n\n\nAdd the property \njob.schedule\n to the \n.pull\n file\n\n\nThe value for this property should be a \nCRONTrigger\n\n\n\n\n\n\n\n\nAzkaban\n\n\nGobblin can be launched via \nAzkaban\n, and open-source Workflow Manager for scheduling and launching Hadoop jobs. Gobblin's \nAzkabanJobLauncher\n can be used to launch a Gobblin job through Azkaban.\n\n\nOne has to follow the typical setup to create a zip file that can be uploaded to Azkaban (it should include all dependent jars, which can be found in \ngobblin-dist.tar.gz\n). The \n.job\n file for the Azkaban Job should contain all configuration properties that would be put in a \n.pull\n file (for example, the \nWikipedia Example\n \n.pull\n file). All Gobblin system dependent properties (e.g. \nconf/gobblin-mapreduce.properties\n or \nconf/gobblin-standalone.properties\n) should also be in the zip file.\n\n\nIn the Azkaban \n.job\n file, the \ntype\n parameter should be set to \nhadoopJava\n (see \nhere\n for more information about the \nhadoopJava\n Job Type). The \njob.class\n parameter should be set to \ngobblin.azkaban.AzkabanJobLauncher\n.\n\n\nOozie\n\n\nOozie\n is a very popular scheduler for the Hadoop environment. It allows users to define complex workflows using XML files. A workflow can be composed of a series of actions, such as Java Jobs, Pig Jobs, Spark Jobs, etc. Gobblin has two integration points with Oozie. It can be run as a stand-alone Java process via Oozie's \njava\n tag, or it can be run as an Map Reduce job via Oozie.\n\n\nThe following guides assume Oozie is already setup and running on some machine, if this is not the case consult the Oozie documentation for getting everything setup.\n\n\nThese tutorial only outline how to launch a basic Oozie job that simply runs a Gobblin java a single time. For information on how to build more complex flows, and how to run jobs on a schedule, check out the Oozie documentation online.\n\n\nLaunching Gobblin in Local Mode\n\n\nThis guide focuses on getting Gobblin to run in as a stand alone Java Process. This means it will not launch a separate MR job to distribute its workload. It is important to understand how the current version of Oozie will launch a Java process. It will first start an MapReduce job and will run the Gobblin as a Java process inside a single map task. The Gobblin job will then ingest all data it is configured to pull and then it will shutdown.\n\n\nExample Config Files\n\n\ngobblin-oozie/src/main/resources/\n contains sample configuration files for launching Gobblin Oozie. There are a number of important files in this directory:\n\n\ngobblin-oozie-example-system.properties\n contains default system level properties for Gobblin. When launched with Oozie, Gobblin will run inside a map task; it is thus recommended to configure Gobblin to write directly to HDFS rather than the local file system. The property \nfs.uri\n in this file should be changed to point to the NameNode of the Hadoop File System the job should write to. By default, all data is written under a folder called \ngobblin-out\n; to change this modify the \ngobblin.work.dir\n parameter in this file.\n\n\ngobblin-oozie-example-workflow.properties\n contains default Oozie properties for any job launched. It is also the entry point for launching an Oozie job (e.g. to launch an Oozie job from the command line you execute \noozie job -config gobblin-oozie-example-workflow.properties -run\n). In this file one needs to update the \nname.node\n and \nresource.manager\n to the values specific to their environment. Another important property in this file is \noozie.wf.application.path\n; it points to a folder on HDFS that contains any workflows to be run. It is important to note, that the \nworkflow.xml\n files must be on HDFS in order for Oozie to pick them up (this is because Oozie typically runs on a separate machine as any client process).\n\n\ngobblin-oozie-example-workflow.xml\n contains an example Oozie workflow. This example simply launches a Java process that invokes the main method of the \nCliLocalJobLauncher\n. The main method of this class expects two file paths to be passed to it (once again these files need to be on HDFS). The \njobconfig\n arg should point to a file on HDFS containing all job configuration parameters. An example \njobconfig\n file can be found \nhere\n. The \nsysconfig\n arg should point to a file on HDFS containing all system configuration parameters. An example \nsysconfig\n file for Oozie can be found \nhere\n.\n\n\n\n\n\nUploading Files to HDFS\n\n\nOozie only reads a job properties file from the local file system (e.g. \ngobblin-oozie-example-workflow.properties\n), it expects all other configuration and dependent files to be uploaded to HDFS. Specifically, it looks for these files under the directory specified by \noozie.wf.application.path\n Make sure this is the case before trying to launch an Oozie job.\n\n\nAdding Gobblin \njar\n Dependencies\n\n\nGobblin has a number of \njar\n dependencies that need to be used when launching a Gobblin job. These dependencies can be taken from the \ngobblin-dist.tar.gz\n file that is created after building Gobblin. The tarball should contain a \nlib\n folder will the necessary dependencies. This folder should be placed into a \nlib\n folder under the same same directory specified by \noozie.wf.application.path\n in the \ngobblin-oozie-example-workflow.properties\n file.\n\n\nLaunching the Job\n\n\nAssuming one has the \nOozie CLI\n installed, the job can be launched using the following command: \noozie job -config gobblin-oozie-example-workflow.properties -run\n.\n\n\nDebugging Tips\n\n\nOnce the job has been launched, its status can be queried via the following command: \noozie job -info \noozie-job-id\n and the logs can be shown via the following command \noozie job -log \noozie-job-id\n.\n\n\nIn order to get see the standard output of Gobblin, one needs to check the logs the Map task running the Gobblin process. \noozie job -info \noozie-job-id\n should show the Hadoop \njob_id\n of the Hadoop Job launched to run the Gobblin process. Using this id one should be able to find the logs of the Map tasks through the UI or other command line tools (e.g. \nyarn logs\n).", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#table-of-contents", 
            "text": "Table of Contents  Introduction  Quartz  Azkaban  Oozie  Launching Gobblin in Local Mode  Example Config Files  Uploading Files to HDFS  Adding Gobblin jar Dependencies    Launching the Job  Debugging Tips", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#introduction", 
            "text": "Gobblin jobs can be scheduled on a recurring basis using a few different tools. Gobblin ships with a built in  Quartz Scheduler . Gobblin also integrates with a few other third party tools.", 
            "title": "Introduction"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#quartz", 
            "text": "Gobblin has a built in Quartz scheduler as part of the  JobScheduler  class. This class integrates with the Gobblin  SchedulerDaemon , which can be run using the Gobblin  `bin/gobblin-standalone.sh  script.  So in order to take advantage of the Quartz scheduler two steps need to be taken:   Use the  bin/gobblin-standalone.sh  script  Add the property  job.schedule  to the  .pull  file  The value for this property should be a  CRONTrigger", 
            "title": "Quartz"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#azkaban", 
            "text": "Gobblin can be launched via  Azkaban , and open-source Workflow Manager for scheduling and launching Hadoop jobs. Gobblin's  AzkabanJobLauncher  can be used to launch a Gobblin job through Azkaban.  One has to follow the typical setup to create a zip file that can be uploaded to Azkaban (it should include all dependent jars, which can be found in  gobblin-dist.tar.gz ). The  .job  file for the Azkaban Job should contain all configuration properties that would be put in a  .pull  file (for example, the  Wikipedia Example   .pull  file). All Gobblin system dependent properties (e.g.  conf/gobblin-mapreduce.properties  or  conf/gobblin-standalone.properties ) should also be in the zip file.  In the Azkaban  .job  file, the  type  parameter should be set to  hadoopJava  (see  here  for more information about the  hadoopJava  Job Type). The  job.class  parameter should be set to  gobblin.azkaban.AzkabanJobLauncher .", 
            "title": "Azkaban"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#oozie", 
            "text": "Oozie  is a very popular scheduler for the Hadoop environment. It allows users to define complex workflows using XML files. A workflow can be composed of a series of actions, such as Java Jobs, Pig Jobs, Spark Jobs, etc. Gobblin has two integration points with Oozie. It can be run as a stand-alone Java process via Oozie's  java  tag, or it can be run as an Map Reduce job via Oozie.  The following guides assume Oozie is already setup and running on some machine, if this is not the case consult the Oozie documentation for getting everything setup.  These tutorial only outline how to launch a basic Oozie job that simply runs a Gobblin java a single time. For information on how to build more complex flows, and how to run jobs on a schedule, check out the Oozie documentation online.", 
            "title": "Oozie"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#launching-gobblin-in-local-mode", 
            "text": "This guide focuses on getting Gobblin to run in as a stand alone Java Process. This means it will not launch a separate MR job to distribute its workload. It is important to understand how the current version of Oozie will launch a Java process. It will first start an MapReduce job and will run the Gobblin as a Java process inside a single map task. The Gobblin job will then ingest all data it is configured to pull and then it will shutdown.", 
            "title": "Launching Gobblin in Local Mode"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#example-config-files", 
            "text": "gobblin-oozie/src/main/resources/  contains sample configuration files for launching Gobblin Oozie. There are a number of important files in this directory:  gobblin-oozie-example-system.properties  contains default system level properties for Gobblin. When launched with Oozie, Gobblin will run inside a map task; it is thus recommended to configure Gobblin to write directly to HDFS rather than the local file system. The property  fs.uri  in this file should be changed to point to the NameNode of the Hadoop File System the job should write to. By default, all data is written under a folder called  gobblin-out ; to change this modify the  gobblin.work.dir  parameter in this file.  gobblin-oozie-example-workflow.properties  contains default Oozie properties for any job launched. It is also the entry point for launching an Oozie job (e.g. to launch an Oozie job from the command line you execute  oozie job -config gobblin-oozie-example-workflow.properties -run ). In this file one needs to update the  name.node  and  resource.manager  to the values specific to their environment. Another important property in this file is  oozie.wf.application.path ; it points to a folder on HDFS that contains any workflows to be run. It is important to note, that the  workflow.xml  files must be on HDFS in order for Oozie to pick them up (this is because Oozie typically runs on a separate machine as any client process).  gobblin-oozie-example-workflow.xml  contains an example Oozie workflow. This example simply launches a Java process that invokes the main method of the  CliLocalJobLauncher . The main method of this class expects two file paths to be passed to it (once again these files need to be on HDFS). The  jobconfig  arg should point to a file on HDFS containing all job configuration parameters. An example  jobconfig  file can be found  here . The  sysconfig  arg should point to a file on HDFS containing all system configuration parameters. An example  sysconfig  file for Oozie can be found  here .", 
            "title": "Example Config Files"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#uploading-files-to-hdfs", 
            "text": "Oozie only reads a job properties file from the local file system (e.g.  gobblin-oozie-example-workflow.properties ), it expects all other configuration and dependent files to be uploaded to HDFS. Specifically, it looks for these files under the directory specified by  oozie.wf.application.path  Make sure this is the case before trying to launch an Oozie job.", 
            "title": "Uploading Files to HDFS"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#adding-gobblin-jar-dependencies", 
            "text": "Gobblin has a number of  jar  dependencies that need to be used when launching a Gobblin job. These dependencies can be taken from the  gobblin-dist.tar.gz  file that is created after building Gobblin. The tarball should contain a  lib  folder will the necessary dependencies. This folder should be placed into a  lib  folder under the same same directory specified by  oozie.wf.application.path  in the  gobblin-oozie-example-workflow.properties  file.", 
            "title": "Adding Gobblin jar Dependencies"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#launching-the-job", 
            "text": "Assuming one has the  Oozie CLI  installed, the job can be launched using the following command:  oozie job -config gobblin-oozie-example-workflow.properties -run .", 
            "title": "Launching the Job"
        }, 
        {
            "location": "/user-guide/Gobblin-Schedulers/#debugging-tips", 
            "text": "Once the job has been launched, its status can be queried via the following command:  oozie job -info  oozie-job-id  and the logs can be shown via the following command  oozie job -log  oozie-job-id .  In order to get see the standard output of Gobblin, one needs to check the logs the Map task running the Gobblin process.  oozie job -info  oozie-job-id  should show the Hadoop  job_id  of the Hadoop Job launched to run the Gobblin process. Using this id one should be able to find the logs of the Map tasks through the UI or other command line tools (e.g.  yarn logs ).", 
            "title": "Debugging Tips"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nOverview\n\n\nInformation Recorded\n\n\nJob Execution Information\n\n\nTask Execution Information\n\n\nDefault Implementation\n\n\nRest Query API\n\n\nExample Queries\n\n\n\n\n\n\nJob Execution History Server\n\n\n\n\n\n\nOverview\n\n\nGobblin provides the users a way of keeping tracking of executions of their jobs through the Job Execution History Store, which can be queried either directly if the implementation supports queries directly or through a Rest API. Note that using the Rest API needs the Job Execution History Server to be up and running. The Job Execution History Server will be discussed later. By default, writing to the Job Execution History Store is not enabled. To enable it, set configuration property \njob.history.store.enabled\n to \ntrue\n.\n\n\nInformation Recorded\n\n\nThe Job Execution History Store stores various pieces of information of a job execution, including both job-level and task-level stats and measurements that are summarized below.\n\n\nJob Execution Information\n\n\nThe following table summarizes job-level execution information the Job Execution History Store stores. \n\n\n\n\n\n\n\n\nInformation\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nJob name\n\n\nGobblin job name.\n\n\n\n\n\n\nJob ID\n\n\nGobblin job ID.\n\n\n\n\n\n\nStart time\n\n\nStart time in epoch time (of unit milliseconds) of the job in the local time zone.\n\n\n\n\n\n\nEnd time\n\n\nEnd time in epoch time (of unit milliseconds) of the job in the local time zone.\n\n\n\n\n\n\nDuration\n\n\nDuration of the job in milliseconds.\n\n\n\n\n\n\nJob state\n\n\nRunning state of the job. Possible values are \nPENDING\n, \nRUNNING\n, \nSUCCESSFUL\n, \nCOMMITTED\n, \nFAILED\n, \nCANCELLED\n.\n\n\n\n\n\n\nLaunched tasks\n\n\nNumber of launched tasks of the job.\n\n\n\n\n\n\nCompleted tasks\n\n\nNumber of tasks of the job that completed.\n\n\n\n\n\n\nLauncher type\n\n\nThe type of the launcher used to launch and run the task.\n\n\n\n\n\n\nJob tracking URL\n\n\nThis will be set to the MapReduce job URL if the Gobblin job is running on Hadoop MapReduce. This may also be set to the Azkaban job execution tracking URL if the job is running through Azkaban but not on Hadoop MapReduce. Otherwise, this will be empty.\n\n\n\n\n\n\nJob-level metrics\n\n\nValues of job-level metrics. Note that this data is not time-series based so the values will be overwritten on every update.\n\n\n\n\n\n\nJob configuration properties\n\n\nJob configuration properties used at runtime for job execution. Note that it may include changes made at runtime by the job.\n\n\n\n\n\n\n\n\nTask Execution Information\n\n\nThe following table summarizes task-level execution information the Job Execution History Store stores. \n\n\n\n\n\n\n\n\nInformation\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nTask ID\n\n\nGobblin task ID.\n\n\n\n\n\n\nJob ID\n\n\nGobblin job ID.\n\n\n\n\n\n\nStart time\n\n\nStart time in epoch time (of unit milliseconds) of the task in the local time zone.\n\n\n\n\n\n\nEnd time\n\n\nEnd time in epoch time (of unit milliseconds) of the task in the local time zone.\n\n\n\n\n\n\nDuration\n\n\nDuration of the task in milliseconds.\n\n\n\n\n\n\nTask state\n\n\nRunning state of the task. Possible values are \nPENDING\n, \nRUNNING\n, \nSUCCESSFUL\n, \nCOMMITTED\n, \nFAILED\n, \nCANCELLED\n.\n\n\n\n\n\n\nTask failure exception\n\n\nException message in case of task failure.\n\n\n\n\n\n\nLow watermark\n\n\nThe low watermark of the task if avaialble.\n\n\n\n\n\n\nHigh watermark\n\n\nThe high watermark of the task if available.\n\n\n\n\n\n\nExtract namespace\n\n\nThe namespace of the \nExtract\n. An \nExtract\n is a concept describing the ingestion work of a job. This stores the value specified through the configuration property \nextract.namespace\n.\n\n\n\n\n\n\nExtract name\n\n\nThe name of the \nExtract\n. This stores the value specified through the configuration property \nextract.table.name\n.\n\n\n\n\n\n\nExtract type\n\n\nThe type of the \nExtract\n. This stores the value specified through the configuration property \nextract.table.type\n.\n\n\n\n\n\n\nTask-level metrics\n\n\nValues of task-level metrics. Note that this data is not time-series based so the values will be overwritten on every update.\n\n\n\n\n\n\nTask configuration properties\n\n\nTask configuration properties used at runtime for task execution. Note that it may include changes made at runtime by the task.\n\n\n\n\n\n\n\n\nDefault Implementation\n\n\nThe default implementation of the Job Execution History Store stores job execution information into a MySQL database in a few different tables. Specifically, the following tables are used and should be created before writing to the store is enabled. Checkout the MySQL \nDDLs\n of the tables for detailed columns of each table.\n\n\n\n\nTable \ngobblin_job_executions\n stores basic information about a job execution including the start and end times, job running state, number of launched and completed tasks, etc. \n\n\nTable \ngobblin_task_executions\n stores basic information on task executions of a job, including the start and end times, task running state, task failure message if any, etc, of each task. \n\n\nTable \ngobblin_job_metrics\n stores values of job-level metrics collected through the \nJobMetrics\n class. Note that this data is not time-series based and values of metrics are overwritten on every update to the job execution information. \n\n\nTable \ngobblin_task_metrics\n stores values of task-level metrics collected through the \nTaskMetrics\n class. Again, this data is not time-series based and values of metrics are overwritten on updates.\n\n\nTable \ngobblin_job_properties\n stores the job configuration properties used at runtime for the job execution, which may include changes made at runtime by the job.\n\n\nTable \ngobblin_task_properties\n stores the task configuration properties used at runtime for task executions, which also may include changes made at runtime by the tasks.\n\n\n\n\nTo enable writing to the MySQL-backed Job Execution History Store, the following configuration properties (with sample values) need to be set:\n\n\njob.history.store.url=jdbc:mysql://localhost/gobblin\njob.history.store.jdbc.driver=com.mysql.jdbc.Driver\njob.history.store.user=gobblin\njob.history.store.password=gobblin\n\n\n\n\nRest Query API\n\n\nThe Job Execution History Store Rest API supports three types of queries: query by job name, query by job ID, or query by extract name. The query type can be specified using the field \nidType\n in the query json object and can have one of the values \nJOB_NAME\n, \nJOB_ID\n, or \nTABLE\n. All three query types require the field \nid\n in the query json object, which should have a proper value as documented in the following table. \n\n\n\n\n\n\n\n\nQuery type\n\n\nQuery ID\n\n\n\n\n\n\n\n\n\n\nJOB_NAME\n\n\nGobblin job name.\n\n\n\n\n\n\nJOB_ID\n\n\nGobblin job ID.\n\n\n\n\n\n\nTABLE\n\n\nA json object following the \nTABLE\n schema shown below.\n\n\n\n\n\n\n\n\n{\n    \ntype\n: \nrecord\n,\n    \nname\n: \nTable\n,\n    \nnamespace\n: \ngobblin.rest\n,\n    \ndoc\n: \nGobblin table definition\n,\n    \nfields\n: [\n      {\n          \nname\n: \nnamespace\n,\n          \ntype\n: \nstring\n,\n          \noptional\n: true,\n          \ndoc\n: \nTable namespace\n\n      },\n      {\n          \nname\n: \nname\n,\n          \ntype\n: \nstring\n,\n          \ndoc\n: \nTable name\n\n      },\n      {\n          \nname\n: \ntype\n,\n          \ntype\n: {\n              \nname\n: \nTableTypeEnum\n,\n              \ntype\n: \nenum\n,\n              \nsymbols\n : [ \nSNAPSHOT_ONLY\n, \nSNAPSHOT_APPEND\n, \nAPPEND_ONLY\n ]\n          },\n          \noptional\n: true,\n          \ndoc\n: \nTable type\n\n      }\n    ]\n}\n\n\n\n\nFor each query type, there are also some option fields that can be used to control the number of records returned and what should be included in the query result. The optional fields are summarized in the following table.\n\n\n\n\n\n\n\n\nOptional field\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nlimit\n\n\nint\n\n\nLimit on the number of records returned.\n\n\n\n\n\n\ntimeRange\n\n\nTimeRange\n\n\nThe query time range. The schema of \nTimeRange\n is shown below.\n\n\n\n\n\n\njobProperties\n\n\nboolean\n\n\nThis controls whether the returned record should include the job configuration properties.\n\n\n\n\n\n\ntaskProperties\n\n\nboolean\n\n\nThis controls whether the returned record should include the task configuration properties.\n\n\n\n\n\n\n\n\n{\n    \ntype\n: \nrecord\n,\n    \nname\n: \nTimeRange\n,\n    \nnamespace\n: \ngobblin.rest\n,\n    \ndoc\n: \nQuery time range\n,\n    \nfields\n: [\n      {\n          \nname\n: \nstartTime\n,\n          \ntype\n: \nstring\n,\n          \noptional\n: true,\n          \ndoc\n: \nStart time of the query range\n\n      },\n      {\n          \nname\n: \nendTime\n,\n          \ntype\n: \nstring\n,\n          \noptional\n: true,\n          \ndoc\n: \nEnd time of the query range\n\n      },\n      {\n          \nname\n: \ntimeFormat\n,\n          \ntype\n: \nstring\n,\n          \ndoc\n: \nDate/time format used to parse the start time and end time\n\n      }\n    ]\n}\n\n\n\n\nThe API is built with \nrest.li\n, which generates documentation on compilation and can be found at \nhttp://\nhostname:port\n/restli/docs\n.\n\n\nExample Queries\n\n\nFetch the 10 most recent job executions with a job name \nTestJobName\n\n\ncurl \nhttp://\nhostname:port\n/jobExecutions/idType=JOB_NAME\nid.string=TestJobName\nlimit=10\n\n\n\n\n\nJob Execution History Server\n\n\nThe Job Execution History Server is a Rest server for serving queries on the Job Execution History Store through the Rest API described above. The Rest endpoint URL is configurable through the following configuration properties (with their default values):\n\n\nrest.server.host=localhost\nrest.server.port=8080\n\n\n\n\nNote:\n This server is started in the standalone deployment if configuration property \njob.execinfo.server.enabled\n is set to \ntrue\n.", 
            "title": "Job Execution History Store"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#table-of-contents", 
            "text": "Table of Contents  Overview  Information Recorded  Job Execution Information  Task Execution Information  Default Implementation  Rest Query API  Example Queries    Job Execution History Server", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#overview", 
            "text": "Gobblin provides the users a way of keeping tracking of executions of their jobs through the Job Execution History Store, which can be queried either directly if the implementation supports queries directly or through a Rest API. Note that using the Rest API needs the Job Execution History Server to be up and running. The Job Execution History Server will be discussed later. By default, writing to the Job Execution History Store is not enabled. To enable it, set configuration property  job.history.store.enabled  to  true .", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#information-recorded", 
            "text": "The Job Execution History Store stores various pieces of information of a job execution, including both job-level and task-level stats and measurements that are summarized below.", 
            "title": "Information Recorded"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#job-execution-information", 
            "text": "The following table summarizes job-level execution information the Job Execution History Store stores.      Information  Description      Job name  Gobblin job name.    Job ID  Gobblin job ID.    Start time  Start time in epoch time (of unit milliseconds) of the job in the local time zone.    End time  End time in epoch time (of unit milliseconds) of the job in the local time zone.    Duration  Duration of the job in milliseconds.    Job state  Running state of the job. Possible values are  PENDING ,  RUNNING ,  SUCCESSFUL ,  COMMITTED ,  FAILED ,  CANCELLED .    Launched tasks  Number of launched tasks of the job.    Completed tasks  Number of tasks of the job that completed.    Launcher type  The type of the launcher used to launch and run the task.    Job tracking URL  This will be set to the MapReduce job URL if the Gobblin job is running on Hadoop MapReduce. This may also be set to the Azkaban job execution tracking URL if the job is running through Azkaban but not on Hadoop MapReduce. Otherwise, this will be empty.    Job-level metrics  Values of job-level metrics. Note that this data is not time-series based so the values will be overwritten on every update.    Job configuration properties  Job configuration properties used at runtime for job execution. Note that it may include changes made at runtime by the job.", 
            "title": "Job Execution Information"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#task-execution-information", 
            "text": "The following table summarizes task-level execution information the Job Execution History Store stores.      Information  Description      Task ID  Gobblin task ID.    Job ID  Gobblin job ID.    Start time  Start time in epoch time (of unit milliseconds) of the task in the local time zone.    End time  End time in epoch time (of unit milliseconds) of the task in the local time zone.    Duration  Duration of the task in milliseconds.    Task state  Running state of the task. Possible values are  PENDING ,  RUNNING ,  SUCCESSFUL ,  COMMITTED ,  FAILED ,  CANCELLED .    Task failure exception  Exception message in case of task failure.    Low watermark  The low watermark of the task if avaialble.    High watermark  The high watermark of the task if available.    Extract namespace  The namespace of the  Extract . An  Extract  is a concept describing the ingestion work of a job. This stores the value specified through the configuration property  extract.namespace .    Extract name  The name of the  Extract . This stores the value specified through the configuration property  extract.table.name .    Extract type  The type of the  Extract . This stores the value specified through the configuration property  extract.table.type .    Task-level metrics  Values of task-level metrics. Note that this data is not time-series based so the values will be overwritten on every update.    Task configuration properties  Task configuration properties used at runtime for task execution. Note that it may include changes made at runtime by the task.", 
            "title": "Task Execution Information"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#default-implementation", 
            "text": "The default implementation of the Job Execution History Store stores job execution information into a MySQL database in a few different tables. Specifically, the following tables are used and should be created before writing to the store is enabled. Checkout the MySQL  DDLs  of the tables for detailed columns of each table.   Table  gobblin_job_executions  stores basic information about a job execution including the start and end times, job running state, number of launched and completed tasks, etc.   Table  gobblin_task_executions  stores basic information on task executions of a job, including the start and end times, task running state, task failure message if any, etc, of each task.   Table  gobblin_job_metrics  stores values of job-level metrics collected through the  JobMetrics  class. Note that this data is not time-series based and values of metrics are overwritten on every update to the job execution information.   Table  gobblin_task_metrics  stores values of task-level metrics collected through the  TaskMetrics  class. Again, this data is not time-series based and values of metrics are overwritten on updates.  Table  gobblin_job_properties  stores the job configuration properties used at runtime for the job execution, which may include changes made at runtime by the job.  Table  gobblin_task_properties  stores the task configuration properties used at runtime for task executions, which also may include changes made at runtime by the tasks.   To enable writing to the MySQL-backed Job Execution History Store, the following configuration properties (with sample values) need to be set:  job.history.store.url=jdbc:mysql://localhost/gobblin\njob.history.store.jdbc.driver=com.mysql.jdbc.Driver\njob.history.store.user=gobblin\njob.history.store.password=gobblin", 
            "title": "Default Implementation"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#rest-query-api", 
            "text": "The Job Execution History Store Rest API supports three types of queries: query by job name, query by job ID, or query by extract name. The query type can be specified using the field  idType  in the query json object and can have one of the values  JOB_NAME ,  JOB_ID , or  TABLE . All three query types require the field  id  in the query json object, which should have a proper value as documented in the following table.      Query type  Query ID      JOB_NAME  Gobblin job name.    JOB_ID  Gobblin job ID.    TABLE  A json object following the  TABLE  schema shown below.     {\n     type :  record ,\n     name :  Table ,\n     namespace :  gobblin.rest ,\n     doc :  Gobblin table definition ,\n     fields : [\n      {\n           name :  namespace ,\n           type :  string ,\n           optional : true,\n           doc :  Table namespace \n      },\n      {\n           name :  name ,\n           type :  string ,\n           doc :  Table name \n      },\n      {\n           name :  type ,\n           type : {\n               name :  TableTypeEnum ,\n               type :  enum ,\n               symbols  : [  SNAPSHOT_ONLY ,  SNAPSHOT_APPEND ,  APPEND_ONLY  ]\n          },\n           optional : true,\n           doc :  Table type \n      }\n    ]\n}  For each query type, there are also some option fields that can be used to control the number of records returned and what should be included in the query result. The optional fields are summarized in the following table.     Optional field  Type  Description      limit  int  Limit on the number of records returned.    timeRange  TimeRange  The query time range. The schema of  TimeRange  is shown below.    jobProperties  boolean  This controls whether the returned record should include the job configuration properties.    taskProperties  boolean  This controls whether the returned record should include the task configuration properties.     {\n     type :  record ,\n     name :  TimeRange ,\n     namespace :  gobblin.rest ,\n     doc :  Query time range ,\n     fields : [\n      {\n           name :  startTime ,\n           type :  string ,\n           optional : true,\n           doc :  Start time of the query range \n      },\n      {\n           name :  endTime ,\n           type :  string ,\n           optional : true,\n           doc :  End time of the query range \n      },\n      {\n           name :  timeFormat ,\n           type :  string ,\n           doc :  Date/time format used to parse the start time and end time \n      }\n    ]\n}  The API is built with  rest.li , which generates documentation on compilation and can be found at  http:// hostname:port /restli/docs .", 
            "title": "Rest Query API"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#example-queries", 
            "text": "Fetch the 10 most recent job executions with a job name  TestJobName  curl  http:// hostname:port /jobExecutions/idType=JOB_NAME id.string=TestJobName limit=10", 
            "title": "Example Queries"
        }, 
        {
            "location": "/user-guide/Job-Execution-History-Store/#job-execution-history-server", 
            "text": "The Job Execution History Server is a Rest server for serving queries on the Job Execution History Store through the Rest API described above. The Rest endpoint URL is configurable through the following configuration properties (with their default values):  rest.server.host=localhost\nrest.server.port=8080  Note:  This server is started in the standalone deployment if configuration property  job.execinfo.server.enabled  is set to  true .", 
            "title": "Job Execution History Server"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nOptions\n\n\nVersions\n\n\nHadoop Version\n\n\nHive Version\n\n\nPegasus Version\n\n\nByteman Version\n\n\n\n\n\n\nExclude Hadoop Dependencies from gobblin-dist.tar.gz\n\n\nExclude Hive Dependencies from gobblin-dist.tar.gz\n\n\n\n\n\n\nCustom Gradle Tasks\n\n\nPrint Project Dependencies\n\n\n\n\n\n\nUseful Gradle Commands\n\n\nSkipping Tests\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nThis page outlines all the options that can be specified when building Gobblin using Gradle. The typical way of building Gobblin is to run:\n\n\n./gradlew build\n\n\n\n\nHowever, there are a number of parameters that can be passed into the above command to customize the build process.\n\n\nOptions\n\n\nThese options just need to be added to the command above to take effect.\n\n\nVersions\n\n\nHadoop Version\n\n\nThe Hadoop version can be specified by adding the option \n-PhadoopVersion=[my-hadoop-version]\n. If using a Hadoop version over \n2.0.0\n the option \n-PuseHadoop2\n must also be added.\n\n\nHive Version\n\n\nThe Hive version can be specified by adding the option \n-PhiveVersion=[my-hive-version]\n.\n\n\nPegasus Version\n\n\nThe Pegasus version can be specified by adding the option \n-PpegasusVersion=[my-pegasus-version]\n.\n\n\nByteman Version\n\n\nThe Byteman version can be specified by adding the option \n-PbytemanVersion=[my-byteman-version]\n.\n\n\nExclude Hadoop Dependencies from \ngobblin-dist.tar.gz\n\n\nAdd the option \n-PexcludeHadoopDeps\n to exclude all Hadoop libraries from \ngobblin-dist.tar.gz\n.\n\n\nExclude Hive Dependencies from \ngobblin-dist.tar.gz\n\n\nAdd the option \n-PexcludeHiveDeps\n to exclude all Hadoop libraries from \ngobblin-dist.tar.gz\n.\n\n\nCustom Gradle Tasks\n\n\nA few custom built Gradle tasks.\n\n\nPrint Project Dependencies\n\n\nExecuting this command will print out all the dependencies between the different Gobblin Gradle sub-projects: \n./gradlew dotProjectDependencies\n.\n\n\nUseful Gradle Commands\n\n\nThese commands make working with Gradle a little easier.\n\n\nSkipping Tests\n\n\nAdd \n-x test\n to the end of the build command.", 
            "title": "Gobblin Build Options"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#table-of-contents", 
            "text": "Table of Contents  Introduction  Options  Versions  Hadoop Version  Hive Version  Pegasus Version  Byteman Version    Exclude Hadoop Dependencies from gobblin-dist.tar.gz  Exclude Hive Dependencies from gobblin-dist.tar.gz    Custom Gradle Tasks  Print Project Dependencies    Useful Gradle Commands  Skipping Tests", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#introduction", 
            "text": "This page outlines all the options that can be specified when building Gobblin using Gradle. The typical way of building Gobblin is to run:  ./gradlew build  However, there are a number of parameters that can be passed into the above command to customize the build process.", 
            "title": "Introduction"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#options", 
            "text": "These options just need to be added to the command above to take effect.", 
            "title": "Options"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#versions", 
            "text": "", 
            "title": "Versions"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#hadoop-version", 
            "text": "The Hadoop version can be specified by adding the option  -PhadoopVersion=[my-hadoop-version] . If using a Hadoop version over  2.0.0  the option  -PuseHadoop2  must also be added.", 
            "title": "Hadoop Version"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#hive-version", 
            "text": "The Hive version can be specified by adding the option  -PhiveVersion=[my-hive-version] .", 
            "title": "Hive Version"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#pegasus-version", 
            "text": "The Pegasus version can be specified by adding the option  -PpegasusVersion=[my-pegasus-version] .", 
            "title": "Pegasus Version"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#byteman-version", 
            "text": "The Byteman version can be specified by adding the option  -PbytemanVersion=[my-byteman-version] .", 
            "title": "Byteman Version"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#exclude-hadoop-dependencies-from-gobblin-disttargz", 
            "text": "Add the option  -PexcludeHadoopDeps  to exclude all Hadoop libraries from  gobblin-dist.tar.gz .", 
            "title": "Exclude Hadoop Dependencies from gobblin-dist.tar.gz"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#exclude-hive-dependencies-from-gobblin-disttargz", 
            "text": "Add the option  -PexcludeHiveDeps  to exclude all Hadoop libraries from  gobblin-dist.tar.gz .", 
            "title": "Exclude Hive Dependencies from gobblin-dist.tar.gz"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#custom-gradle-tasks", 
            "text": "A few custom built Gradle tasks.", 
            "title": "Custom Gradle Tasks"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#print-project-dependencies", 
            "text": "Executing this command will print out all the dependencies between the different Gobblin Gradle sub-projects:  ./gradlew dotProjectDependencies .", 
            "title": "Print Project Dependencies"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#useful-gradle-commands", 
            "text": "These commands make working with Gradle a little easier.", 
            "title": "Useful Gradle Commands"
        }, 
        {
            "location": "/user-guide/Gobblin-Build-Options/#skipping-tests", 
            "text": "Add  -x test  to the end of the build command.", 
            "title": "Skipping Tests"
        }, 
        {
            "location": "/user-guide/Troubleshooting/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nChecking Job State\n\n\n\n\n\n\nChecking Job State\n\n\nWhen there's an issue with a Gobblin job to troubleshoot, it is often helpful to check the state of the job persisted in the state store. Gobblin provides a tool `gobblin-dist/bin/statestore-checker.sh' for checking job states. The tool print job state(s) as a Json document that are easily readable. The usage of the tool is as follows:\n\n\nusage: statestore-checker.sh\n -a,--all                                  Whether to convert all past job\n                                           states of the given job\n -i,--id \ngobblin job id\n                  Gobblin job id\n -kc,--keepConfig                          Whether to keep all\n                                           configuration properties\n -n,--name \ngobblin job name\n              Gobblin job name\n -u,--storeurl \ngobblin state store URL\n   Gobblin state store root path\n                                           URL\n\n\n\n\nFor example, assume that the state store is located at \nfile://gobblin/state-store/\n, to check the job state of the most recent run of a job named \"Foo\", run the following command:\n\n\nstatestore-checker.sh -u file://gobblin/state-store/ -n Foo\n\n\n\n\nTo check the job state of a particular run (say, with job ID job_Foo_123456) of job \"Foo\", run the following command:\n\n\nstatestore-checker.sh -u file://gobblin/state-store/ -n Foo -i job_Foo_123456\n\n\n\n\nTo check the job states of all past runs of job \"Foo\", run the following command:\n\n\nstatestore-checker.sh -u file://gobblin/state-store/ -n Foo -a\n\n\n\n\nTo include job configuration in the output Json document, add option \n-kc\n or \n--keepConfig\n in the command.\n\n\nA sample output Json document is as follows:\n\n\n{\n    \njob name\n: \nGobblinMRTest\n,\n    \njob id\n: \njob_GobblinMRTest_1425622600239\n,\n    \njob state\n: \nCOMMITTED\n,\n    \nstart time\n: 1425622600240,\n    \nend time\n: 1425622601326,\n    \nduration\n: 1086,\n    \ntasks\n: 4,\n    \ncompleted tasks\n: 4,\n    \ntask states\n: [\n        {\n            \ntask id\n: \ntask_GobblinMRTest_1425622600239_3\n,\n            \ntask state\n: \nCOMMITTED\n,\n            \nstart time\n: 1425622600383,\n            \nend time\n: 1425622600395,\n            \nduration\n: 12,\n            \nhigh watermark\n: -1,\n            \nretry count\n: 0\n        },\n        {\n            \ntask id\n: \ntask_GobblinMRTest_1425622600239_2\n,\n            \ntask state\n: \nCOMMITTED\n,\n            \nstart time\n: 1425622600354,\n            \nend time\n: 1425622600374,\n            \nduration\n: 20,\n            \nhigh watermark\n: -1,\n            \nretry count\n: 0\n        },\n        {\n            \ntask id\n: \ntask_GobblinMRTest_1425622600239_1\n,\n            \ntask state\n: \nCOMMITTED\n,\n            \nstart time\n: 1425622600325,\n            \nend time\n: 1425622600344,\n            \nduration\n: 19,\n            \nhigh watermark\n: -1,\n            \nretry count\n: 0\n        },\n        {\n            \ntask id\n: \ntask_GobblinMRTest_1425622600239_0\n,\n            \ntask state\n: \nCOMMITTED\n,\n            \nstart time\n: 1425622600405,\n            \nend time\n: 1425622600421,\n            \nduration\n: 16,\n            \nhigh watermark\n: -1,\n            \nretry count\n: 0\n        }\n    ]\n}", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/Troubleshooting/#table-of-contents", 
            "text": "Table of Contents  Checking Job State", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/Troubleshooting/#checking-job-state", 
            "text": "When there's an issue with a Gobblin job to troubleshoot, it is often helpful to check the state of the job persisted in the state store. Gobblin provides a tool `gobblin-dist/bin/statestore-checker.sh' for checking job states. The tool print job state(s) as a Json document that are easily readable. The usage of the tool is as follows:  usage: statestore-checker.sh\n -a,--all                                  Whether to convert all past job\n                                           states of the given job\n -i,--id  gobblin job id                   Gobblin job id\n -kc,--keepConfig                          Whether to keep all\n                                           configuration properties\n -n,--name  gobblin job name               Gobblin job name\n -u,--storeurl  gobblin state store URL    Gobblin state store root path\n                                           URL  For example, assume that the state store is located at  file://gobblin/state-store/ , to check the job state of the most recent run of a job named \"Foo\", run the following command:  statestore-checker.sh -u file://gobblin/state-store/ -n Foo  To check the job state of a particular run (say, with job ID job_Foo_123456) of job \"Foo\", run the following command:  statestore-checker.sh -u file://gobblin/state-store/ -n Foo -i job_Foo_123456  To check the job states of all past runs of job \"Foo\", run the following command:  statestore-checker.sh -u file://gobblin/state-store/ -n Foo -a  To include job configuration in the output Json document, add option  -kc  or  --keepConfig  in the command.  A sample output Json document is as follows:  {\n     job name :  GobblinMRTest ,\n     job id :  job_GobblinMRTest_1425622600239 ,\n     job state :  COMMITTED ,\n     start time : 1425622600240,\n     end time : 1425622601326,\n     duration : 1086,\n     tasks : 4,\n     completed tasks : 4,\n     task states : [\n        {\n             task id :  task_GobblinMRTest_1425622600239_3 ,\n             task state :  COMMITTED ,\n             start time : 1425622600383,\n             end time : 1425622600395,\n             duration : 12,\n             high watermark : -1,\n             retry count : 0\n        },\n        {\n             task id :  task_GobblinMRTest_1425622600239_2 ,\n             task state :  COMMITTED ,\n             start time : 1425622600354,\n             end time : 1425622600374,\n             duration : 20,\n             high watermark : -1,\n             retry count : 0\n        },\n        {\n             task id :  task_GobblinMRTest_1425622600239_1 ,\n             task state :  COMMITTED ,\n             start time : 1425622600325,\n             end time : 1425622600344,\n             duration : 19,\n             high watermark : -1,\n             retry count : 0\n        },\n        {\n             task id :  task_GobblinMRTest_1425622600239_0 ,\n             task state :  COMMITTED ,\n             start time : 1425622600405,\n             end time : 1425622600421,\n             duration : 16,\n             high watermark : -1,\n             retry count : 0\n        }\n    ]\n}", 
            "title": "Checking Job State"
        }, 
        {
            "location": "/user-guide/FAQs/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nGobblin\n\n\nGeneral Questions \n\n\nWhat is Gobblin?\n\n\nWhat programming languages does Gobblin support?\n\n\nDoes Gobblin require any external software to be installed?\n\n\nWhat Hadoop version can Gobblin run on?\n\n\nHow do I run and schedule a Gobblin job?\n\n\nHow is Gobblin different from Sqoop?\n\n\n\n\n\n\nTechnical Questions \n\n\nWhen running on Hadoop, each map task quickly reaches 100 Percent completion, but then stalls for a long time. Why does this happen?\n\n\nWhy does Gobblin on Hadoop stall for a long time between adding files to the DistrbutedCache, and launching the actual job?\n\n\nHow do I fix UnsupportedFileSystemException: No AbstractFileSystem for scheme: null?\n\n\nHow do I compile Gobblin against CDH?\n\n\nResolve Gobblin-on-MR Exception IOException: Not all tasks running in mapper attempt_id completed successfully\n\n\nGradle Build Fails With Cannot invoke method getURLs on null object\n\n\n\n\n\n\n\n\n\n\nGradle\n\n\nTechnical Questions\n\n\nHow do I add a new external dependency?\n\n\nHow do I add a new Maven Repository to pull artifacts from?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGobblin\n\n\nGeneral Questions \n\n\nWhat is Gobblin?\n\n\nGobblin is a universal ingestion framework. It's goal is to pull data from any source into an arbitrary data store. One major use case for Gobblin is pulling data into Hadoop. Gobblin can pull data from file systems, SQL stores, and data that is exposed by a REST API. See the Gobblin \nHome\n page for more information.\n\n\nWhat programming languages does Gobblin support?\n\n\nGobblin currently only supports Java 6 and up.\n\n\nDoes Gobblin require any external software to be installed?\n\n\nThe machine that Gobblin is built on must have Java installed, and the \n$JAVA_HOME\n environment variable must be set.\n\n\nWhat Hadoop version can Gobblin run on?\n\n\nGobblin can run on both Hadoop 1.x and Hadoop 2.x. By default, Gobblin compiles against Hadoop 1.2.1, and can compiled against Hadoop 2.3.0 by running \n./gradlew -PuseHadoop2 clean build\n.\n\n\nHow do I run and schedule a Gobblin job?\n\n\nCheck out the \nDeployment\n page for information on how to run and schedule Gobblin jobs. Check out the \nConfiguration\n page for information on how to set proper configuration properties for a job.\n\n\nHow is Gobblin different from Sqoop?\n\n\nSqoop main focus bulk import and export of data from relational databases to HDFS, it lacks the ETL functionality of data cleansing, data transformation, and data quality checks that Gobblin provides. Gobblin is also capable of pulling from any data source (e.g. file systems, RDMS, REST APIs).\n\n\nTechnical Questions \n\n\nWhen running on Hadoop, each map task quickly reaches 100 Percent completion, but then stalls for a long time. Why does this happen?\n\n\nGobblin currently uses Hadoop map tasks as a container for running Gobblin tasks. Each map task runs 1 or more Gobblin workunits, and the progress of each workunit is not hooked into the progress of each map task. Even though the Hadoop job reports 100% completion, Gobblin is still doing work. See the \nGobblin Deployment\n page for more information.\n\n\nWhy does Gobblin on Hadoop stall for a long time between adding files to the DistrbutedCache, and launching the actual job?\n\n\nGobblin takes all WorkUnits created by the Source class and serializes each one into a file on Hadoop. These files are read by each map task, and are deserialized into Gobblin Tasks. These Tasks are then run by the map-task. The reason the job stalls is that Gobblin is writing all these files to HDFS, which can take a while especially if there are a lot of tasks to run. See the \nGobblin Deployment\n page for more information.\n\n\nHow do I fix \nUnsupportedFileSystemException: No AbstractFileSystem for scheme: null\n?\n\n\nThis error typically occurs due to Hadoop version conflict issues. If Gobblin is compiled against a specific Hadoop version, but then deployed on a different Hadoop version or installation, this error may be thrown. For example, if you simply compile Gobblin using \n./gradlew clean build -PuseHadoop2\n, but deploy Gobblin to a cluster with \nCDH\n installed, you may hit this error.\n\n\nIt is important to realize that the the \ngobblin-dist.tar.gz\n file produced by \n./gradlew clean build\n will include all the Hadoop jar dependencies; and if one follows the \nMR deployment guide\n, Gobblin will be launched with these dependencies on the classpath.\n\n\nTo fix this take the following steps:\n\n\n\n\nDelete all the Hadoop jars from the Gobblin \nlib\n folder\n\n\nEnsure that the environment variable \nHADOOP_CLASSPATH\n is set and points to a directory containing the Hadoop libraries for the cluster\n\n\n\n\nHow do I compile Gobblin against CDH?\n\n\nCloudera Distributed Hadoop\n (often abbreviated as CDH) is a popular Hadoop distribution. Typically, when running Gobblin on a CDH cluster it is recommended that one also compile Gobblin against the same CDH version. Not doing so may cause unexpected runtime behavior. To compile against a specific CDH version simply use the \nhadoopVersion\n parameter. For example, to compile against version \n2.5.0-cdh5.3.0\n run \n./gradlew clean build -PuseHadoop2 -PhadoopVersion=2.5.0-cdh5.3.0\n.\n\n\nResolve Gobblin-on-MR Exception \nIOException: Not all tasks running in mapper attempt_id completed successfully\n\n\nThis exception usually just means that a Hadoop Map Task running Gobblin Tasks threw some exception. Unfortunately, the exception isn't truly indicative of the underlying problem, all it is really saying is that something went wrong in the Gobblin Task. Each Hadoop Map Task has its own log file and it is often easiest to look at the logs of the Map Task when debugging this problem. There are multiple ways to do this, but one of the easiest ways is to execute \nyarn logs -applicationId \napplication ID\n [OPTIONS]\n\n\nGradle Build Fails With \nCannot invoke method getURLs on null object\n\n\nAdd \n-x test\n to build the project without running the tests; this will make the exception go away. If one needs to run the tests then make sure \nJava Cryptography Extension\n is installed.\n\n\nGradle\n\n\nTechnical Questions\n\n\nHow do I add a new external dependency?\n\n\nSay I want to add \noozie-core-4.2.0.jar\n as a dependency to the \ngobblin-scheduler\n subproject. I would first open the file \nbuild.gradle\n and add the following entry to the \next.externalDependency\n array: \n\"oozieCore\": \"org.apache.oozie:oozie-core:4.2.0\"\n.\n\n\nThen in the \ngobblin-scheduler/build.gradle\n file I would add the following line to the dependency block: \ncompile externalDependency.oozieCore\n.\n\n\nHow do I add a new Maven Repository to pull artifacts from?\n\n\nOften times, one may have important artifacts stored in a local or private Maven repository. As of 01/21/2016 Gobblin only pulls artifacts from the following Maven Repositories: \nMaven Central\n, \nConjars\n, and \nCloudera\n.\n\n\nIn order to add another Maven Repository modify the \ndefaultEnvironment.gradle\n file and the new repository using the same pattern as the existing ones.", 
            "title": "FAQs"
        }, 
        {
            "location": "/user-guide/FAQs/#table-of-contents", 
            "text": "Table of Contents  Gobblin  General Questions   What is Gobblin?  What programming languages does Gobblin support?  Does Gobblin require any external software to be installed?  What Hadoop version can Gobblin run on?  How do I run and schedule a Gobblin job?  How is Gobblin different from Sqoop?    Technical Questions   When running on Hadoop, each map task quickly reaches 100 Percent completion, but then stalls for a long time. Why does this happen?  Why does Gobblin on Hadoop stall for a long time between adding files to the DistrbutedCache, and launching the actual job?  How do I fix UnsupportedFileSystemException: No AbstractFileSystem for scheme: null?  How do I compile Gobblin against CDH?  Resolve Gobblin-on-MR Exception IOException: Not all tasks running in mapper attempt_id completed successfully  Gradle Build Fails With Cannot invoke method getURLs on null object      Gradle  Technical Questions  How do I add a new external dependency?  How do I add a new Maven Repository to pull artifacts from?", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/user-guide/FAQs/#gobblin", 
            "text": "", 
            "title": "Gobblin"
        }, 
        {
            "location": "/user-guide/FAQs/#general-questions", 
            "text": "", 
            "title": "General Questions "
        }, 
        {
            "location": "/user-guide/FAQs/#what-is-gobblin", 
            "text": "Gobblin is a universal ingestion framework. It's goal is to pull data from any source into an arbitrary data store. One major use case for Gobblin is pulling data into Hadoop. Gobblin can pull data from file systems, SQL stores, and data that is exposed by a REST API. See the Gobblin  Home  page for more information.", 
            "title": "What is Gobblin?"
        }, 
        {
            "location": "/user-guide/FAQs/#what-programming-languages-does-gobblin-support", 
            "text": "Gobblin currently only supports Java 6 and up.", 
            "title": "What programming languages does Gobblin support?"
        }, 
        {
            "location": "/user-guide/FAQs/#does-gobblin-require-any-external-software-to-be-installed", 
            "text": "The machine that Gobblin is built on must have Java installed, and the  $JAVA_HOME  environment variable must be set.", 
            "title": "Does Gobblin require any external software to be installed?"
        }, 
        {
            "location": "/user-guide/FAQs/#what-hadoop-version-can-gobblin-run-on", 
            "text": "Gobblin can run on both Hadoop 1.x and Hadoop 2.x. By default, Gobblin compiles against Hadoop 1.2.1, and can compiled against Hadoop 2.3.0 by running  ./gradlew -PuseHadoop2 clean build .", 
            "title": "What Hadoop version can Gobblin run on?"
        }, 
        {
            "location": "/user-guide/FAQs/#how-do-i-run-and-schedule-a-gobblin-job", 
            "text": "Check out the  Deployment  page for information on how to run and schedule Gobblin jobs. Check out the  Configuration  page for information on how to set proper configuration properties for a job.", 
            "title": "How do I run and schedule a Gobblin job?"
        }, 
        {
            "location": "/user-guide/FAQs/#how-is-gobblin-different-from-sqoop", 
            "text": "Sqoop main focus bulk import and export of data from relational databases to HDFS, it lacks the ETL functionality of data cleansing, data transformation, and data quality checks that Gobblin provides. Gobblin is also capable of pulling from any data source (e.g. file systems, RDMS, REST APIs).", 
            "title": "How is Gobblin different from Sqoop?"
        }, 
        {
            "location": "/user-guide/FAQs/#technical-questions", 
            "text": "", 
            "title": "Technical Questions "
        }, 
        {
            "location": "/user-guide/FAQs/#when-running-on-hadoop-each-map-task-quickly-reaches-100-percent-completion-but-then-stalls-for-a-long-time-why-does-this-happen", 
            "text": "Gobblin currently uses Hadoop map tasks as a container for running Gobblin tasks. Each map task runs 1 or more Gobblin workunits, and the progress of each workunit is not hooked into the progress of each map task. Even though the Hadoop job reports 100% completion, Gobblin is still doing work. See the  Gobblin Deployment  page for more information.", 
            "title": "When running on Hadoop, each map task quickly reaches 100 Percent completion, but then stalls for a long time. Why does this happen?"
        }, 
        {
            "location": "/user-guide/FAQs/#why-does-gobblin-on-hadoop-stall-for-a-long-time-between-adding-files-to-the-distrbutedcache-and-launching-the-actual-job", 
            "text": "Gobblin takes all WorkUnits created by the Source class and serializes each one into a file on Hadoop. These files are read by each map task, and are deserialized into Gobblin Tasks. These Tasks are then run by the map-task. The reason the job stalls is that Gobblin is writing all these files to HDFS, which can take a while especially if there are a lot of tasks to run. See the  Gobblin Deployment  page for more information.", 
            "title": "Why does Gobblin on Hadoop stall for a long time between adding files to the DistrbutedCache, and launching the actual job?"
        }, 
        {
            "location": "/user-guide/FAQs/#how-do-i-fix-unsupportedfilesystemexception-no-abstractfilesystem-for-scheme-null", 
            "text": "This error typically occurs due to Hadoop version conflict issues. If Gobblin is compiled against a specific Hadoop version, but then deployed on a different Hadoop version or installation, this error may be thrown. For example, if you simply compile Gobblin using  ./gradlew clean build -PuseHadoop2 , but deploy Gobblin to a cluster with  CDH  installed, you may hit this error.  It is important to realize that the the  gobblin-dist.tar.gz  file produced by  ./gradlew clean build  will include all the Hadoop jar dependencies; and if one follows the  MR deployment guide , Gobblin will be launched with these dependencies on the classpath.  To fix this take the following steps:   Delete all the Hadoop jars from the Gobblin  lib  folder  Ensure that the environment variable  HADOOP_CLASSPATH  is set and points to a directory containing the Hadoop libraries for the cluster", 
            "title": "How do I fix UnsupportedFileSystemException: No AbstractFileSystem for scheme: null?"
        }, 
        {
            "location": "/user-guide/FAQs/#how-do-i-compile-gobblin-against-cdh", 
            "text": "Cloudera Distributed Hadoop  (often abbreviated as CDH) is a popular Hadoop distribution. Typically, when running Gobblin on a CDH cluster it is recommended that one also compile Gobblin against the same CDH version. Not doing so may cause unexpected runtime behavior. To compile against a specific CDH version simply use the  hadoopVersion  parameter. For example, to compile against version  2.5.0-cdh5.3.0  run  ./gradlew clean build -PuseHadoop2 -PhadoopVersion=2.5.0-cdh5.3.0 .", 
            "title": "How do I compile Gobblin against CDH?"
        }, 
        {
            "location": "/user-guide/FAQs/#resolve-gobblin-on-mr-exception-ioexception-not-all-tasks-running-in-mapper-attempt_id-completed-successfully", 
            "text": "This exception usually just means that a Hadoop Map Task running Gobblin Tasks threw some exception. Unfortunately, the exception isn't truly indicative of the underlying problem, all it is really saying is that something went wrong in the Gobblin Task. Each Hadoop Map Task has its own log file and it is often easiest to look at the logs of the Map Task when debugging this problem. There are multiple ways to do this, but one of the easiest ways is to execute  yarn logs -applicationId  application ID  [OPTIONS]", 
            "title": "Resolve Gobblin-on-MR Exception IOException: Not all tasks running in mapper attempt_id completed successfully"
        }, 
        {
            "location": "/user-guide/FAQs/#gradle-build-fails-with-cannot-invoke-method-geturls-on-null-object", 
            "text": "Add  -x test  to build the project without running the tests; this will make the exception go away. If one needs to run the tests then make sure  Java Cryptography Extension  is installed.", 
            "title": "Gradle Build Fails With Cannot invoke method getURLs on null object"
        }, 
        {
            "location": "/user-guide/FAQs/#gradle", 
            "text": "", 
            "title": "Gradle"
        }, 
        {
            "location": "/user-guide/FAQs/#technical-questions_1", 
            "text": "", 
            "title": "Technical Questions"
        }, 
        {
            "location": "/user-guide/FAQs/#how-do-i-add-a-new-external-dependency", 
            "text": "Say I want to add  oozie-core-4.2.0.jar  as a dependency to the  gobblin-scheduler  subproject. I would first open the file  build.gradle  and add the following entry to the  ext.externalDependency  array:  \"oozieCore\": \"org.apache.oozie:oozie-core:4.2.0\" .  Then in the  gobblin-scheduler/build.gradle  file I would add the following line to the dependency block:  compile externalDependency.oozieCore .", 
            "title": "How do I add a new external dependency?"
        }, 
        {
            "location": "/user-guide/FAQs/#how-do-i-add-a-new-maven-repository-to-pull-artifacts-from", 
            "text": "Often times, one may have important artifacts stored in a local or private Maven repository. As of 01/21/2016 Gobblin only pulls artifacts from the following Maven Repositories:  Maven Central ,  Conjars , and  Cloudera .  In order to add another Maven Repository modify the  defaultEnvironment.gradle  file and the new repository using the same pattern as the existing ones.", 
            "title": "How do I add a new Maven Repository to pull artifacts from?"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nGetting Started\n\n\nStandalone\n\n\nMapReduce\n\n\n\n\n\n\nSetting up Kafka-HDFS Ingestion Jobs\n\n\nJob Constructs\n\n\nJob Config Properties\n\n\nMetrics and Events\n\n\nMerging and Grouping Workunits in KafkaSource\n\n\nSingle-Level Packing\n\n\nBi-Level Packing\n\n\nAverage Record Size-Based Workunit Size Estimator\n\n\nAverage Record Time-Based Workunit Size Estimator\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started\n\n\nThis section helps you set up a quick-start job for ingesting Kafka topics on a single machine. We provide quick start examples in both standalone and MapReduce mode.\n\n\nStandalone\n\n\n\n\n\n\nSetup a single node Kafka broker by following the \nKafka quick start guide\n. Suppose your broker URI is \nlocalhost:9092\n, and you've created a topic \"test\" with two events \"This is a message\" and \"This is a another message\".\n\n\n\n\n\n\nThe remaining steps are the same as the \nWikipedia example\n, except using the following job config properties:\n\n\n\n\n\n\njob.name=GobblinKafkaQuickStart\njob.group=GobblinKafka\njob.description=Gobblin quick start job for Kafka\njob.lock.enabled=false\n\nkafka.brokers=localhost:9092\n\nsource.class=gobblin.source.extractor.extract.kafka.KafkaSimpleSource\nextract.namespace=gobblin.extract.kafka\n\nwriter.builder.class=gobblin.writer.SimpleDataWriterBuilder\nwriter.file.path.type=tablename\nwriter.destination.type=HDFS\nwriter.output.format=txt\n\ndata.publisher.type=gobblin.publisher.BaseDataPublisher\n\nmr.job.max.mappers=1\n\nmetrics.reporting.file.enabled=true\nmetrics.log.dir=${env:GOBBLIN_WORK_DIR}/metrics\nmetrics.reporting.file.suffix=txt\n\nbootstrap.with.offset=earliest\n\n\n\n\nAfter the job finishes, the following messages should be in the job log:\n\n\nINFO Pulling topic test\nINFO Pulling partition test:0 from offset 0 to 2, range=2\nINFO Finished pulling partition test:0\nINFO Finished pulling topic test\nINFO Extracted 2 data records\nINFO Actual high watermark for partition test:0=2, expected=2\nINFO Task \ntask_id\n completed in 31212ms with state SUCCESSFUL\n\n\n\n\nThe output file will be in \nGOBBLIN_WORK_DIR/job-output/test\n, with the two messages you've just created in the Kafka broker. \nGOBBLIN_WORK_DIR/metrics\n will contain metrics collected from this run.\n\n\nMapReduce\n\n\n\n\nSetup a single node Kafka broker same as in standalone mode.\n\n\nSetup a single node Hadoop cluster by following the steps in \nHadoop: Setting up a Single Node Cluster\n. Suppose your HDFS URI is \nhdfs://localhost:9000\n.\n\n\nCreate a job config file with the following properties:\n\n\n\n\njob.name=GobblinKafkaQuickStart\njob.group=GobblinKafka\njob.description=Gobblin quick start job for Kafka\njob.lock.enabled=false\n\nkafka.brokers=localhost:9092\n\nsource.class=gobblin.source.extractor.extract.kafka.KafkaSimpleSource\nextract.namespace=gobblin.extract.kafka\n\nwriter.builder.class=gobblin.writer.SimpleDataWriterBuilder\nwriter.file.path.type=tablename\nwriter.destination.type=HDFS\nwriter.output.format=txt\n\ndata.publisher.type=gobblin.publisher.BaseDataPublisher\n\nmr.job.max.mappers=1\n\nmetrics.reporting.file.enabled=true\nmetrics.log.dir=/gobblin-kafka/metrics\nmetrics.reporting.file.suffix=txt\n\nbootstrap.with.offset=earliest\n\nfs.uri=hdfs://localhost:9000\nwriter.fs.uri=hdfs://localhost:9000\nstate.store.fs.uri=hdfs://localhost:9000\n\nmr.job.root.dir=/gobblin-kafka/working\nstate.store.dir=/gobblin-kafka/state-store\ntask.data.root.dir=/jobs/kafkaetl/gobblin/gobblin-kafka/task-data\ndata.publisher.final.dir=/gobblintest/job-output\n\n\n\n\n\n\nRun \ngobblin-mapreduce.sh\n:\n\n\n\n\ngobblin-mapreduce.sh --conf \npath-to-job-config-file\n\n\nAfter the job finishes, the job output file will be in \n/gobblintest/job-output/test\n in HDFS, and the metrics will be in \n/gobblin-kafka/metrics\n.\n\n\nSetting up Kafka-HDFS Ingestion Jobs\n\n\nJob Constructs\n\n\nSource and Extractor\n\n\nGobblin provides two abstract classes, \nKafkaSource\n and \nKafkaExtractor\n. \nKafkaSource\n creates a workunit for each Kafka topic partition to be pulled, then merges and groups the workunits based on the desired number of workunits specified by property \nmr.job.max.mappers\n (this property is used in both standalone and MR mode). More details about how workunits are merged and grouped is available \nhere\n. \nKafkaExtractor\n extracts the partitions assigned to a workunit, based on the specified low watermark and high watermark.\n\n\nTo use them in a Kafka-HDFS ingestion job, one should subclass \nKafkaExtractor\n and implement method \ndecodeRecord(MessageAndOffset)\n, which takes a \nMessageAndOffset\n object pulled from the Kafka broker and decodes it into a desired object. One should also subclass \nKafkaSource\n and implement \ngetExtractor(WorkUnitState)\n which should return an instance of the Extractor class.\n\n\nGobblin currently provides two concrete implementations: \nKafkaSimpleSource\n/\nKafkaSimpleExtractor\n, and \nKafkaAvroSource\n/\nKafkaAvroExtractor\n. \n\n\nKafkaSimpleExtractor\n simply returns the payload of the \nMessageAndOffset\n object as a byte array. A job that uses \nKafkaSimpleExtractor\n may use a \nConverter\n to convert the byte array to whatever format desired. For example, if the desired output format is JSON, one may implement an \nByteArrayToJsonConverter\n to convert the byte array to JSON. Alternatively one may implement a \nKafkaJsonExtractor\n, which extends \nKafkaExtractor\n and convert the \nMessageAndOffset\n object into a JSON object in the \ndecodeRecord\n method. Both approaches should work equally well.\n\n\nKafkaAvroExtractor\n decodes the payload of the \nMessageAndOffset\n object into an Avro \nGenericRecord\n object. It requires that the byte 0 of the payload be 0, bytes 1-16 of the payload be a 16-byte schema ID, and the remaining bytes be the encoded Avro record. It also requires the existence of a schema registry that returns the Avro schema given the schema ID, which is used to decode the byte array. Thus this class is mainly applicable to LinkedIn's internal Kafka clusters.\n\n\nWriter and Publisher\n\n\nAny desired writer and publisher can be used, e.g., one may use the \nAvroHdfsDataWriter\n and the \nBaseDataPublisher\n, similar as the \nWikipedia example job\n. If plain text output file is desired, one may use \nSimpleDataWriter\n.\n\n\nJob Config Properties\n\n\nThese are some of the job config properties used by \nKafkaSource\n and \nKafkaExtractor\n.\n\n\n\n\n\n\n\n\nProperty Name\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\ntopic.whitelist\n (regex)\n\n\nKafka topics to be pulled. Default value = .*\n\n\n\n\n\n\ntopic.blacklist\n (regex)\n\n\nKafka topics not to be pulled. Default value = empty\n\n\n\n\n\n\nkafka.brokers\n\n\nComma separated Kafka brokers to ingest data from.\n\n\n\n\n\n\nmr.job.max.mappers\n\n\nNumber of tasks to launch. In MR mode, this will be the number of mappers launched. If the number of topic partitions to be pulled is larger than the number of tasks, \nKafkaSource\n will assign partitions to tasks in a balanced manner.\n\n\n\n\n\n\nbootstrap.with.offset\n\n\nFor new topics / partitions, this property controls whether they start at the earliest offset or the latest offset. Possible values: earliest, latest, skip. Default: latest\n\n\n\n\n\n\nreset.on.offset.out.of.range\n\n\nThis property controls what to do if a partition's previously persisted offset is out of the range of the currently available offsets. Possible values: earliest (always move to earliest available offset), latest (always move to latest available offset), nearest (move to earliest if the previously persisted offset is smaller than the earliest offset, otherwise move to latest), skip (skip this partition). Default: nearest\n\n\n\n\n\n\ntopics.move.to.latest.offset\n (no regex)\n\n\nTopics in this list will always start from the latest offset (i.e., no records will be pulled). To move all topics to the latest offset, use \"all\". This property should rarely, if ever, be used.\n\n\n\n\n\n\n\n\nIt is also possible to set a time limit for each task. For example, to set the time limit to 15 minutes, set the following properties:\n\n\nextract.limit.enabled=true\nextract.limit.type=time #(other possible values: rate, count, pool)\nextract.limit.time.limit=15\nextract.limit.time.limit.timeunit=minutes \n\n\n\n\nMetrics and Events\n\n\nTask Level Metrics\n\n\nTask level metrics can be created in \nExtractor\n, \nConverter\n and \nWriter\n by extending \nInstrumentedExtractor\n, \nInstrumentedConverter\n and \nInstrumentedDataWriter\n.\n\n\nFor example, \nKafkaExtractor\n extends \nInstrumentedExtractor\n. So you can do the following in subclasses of \nKafkaExtractor\n:\n\n\nCounter decodingErrorCounter = this.getMetricContext().counter(\nnum.of.decoding.errors\n);\ndecodingErrorCounter.inc();\n\n\n\n\nBesides Counter, Meter and Histogram are also supported.\n\n\nTask Level Events\n\n\nTask level events can be submitted by creating an \nEventSubmitter\n instance and using \nEventSubmitter.submit()\n or \nEventSubmitter.getTimingEvent()\n.\n\n\nJob Level Metrics\n\n\nTo create job level metrics, one may extend \nAbstractJobLauncher\n and create metrics there. For example:\n\n\nOptional\nJobMetrics\n jobMetrics = this.jobContext.getJobMetricsOptional();\nif (!jobMetrics.isPresent()) {\n  LOG.warn(\njob metrics is absent\n);\n  return;\n}\nCounter recordsWrittenCounter = jobMetrics.get().getCounter(\njob.records.written\n);\nrecordsWrittenCounter.inc(value);\n\n\n\n\nJob level metrics are often aggregations of task level metrics, such as the \njob.records.written\n counter above. Since \nAbstractJobLauncher\n doesn't have access to task-level metrics, one should set these counters in \nTaskState\ns, and override \nAbstractJobLauncher.postProcessTaskStates()\n to aggregate them. For example, in \nAvroHdfsTimePartitionedWriter.close()\n, property \nwriter.records.written\n is set for the \nTaskState\n. \n\n\nJob Level Events\n\n\nJob level events can be created by extending \nAbstractJobLauncher\n and use \nthis.eventSubmitter.submit()\n or \nthis.eventSubmitter.getTimingEvent()\n.\n\n\nFor more details about metrics, events and reporting them, please see Gobblin Metrics section.\n\n\nMerging and Grouping Workunits in \nKafkaSource\n\n\nFor each topic partition that should be ingested, \nKafkaSource\n first retrieves the last offset pulled by the previous run, which should be the first offset of the current run. It also retrieves the earliest and latest offsets currently available from the Kafka cluster and verifies that the first offset is between the earliest and the latest offsets. The latest offset is the last offset to be pulled by the current workunit. Since new records may be constantly published to Kafka and old records are deleted based on retention policies, the earliest and latest offsets of a partition may change constantly.\n\n\nFor each partition, after the first and last offsets are determined, a workunit is created. If the number of Kafka partitions exceeds the desired number of workunits specified by property \nmr.job.max.mappers\n, \nKafkaSource\n will merge and group them into \nn\n \nMultiWorkUnit\ns where \nn=mr.job.max.mappers\n. This is done using \nKafkaWorkUnitPacker\n, which has two implementations: \nKafkaSingleLevelWorkUnitPacker\n and \nKafkaBiLevelWorkUnitPacker\n. The packer packs workunits based on the estimated size of each workunit, which is obtained from \nKafkaWorkUnitSizeEstimator\n, which also has two implementations, \nKafkaAvgRecordSizeBasedWorkUnitSizeEstimator\n and \nKafkaAvgRecordTimeBasedWorkUnitSizeEstimator\n.\n\n\nSingle-Level Packing\n\n\nThe single-level packer uses a worst-fit-decreasing approach for assigning workunits to mappers: each workunit goes to the mapper that currently has the lightest load. This approach balances the mappers well. However, multiple partitions of the same topic are usually assigned to different mappers. This may cause two issues: (1) many small output files: if multiple partitions of a topic are assigned to different mappers, they cannot share output files. (2) task overhead: when multiple partitions of a topic are assigned to different mappers, a task is created for each partition, which may lead to a large number of tasks and large overhead.\n\n\nBi-Level Packing\n\n\nThe bi-level packer packs workunits in two steps.\n\n\nIn the first step, all workunits are grouped into approximately \n3n\n groups, each of which contains partitions of the same topic. The max group size is set as\n\n\nmaxGroupSize = totalWorkunitSize/3n\n\n\nThe best-fit-decreasing algorithm is run on all partitions of each topic. If an individual workunit\u2019s size exceeds \nmaxGroupSize\n, it is put in a separate group. For each group, a new workunit is created which will be responsible for extracting all partitions in the group.\n\n\nThe reason behind \n3n\n is that if this number is too small (i.e., too close to \nn\n), it is difficult for the second level to pack these groups into n balanced multiworkunits; if this number is too big, \navgGroupSize\n will be small which doesn\u2019t help grouping partitions of the same topic together. \n3n\n is a number that is empirically selected.\n\n\nThe second step uses the same worst-fit-decreasing method as the first-level packer.\n\n\nThis approach reduces the number of small files and the number of tasks, but it may have more mapper skew for two reasons: (1) in the worst-fit-decreasing approach, the less number of items to be packed, the more skew there will be; (2) when multiple partitions of a topic are assigned to the same mapper, if we underestimate the size of this topic, this mapper may take a much longer time than other mappers and the entire MR job has to wait for this mapper. This, however, can be mitigated by setting a time limit for each task, as explained above.\n\n\nAverage Record Size-Based Workunit Size Estimator\n\n\nThis size estimator uses the average record size of each partition to estimate the sizes of workunits. When using this size estimator, each job run will record the average record size of each partition it pulled. In the next run, for each partition the average record size pulled in the previous run is considered the average record size\nto be pulled in this run.\n\n\nIf a partition was not pulled in a run, a default value of 1024 will be used in the next run.\n\n\nAverage Record Time-Based Workunit Size Estimator\n\n\nThis size estimator uses the average time to pull a record in each run to estimate the sizes of the workunits in the next run.\n\n\nWhen using this size estimator, each job run will record the average time per record of each partition. In the next run, the estimated average time per record for each topic is the geometric mean of the avg time per record of all partitions. For example if a topic has two partitions whose average time per record in the previous run are 2 and 8, the next run will use 4 as the estimated average time per record.\n\n\nIf a topic is not pulled in a run, its estimated average time per record is the geometric mean of the estimated average time per record of all topics that are pulled in this run. If no topic was pulled in this run, a default value of 1.0 is used.\n\n\nThe time-based estimator is more accurate than the size-based estimator when the time to pull a record is not proportional to the size of the record. However, the time-based estimator may lose accuracy when there are fluctuations in the Hadoop cluster which causes the average time for a partition to vary between different runs.", 
            "title": "Kafka-HDFS Ingestion"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#table-of-contents", 
            "text": "Table of Contents  Getting Started  Standalone  MapReduce    Setting up Kafka-HDFS Ingestion Jobs  Job Constructs  Job Config Properties  Metrics and Events  Merging and Grouping Workunits in KafkaSource  Single-Level Packing  Bi-Level Packing  Average Record Size-Based Workunit Size Estimator  Average Record Time-Based Workunit Size Estimator", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#getting-started", 
            "text": "This section helps you set up a quick-start job for ingesting Kafka topics on a single machine. We provide quick start examples in both standalone and MapReduce mode.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#standalone", 
            "text": "Setup a single node Kafka broker by following the  Kafka quick start guide . Suppose your broker URI is  localhost:9092 , and you've created a topic \"test\" with two events \"This is a message\" and \"This is a another message\".    The remaining steps are the same as the  Wikipedia example , except using the following job config properties:    job.name=GobblinKafkaQuickStart\njob.group=GobblinKafka\njob.description=Gobblin quick start job for Kafka\njob.lock.enabled=false\n\nkafka.brokers=localhost:9092\n\nsource.class=gobblin.source.extractor.extract.kafka.KafkaSimpleSource\nextract.namespace=gobblin.extract.kafka\n\nwriter.builder.class=gobblin.writer.SimpleDataWriterBuilder\nwriter.file.path.type=tablename\nwriter.destination.type=HDFS\nwriter.output.format=txt\n\ndata.publisher.type=gobblin.publisher.BaseDataPublisher\n\nmr.job.max.mappers=1\n\nmetrics.reporting.file.enabled=true\nmetrics.log.dir=${env:GOBBLIN_WORK_DIR}/metrics\nmetrics.reporting.file.suffix=txt\n\nbootstrap.with.offset=earliest  After the job finishes, the following messages should be in the job log:  INFO Pulling topic test\nINFO Pulling partition test:0 from offset 0 to 2, range=2\nINFO Finished pulling partition test:0\nINFO Finished pulling topic test\nINFO Extracted 2 data records\nINFO Actual high watermark for partition test:0=2, expected=2\nINFO Task  task_id  completed in 31212ms with state SUCCESSFUL  The output file will be in  GOBBLIN_WORK_DIR/job-output/test , with the two messages you've just created in the Kafka broker.  GOBBLIN_WORK_DIR/metrics  will contain metrics collected from this run.", 
            "title": "Standalone"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#mapreduce", 
            "text": "Setup a single node Kafka broker same as in standalone mode.  Setup a single node Hadoop cluster by following the steps in  Hadoop: Setting up a Single Node Cluster . Suppose your HDFS URI is  hdfs://localhost:9000 .  Create a job config file with the following properties:   job.name=GobblinKafkaQuickStart\njob.group=GobblinKafka\njob.description=Gobblin quick start job for Kafka\njob.lock.enabled=false\n\nkafka.brokers=localhost:9092\n\nsource.class=gobblin.source.extractor.extract.kafka.KafkaSimpleSource\nextract.namespace=gobblin.extract.kafka\n\nwriter.builder.class=gobblin.writer.SimpleDataWriterBuilder\nwriter.file.path.type=tablename\nwriter.destination.type=HDFS\nwriter.output.format=txt\n\ndata.publisher.type=gobblin.publisher.BaseDataPublisher\n\nmr.job.max.mappers=1\n\nmetrics.reporting.file.enabled=true\nmetrics.log.dir=/gobblin-kafka/metrics\nmetrics.reporting.file.suffix=txt\n\nbootstrap.with.offset=earliest\n\nfs.uri=hdfs://localhost:9000\nwriter.fs.uri=hdfs://localhost:9000\nstate.store.fs.uri=hdfs://localhost:9000\n\nmr.job.root.dir=/gobblin-kafka/working\nstate.store.dir=/gobblin-kafka/state-store\ntask.data.root.dir=/jobs/kafkaetl/gobblin/gobblin-kafka/task-data\ndata.publisher.final.dir=/gobblintest/job-output   Run  gobblin-mapreduce.sh :   gobblin-mapreduce.sh --conf  path-to-job-config-file  After the job finishes, the job output file will be in  /gobblintest/job-output/test  in HDFS, and the metrics will be in  /gobblin-kafka/metrics .", 
            "title": "MapReduce"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#setting-up-kafka-hdfs-ingestion-jobs", 
            "text": "", 
            "title": "Setting up Kafka-HDFS Ingestion Jobs"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#job-constructs", 
            "text": "Source and Extractor  Gobblin provides two abstract classes,  KafkaSource  and  KafkaExtractor .  KafkaSource  creates a workunit for each Kafka topic partition to be pulled, then merges and groups the workunits based on the desired number of workunits specified by property  mr.job.max.mappers  (this property is used in both standalone and MR mode). More details about how workunits are merged and grouped is available  here .  KafkaExtractor  extracts the partitions assigned to a workunit, based on the specified low watermark and high watermark.  To use them in a Kafka-HDFS ingestion job, one should subclass  KafkaExtractor  and implement method  decodeRecord(MessageAndOffset) , which takes a  MessageAndOffset  object pulled from the Kafka broker and decodes it into a desired object. One should also subclass  KafkaSource  and implement  getExtractor(WorkUnitState)  which should return an instance of the Extractor class.  Gobblin currently provides two concrete implementations:  KafkaSimpleSource / KafkaSimpleExtractor , and  KafkaAvroSource / KafkaAvroExtractor .   KafkaSimpleExtractor  simply returns the payload of the  MessageAndOffset  object as a byte array. A job that uses  KafkaSimpleExtractor  may use a  Converter  to convert the byte array to whatever format desired. For example, if the desired output format is JSON, one may implement an  ByteArrayToJsonConverter  to convert the byte array to JSON. Alternatively one may implement a  KafkaJsonExtractor , which extends  KafkaExtractor  and convert the  MessageAndOffset  object into a JSON object in the  decodeRecord  method. Both approaches should work equally well.  KafkaAvroExtractor  decodes the payload of the  MessageAndOffset  object into an Avro  GenericRecord  object. It requires that the byte 0 of the payload be 0, bytes 1-16 of the payload be a 16-byte schema ID, and the remaining bytes be the encoded Avro record. It also requires the existence of a schema registry that returns the Avro schema given the schema ID, which is used to decode the byte array. Thus this class is mainly applicable to LinkedIn's internal Kafka clusters.  Writer and Publisher  Any desired writer and publisher can be used, e.g., one may use the  AvroHdfsDataWriter  and the  BaseDataPublisher , similar as the  Wikipedia example job . If plain text output file is desired, one may use  SimpleDataWriter .", 
            "title": "Job Constructs"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#job-config-properties", 
            "text": "These are some of the job config properties used by  KafkaSource  and  KafkaExtractor .     Property Name  Semantics      topic.whitelist  (regex)  Kafka topics to be pulled. Default value = .*    topic.blacklist  (regex)  Kafka topics not to be pulled. Default value = empty    kafka.brokers  Comma separated Kafka brokers to ingest data from.    mr.job.max.mappers  Number of tasks to launch. In MR mode, this will be the number of mappers launched. If the number of topic partitions to be pulled is larger than the number of tasks,  KafkaSource  will assign partitions to tasks in a balanced manner.    bootstrap.with.offset  For new topics / partitions, this property controls whether they start at the earliest offset or the latest offset. Possible values: earliest, latest, skip. Default: latest    reset.on.offset.out.of.range  This property controls what to do if a partition's previously persisted offset is out of the range of the currently available offsets. Possible values: earliest (always move to earliest available offset), latest (always move to latest available offset), nearest (move to earliest if the previously persisted offset is smaller than the earliest offset, otherwise move to latest), skip (skip this partition). Default: nearest    topics.move.to.latest.offset  (no regex)  Topics in this list will always start from the latest offset (i.e., no records will be pulled). To move all topics to the latest offset, use \"all\". This property should rarely, if ever, be used.     It is also possible to set a time limit for each task. For example, to set the time limit to 15 minutes, set the following properties:  extract.limit.enabled=true\nextract.limit.type=time #(other possible values: rate, count, pool)\nextract.limit.time.limit=15\nextract.limit.time.limit.timeunit=minutes", 
            "title": "Job Config Properties"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#metrics-and-events", 
            "text": "Task Level Metrics  Task level metrics can be created in  Extractor ,  Converter  and  Writer  by extending  InstrumentedExtractor ,  InstrumentedConverter  and  InstrumentedDataWriter .  For example,  KafkaExtractor  extends  InstrumentedExtractor . So you can do the following in subclasses of  KafkaExtractor :  Counter decodingErrorCounter = this.getMetricContext().counter( num.of.decoding.errors );\ndecodingErrorCounter.inc();  Besides Counter, Meter and Histogram are also supported.  Task Level Events  Task level events can be submitted by creating an  EventSubmitter  instance and using  EventSubmitter.submit()  or  EventSubmitter.getTimingEvent() .  Job Level Metrics  To create job level metrics, one may extend  AbstractJobLauncher  and create metrics there. For example:  Optional JobMetrics  jobMetrics = this.jobContext.getJobMetricsOptional();\nif (!jobMetrics.isPresent()) {\n  LOG.warn( job metrics is absent );\n  return;\n}\nCounter recordsWrittenCounter = jobMetrics.get().getCounter( job.records.written );\nrecordsWrittenCounter.inc(value);  Job level metrics are often aggregations of task level metrics, such as the  job.records.written  counter above. Since  AbstractJobLauncher  doesn't have access to task-level metrics, one should set these counters in  TaskState s, and override  AbstractJobLauncher.postProcessTaskStates()  to aggregate them. For example, in  AvroHdfsTimePartitionedWriter.close() , property  writer.records.written  is set for the  TaskState .   Job Level Events  Job level events can be created by extending  AbstractJobLauncher  and use  this.eventSubmitter.submit()  or  this.eventSubmitter.getTimingEvent() .  For more details about metrics, events and reporting them, please see Gobblin Metrics section.", 
            "title": "Metrics and Events"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#merging-and-grouping-workunits-in-kafkasource", 
            "text": "For each topic partition that should be ingested,  KafkaSource  first retrieves the last offset pulled by the previous run, which should be the first offset of the current run. It also retrieves the earliest and latest offsets currently available from the Kafka cluster and verifies that the first offset is between the earliest and the latest offsets. The latest offset is the last offset to be pulled by the current workunit. Since new records may be constantly published to Kafka and old records are deleted based on retention policies, the earliest and latest offsets of a partition may change constantly.  For each partition, after the first and last offsets are determined, a workunit is created. If the number of Kafka partitions exceeds the desired number of workunits specified by property  mr.job.max.mappers ,  KafkaSource  will merge and group them into  n   MultiWorkUnit s where  n=mr.job.max.mappers . This is done using  KafkaWorkUnitPacker , which has two implementations:  KafkaSingleLevelWorkUnitPacker  and  KafkaBiLevelWorkUnitPacker . The packer packs workunits based on the estimated size of each workunit, which is obtained from  KafkaWorkUnitSizeEstimator , which also has two implementations,  KafkaAvgRecordSizeBasedWorkUnitSizeEstimator  and  KafkaAvgRecordTimeBasedWorkUnitSizeEstimator .", 
            "title": "Merging and Grouping Workunits in KafkaSource"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#single-level-packing", 
            "text": "The single-level packer uses a worst-fit-decreasing approach for assigning workunits to mappers: each workunit goes to the mapper that currently has the lightest load. This approach balances the mappers well. However, multiple partitions of the same topic are usually assigned to different mappers. This may cause two issues: (1) many small output files: if multiple partitions of a topic are assigned to different mappers, they cannot share output files. (2) task overhead: when multiple partitions of a topic are assigned to different mappers, a task is created for each partition, which may lead to a large number of tasks and large overhead.", 
            "title": "Single-Level Packing"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#bi-level-packing", 
            "text": "The bi-level packer packs workunits in two steps.  In the first step, all workunits are grouped into approximately  3n  groups, each of which contains partitions of the same topic. The max group size is set as  maxGroupSize = totalWorkunitSize/3n  The best-fit-decreasing algorithm is run on all partitions of each topic. If an individual workunit\u2019s size exceeds  maxGroupSize , it is put in a separate group. For each group, a new workunit is created which will be responsible for extracting all partitions in the group.  The reason behind  3n  is that if this number is too small (i.e., too close to  n ), it is difficult for the second level to pack these groups into n balanced multiworkunits; if this number is too big,  avgGroupSize  will be small which doesn\u2019t help grouping partitions of the same topic together.  3n  is a number that is empirically selected.  The second step uses the same worst-fit-decreasing method as the first-level packer.  This approach reduces the number of small files and the number of tasks, but it may have more mapper skew for two reasons: (1) in the worst-fit-decreasing approach, the less number of items to be packed, the more skew there will be; (2) when multiple partitions of a topic are assigned to the same mapper, if we underestimate the size of this topic, this mapper may take a much longer time than other mappers and the entire MR job has to wait for this mapper. This, however, can be mitigated by setting a time limit for each task, as explained above.", 
            "title": "Bi-Level Packing"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#average-record-size-based-workunit-size-estimator", 
            "text": "This size estimator uses the average record size of each partition to estimate the sizes of workunits. When using this size estimator, each job run will record the average record size of each partition it pulled. In the next run, for each partition the average record size pulled in the previous run is considered the average record size\nto be pulled in this run.  If a partition was not pulled in a run, a default value of 1024 will be used in the next run.", 
            "title": "Average Record Size-Based Workunit Size Estimator"
        }, 
        {
            "location": "/case-studies/Kafka-HDFS-Ingestion/#average-record-time-based-workunit-size-estimator", 
            "text": "This size estimator uses the average time to pull a record in each run to estimate the sizes of the workunits in the next run.  When using this size estimator, each job run will record the average time per record of each partition. In the next run, the estimated average time per record for each topic is the geometric mean of the avg time per record of all partitions. For example if a topic has two partitions whose average time per record in the previous run are 2 and 8, the next run will use 4 as the estimated average time per record.  If a topic is not pulled in a run, its estimated average time per record is the geometric mean of the estimated average time per record of all topics that are pulled in this run. If no topic was pulled in this run, a default value of 1.0 is used.  The time-based estimator is more accurate than the size-based estimator when the time to pull a record is not proportional to the size of the record. However, the time-based estimator may lose accuracy when there are fluctuations in the Hadoop cluster which causes the average time for a partition to vary between different runs.", 
            "title": "Average Record Time-Based Workunit Size Estimator"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nHadoop and S3\n\n\nThe s3a File System\n\n\nThe s3 File System\n\n\n\n\n\n\nGetting Gobblin to Publish to S3\n\n\nSigning Up For AWS\n\n\nSetting Up EC2\n\n\nLaunching an EC2 Instance\n\n\nEC2 Package Installations\n\n\nInstalling Java\n\n\n\n\n\n\n\n\n\n\nSetting Up S3\n\n\nSetting Up Gobblin on EC2\n\n\nConfiguring Gobblin on EC2\n\n\nLaunching Gobblin on EC2\n\n\nWriting to S3 Outside EC2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nWhile Gobblin is not tied to any specific cloud provider, \nAmazon Web Services\n is a popular choice. This document will outline how Gobblin can publish data to \nS3\n. Specifically, it will provide a step by step guide to help setup Gobblin on Amazon \nEC2\n, run Gobblin on EC2, and publish data from EC2 to S3.\n\n\nIt is recommended to configure Gobblin to first write data to \nEBS\n, and then publish the data to S3. This is the recommended approach because there are a few caveats when working with with S3. See the \nHadoop and S3\n section for more details.\n\n\nThis document will also provide a step by step guide for launching and configuring an EC2 instance and creating a S3 bucket. However, it is by no means a source of truth guide to working with AWS, it will only provide high level steps. The best place to learn about how to use AWS is through the \nAmazon documentation\n.\n\n\nHadoop and S3\n\n\nA majority of Gobblin's code base uses Hadoop's \nFileSystem\n object to read and write data. The \nFileSystem\n object is an abstract class, and typical implementations either write to the local file system, or write to HDFS. There has been significant work to create an implementation of the \nFileSystem\n object that reads and writes to S3. The best guide to read about the different S3 \nFileSystem\n implementations is \nhere\n.\n\n\nThere are a few different S3 \nFileSystem\n implementations, the two of note are the \ns3a\n and the \ns3\n file systems. The \ns3a\n file system is relatively new and is only available in Hadoop 2.6.0 (see the original \nJIRA\n for more information). The \ns3\n filesystem has been around for a while.\n\n\nThe \ns3a\n File System\n\n\nThe \ns3a\n file system uploads files to a specified bucket. The data uploaded to S3 via this file system is interoperable with other S3 tools. However, there are a few caveats when working with this file system:\n\n\n\n\nSince S3 does not support renaming of files in a bucket, the \nS3AFileSystem.rename(Path, Path)\n operation will actually copy data from the source \nPath\n to the destination \nPath\n, and then delete the source \nPath\n (see the \nsource code\n for more information)\n\n\nWhen creating a file using \nS3AFileSystem.create(...)\n data will be first written to a staging file on the local file system, and when the file is closed, the staging file will be uploaded to S3 (see the \nsource code\n for more information)\n\n\n\n\nThus, when using the \ns3a\n file system with Gobblin it is recommended that one configures Gobblin to first write its staging data to the local filesystem, and then to publish the data to S3. The reason this is the recommended approach is that each Gobblin \nTask\n will write data to a staging file, and once the file has been completely written it publishes the file to a output directory (it does this by using a rename function). Finally, the \nDataPublisher\n moves the files from the staging directory to its final directory (again done using a rename function). This requires two renames operations and would be very inefficient if a \nTask\n wrote directly to S3.\n\n\nFurthermore, writing directly to S3 requires creating a staging file on the local file system, and then creating a \nPutObjectRequest\n to upload the data to S3. This is logically equivalent to just configuring Gobblin to write to a local file and then publishing it to S3.\n\n\nThe \ns3\n File System\n\n\nThe \ns3\n file system stores file as blocks, similar to how HDFS stores blocks. This makes renaming of files more efficient, but data written using this file system is not interoperable with other S3 tools. This limitation may make using this file system less desirable, so the majority of this document focuses on the \ns3a\n file system. Although the majority of the walkthrough should apply for the \ns3\n file system also.\n\n\nGetting Gobblin to Publish to S3\n\n\nThis section will provide a step by step guide to setting up an EC2 instance, a S3 bucket, installing Gobblin on EC2, and configuring Gobblin to publish data to S3.\n\n\nThis guide will use the free-tier provided by AWS to setup EC2 and S3.\n\n\nSigning Up For AWS\n\n\nIn order to use EC2 and S3, one first needs to sign up for an AWS account. The easiest way to get started with AWS is to use their \nfree tier\n.\n\n\nSetting Up EC2\n\n\nLaunching an EC2 Instance\n\n\nOnce you have an AWS account, login to the AWS \nconsole\n. Select the EC2 link, which will bring you to the \nEC2 dashboard\n.\n\n\nClick on \nLaunch Instance\n to create a new EC2 instance. Before the instance actually starts to run, there area a few more configuration steps necessary:\n\n\n\n\nChoose an Amazon Machine Image (\nAMI\n)\n\n\nFor this walkthrough we will pick Red Hat Enterprise Linux (\nRHEL\n) AMI\n\n\n\n\n\n\nChoose an Instance Type\n\n\nSince this walkthrough uses the Amazon Free Tier, we will pick the General Purpose \nt2.micro\n instance\n\n\nThis instance provides us with 1 vCPU and 1 GiB of RAM\n\n\n\n\n\n\nFor more information on other instance types, check out the AWS \ndocs\n\n\n\n\n\n\nClick Review and Launch\n\n\nWe will use the defaults for all other setting options\n\n\nWhen reviewing your instance, you will most likely get a warning saying access to your EC2 instance is open to the world\n\n\nIf you want to fix this you have to edit the \nSecurity Groups\n; how to do that is out of the scope of this document\n\n\n\n\n\n\nSet Up SSH Keys\n\n\nAfter reviewing your instance, click \nLaunch\n\n\nYou should be prompted to setup \nSSH\n keys\n\n\nUse an existing key pair if you have one, otherwise create a new one and download it\n\n\n\n\n\n\nSSH to Launched Instance\n\n\nSSH using the following command: \nssh -i my-private-key-file.pem ec2-user@instance-name\n\n\nThe \ninstance-name\n can be taken from the \nPublic DNS\n field from the instance information\n\n\nSSH may complain that the private key file has insufficient permissions\n\n\nExecute \nchmod 600 my-private-key-file.pem\n to fix this\n\n\n\n\n\n\nAlternatively, one can modify the \n~/.ssh/config\n file instead of specifying the \n-i\n option\n\n\n\n\n\n\n\n\n\n\n\n\nAfter following the above steps, you should be able to freely SSH into the launched EC2 instance, and monitor / control the instance from the \nEC2 dashboard\n.\n\n\nEC2 Package Installations\n\n\nBefore setting up Gobblin, you need to install \nJava\n first. Depending on the AMI instance you are running Java may or may not already be installed (you can check if Java is already installed by executing \njava -version\n).\n\n\nInstalling Java\n\n\n\n\nExecute \nsudo yum install java-1.8.0-openjdk*\n to install Open JDK 8\n\n\nConfirm the installation was successful by executing \njava -version\n\n\nSet the \nJAVA_HOME\n environment variable in the \n~/.bashrc/\n file\n\n\nThe value for \nJAVA_HOME\n can be found by executing \nreadlink `which java`\n\n\n\n\n\n\n\n\nSetting Up S3\n\n\nGo to the \nS3 dashboard\n\n\n\n\nClick on \nCreate Bucket\n\n\nEnter a name for the bucket (e.g. \ngobblin-demo-bucket\n)\n\n\nEnter a \nRegion\n for the bucket (e.g. \nUS Standard\n)\n\n\n\n\n\n\n\n\nSetting Up Gobblin on EC2\n\n\n\n\nDownload and Build Gobblin Locally\n\n\nOn your local machine, clone the \nGobblin repository\n: \ngit clone git@github.com:linkedin/gobblin.git\n (this assumes you have \nGit\n installed locally)\n\n\nBuild Gobblin using the following commands (it is important to use Hadoop version 2.6.0 as it includes the \ns3a\n file system implementation):\n\n\n\n\n\n\n\n\ncd gobblin\n./gradlew clean build -PuseHadoop2 -PhadoopVersion=2.6.0 -x test\n\n\n\n\n\n\nUpload the Gobblin Tar to EC2\n\n\nExecute the command: \n\n\n\n\n\n\n\n\nscp -i my-private-key-file.pem gobblin-dist-[project-version].tar.gz ec2-user@instance-name:\n\n\n\n\n\n\nUn-tar the Gobblin Distribution\n\n\nSSH to the EC2 Instance\n\n\nUn-tar the Gobblin distribution: \ntar -xvf gobblin-dist-[project-version].tar.gz\n\n\n\n\n\n\nDownload AWS Libraries\n\n\nA few JARs need to be downloaded using some cURL commands:\n\n\n\n\n\n\n\n\ncurl http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar \n gobblin-dist/lib/aws-java-sdk-1.7.4.jar\ncurl http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.6.0/hadoop-aws-2.6.0.jar \n gobblin-dist/lib/hadoop-aws-2.6.0.jar\n\n\n\n\nConfiguring Gobblin on EC2\n\n\nAssuming we are running Gobblin in \nstandalone mode\n, the following configuration options need to be modified in the file \ngobblin-dist/conf/gobblin-standalone.properties\n.\n\n\n\n\nAdd the key \ndata.publisher.fs.uri\n and set it to \ns3a://gobblin-demo-bucket/\n\n\nThis configures the job to publish data to the S3 bucket named \ngobblin-demo-bucket\n\n\n\n\n\n\nAdd the AWS Access Key Id and Secret Access Key\n\n\nSet the keys \nfs.s3a.access.key\n and \nfs.s3a.secret.key\n to the appropriate values\n\n\nThese keys correspond to \nAWS security credentials\n\n\nFor information on how to get these credentials, check out the AWS documentation \nhere\n\n\nThe AWS documentation recommends using \nIAM roles\n; how to set this up is out of the scope of this document; for this walkthrough we will use root access credentials\n\n\n\n\n\n\n\n\nLaunching Gobblin on EC2\n\n\nAssuming we want Gobblin to run in standalone mode, follow the usual steps for \nstandalone deployment\n.\n\n\nFor the sake of this walkthrough, we will launch the Gobblin \nwikipedia example\n. Directions on how to run this example can be found \nhere\n. The command to launch Gobblin should look similar to:\n\n\nsh bin/gobblin-standalone.sh start --workdir /home/ec2-user/gobblin-dist/work --logdir /home/ec2-user/gobblin-dist/logs --conf /home/ec2-user/gobblin-dist/config\n\n\n\n\nIf you are running on the Amazon free tier, you will probably get an error in the \nnohup.out\n file saying there is insufficient memory for the JVM. To fix this add \n--jvmflags \"-Xms256m -Xmx512m\"\n to the \nstart\n command.\n\n\nData should be written to S3 during the publishing phase of Gobblin. One can confirm data was successfully written to S3 by looking at the \nS3 dashboard\n.\n\n\nWriting to S3 Outside EC2\n\n\nIt is possible to write to an S3 bucket outside of an EC2 instance. The setup steps are similar to walkthrough outlined above. For more information on writing to S3 outside of AWS, check out \nthis article\n.", 
            "title": "Publishing Data to S3"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#table-of-contents", 
            "text": "Table of Contents  Introduction  Hadoop and S3  The s3a File System  The s3 File System    Getting Gobblin to Publish to S3  Signing Up For AWS  Setting Up EC2  Launching an EC2 Instance  EC2 Package Installations  Installing Java      Setting Up S3  Setting Up Gobblin on EC2  Configuring Gobblin on EC2  Launching Gobblin on EC2  Writing to S3 Outside EC2", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#introduction", 
            "text": "While Gobblin is not tied to any specific cloud provider,  Amazon Web Services  is a popular choice. This document will outline how Gobblin can publish data to  S3 . Specifically, it will provide a step by step guide to help setup Gobblin on Amazon  EC2 , run Gobblin on EC2, and publish data from EC2 to S3.  It is recommended to configure Gobblin to first write data to  EBS , and then publish the data to S3. This is the recommended approach because there are a few caveats when working with with S3. See the  Hadoop and S3  section for more details.  This document will also provide a step by step guide for launching and configuring an EC2 instance and creating a S3 bucket. However, it is by no means a source of truth guide to working with AWS, it will only provide high level steps. The best place to learn about how to use AWS is through the  Amazon documentation .", 
            "title": "Introduction"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#hadoop-and-s3", 
            "text": "A majority of Gobblin's code base uses Hadoop's  FileSystem  object to read and write data. The  FileSystem  object is an abstract class, and typical implementations either write to the local file system, or write to HDFS. There has been significant work to create an implementation of the  FileSystem  object that reads and writes to S3. The best guide to read about the different S3  FileSystem  implementations is  here .  There are a few different S3  FileSystem  implementations, the two of note are the  s3a  and the  s3  file systems. The  s3a  file system is relatively new and is only available in Hadoop 2.6.0 (see the original  JIRA  for more information). The  s3  filesystem has been around for a while.", 
            "title": "Hadoop and S3"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#the-s3a-file-system", 
            "text": "The  s3a  file system uploads files to a specified bucket. The data uploaded to S3 via this file system is interoperable with other S3 tools. However, there are a few caveats when working with this file system:   Since S3 does not support renaming of files in a bucket, the  S3AFileSystem.rename(Path, Path)  operation will actually copy data from the source  Path  to the destination  Path , and then delete the source  Path  (see the  source code  for more information)  When creating a file using  S3AFileSystem.create(...)  data will be first written to a staging file on the local file system, and when the file is closed, the staging file will be uploaded to S3 (see the  source code  for more information)   Thus, when using the  s3a  file system with Gobblin it is recommended that one configures Gobblin to first write its staging data to the local filesystem, and then to publish the data to S3. The reason this is the recommended approach is that each Gobblin  Task  will write data to a staging file, and once the file has been completely written it publishes the file to a output directory (it does this by using a rename function). Finally, the  DataPublisher  moves the files from the staging directory to its final directory (again done using a rename function). This requires two renames operations and would be very inefficient if a  Task  wrote directly to S3.  Furthermore, writing directly to S3 requires creating a staging file on the local file system, and then creating a  PutObjectRequest  to upload the data to S3. This is logically equivalent to just configuring Gobblin to write to a local file and then publishing it to S3.", 
            "title": "The s3a File System"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#the-s3-file-system", 
            "text": "The  s3  file system stores file as blocks, similar to how HDFS stores blocks. This makes renaming of files more efficient, but data written using this file system is not interoperable with other S3 tools. This limitation may make using this file system less desirable, so the majority of this document focuses on the  s3a  file system. Although the majority of the walkthrough should apply for the  s3  file system also.", 
            "title": "The s3 File System"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#getting-gobblin-to-publish-to-s3", 
            "text": "This section will provide a step by step guide to setting up an EC2 instance, a S3 bucket, installing Gobblin on EC2, and configuring Gobblin to publish data to S3.  This guide will use the free-tier provided by AWS to setup EC2 and S3.", 
            "title": "Getting Gobblin to Publish to S3"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#signing-up-for-aws", 
            "text": "In order to use EC2 and S3, one first needs to sign up for an AWS account. The easiest way to get started with AWS is to use their  free tier .", 
            "title": "Signing Up For AWS"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#setting-up-ec2", 
            "text": "", 
            "title": "Setting Up EC2"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#launching-an-ec2-instance", 
            "text": "Once you have an AWS account, login to the AWS  console . Select the EC2 link, which will bring you to the  EC2 dashboard .  Click on  Launch Instance  to create a new EC2 instance. Before the instance actually starts to run, there area a few more configuration steps necessary:   Choose an Amazon Machine Image ( AMI )  For this walkthrough we will pick Red Hat Enterprise Linux ( RHEL ) AMI    Choose an Instance Type  Since this walkthrough uses the Amazon Free Tier, we will pick the General Purpose  t2.micro  instance  This instance provides us with 1 vCPU and 1 GiB of RAM    For more information on other instance types, check out the AWS  docs    Click Review and Launch  We will use the defaults for all other setting options  When reviewing your instance, you will most likely get a warning saying access to your EC2 instance is open to the world  If you want to fix this you have to edit the  Security Groups ; how to do that is out of the scope of this document    Set Up SSH Keys  After reviewing your instance, click  Launch  You should be prompted to setup  SSH  keys  Use an existing key pair if you have one, otherwise create a new one and download it    SSH to Launched Instance  SSH using the following command:  ssh -i my-private-key-file.pem ec2-user@instance-name  The  instance-name  can be taken from the  Public DNS  field from the instance information  SSH may complain that the private key file has insufficient permissions  Execute  chmod 600 my-private-key-file.pem  to fix this    Alternatively, one can modify the  ~/.ssh/config  file instead of specifying the  -i  option       After following the above steps, you should be able to freely SSH into the launched EC2 instance, and monitor / control the instance from the  EC2 dashboard .", 
            "title": "Launching an EC2 Instance"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#ec2-package-installations", 
            "text": "Before setting up Gobblin, you need to install  Java  first. Depending on the AMI instance you are running Java may or may not already be installed (you can check if Java is already installed by executing  java -version ).", 
            "title": "EC2 Package Installations"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#installing-java", 
            "text": "Execute  sudo yum install java-1.8.0-openjdk*  to install Open JDK 8  Confirm the installation was successful by executing  java -version  Set the  JAVA_HOME  environment variable in the  ~/.bashrc/  file  The value for  JAVA_HOME  can be found by executing  readlink `which java`", 
            "title": "Installing Java"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#setting-up-s3", 
            "text": "Go to the  S3 dashboard   Click on  Create Bucket  Enter a name for the bucket (e.g.  gobblin-demo-bucket )  Enter a  Region  for the bucket (e.g.  US Standard )", 
            "title": "Setting Up S3"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#setting-up-gobblin-on-ec2", 
            "text": "Download and Build Gobblin Locally  On your local machine, clone the  Gobblin repository :  git clone git@github.com:linkedin/gobblin.git  (this assumes you have  Git  installed locally)  Build Gobblin using the following commands (it is important to use Hadoop version 2.6.0 as it includes the  s3a  file system implementation):     cd gobblin\n./gradlew clean build -PuseHadoop2 -PhadoopVersion=2.6.0 -x test   Upload the Gobblin Tar to EC2  Execute the command:      scp -i my-private-key-file.pem gobblin-dist-[project-version].tar.gz ec2-user@instance-name:   Un-tar the Gobblin Distribution  SSH to the EC2 Instance  Un-tar the Gobblin distribution:  tar -xvf gobblin-dist-[project-version].tar.gz    Download AWS Libraries  A few JARs need to be downloaded using some cURL commands:     curl http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar   gobblin-dist/lib/aws-java-sdk-1.7.4.jar\ncurl http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.6.0/hadoop-aws-2.6.0.jar   gobblin-dist/lib/hadoop-aws-2.6.0.jar", 
            "title": "Setting Up Gobblin on EC2"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#configuring-gobblin-on-ec2", 
            "text": "Assuming we are running Gobblin in  standalone mode , the following configuration options need to be modified in the file  gobblin-dist/conf/gobblin-standalone.properties .   Add the key  data.publisher.fs.uri  and set it to  s3a://gobblin-demo-bucket/  This configures the job to publish data to the S3 bucket named  gobblin-demo-bucket    Add the AWS Access Key Id and Secret Access Key  Set the keys  fs.s3a.access.key  and  fs.s3a.secret.key  to the appropriate values  These keys correspond to  AWS security credentials  For information on how to get these credentials, check out the AWS documentation  here  The AWS documentation recommends using  IAM roles ; how to set this up is out of the scope of this document; for this walkthrough we will use root access credentials", 
            "title": "Configuring Gobblin on EC2"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#launching-gobblin-on-ec2", 
            "text": "Assuming we want Gobblin to run in standalone mode, follow the usual steps for  standalone deployment .  For the sake of this walkthrough, we will launch the Gobblin  wikipedia example . Directions on how to run this example can be found  here . The command to launch Gobblin should look similar to:  sh bin/gobblin-standalone.sh start --workdir /home/ec2-user/gobblin-dist/work --logdir /home/ec2-user/gobblin-dist/logs --conf /home/ec2-user/gobblin-dist/config  If you are running on the Amazon free tier, you will probably get an error in the  nohup.out  file saying there is insufficient memory for the JVM. To fix this add  --jvmflags \"-Xms256m -Xmx512m\"  to the  start  command.  Data should be written to S3 during the publishing phase of Gobblin. One can confirm data was successfully written to S3 by looking at the  S3 dashboard .", 
            "title": "Launching Gobblin on EC2"
        }, 
        {
            "location": "/case-studies/Publishing-Data-to-S3/#writing-to-s3-outside-ec2", 
            "text": "It is possible to write to an S3 bucket outside of an EC2 instance. The setup steps are similar to walkthrough outlined above. For more information on writing to S3 outside of AWS, check out  this article .", 
            "title": "Writing to S3 Outside EC2"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nQuick Start\n\n\nMetric Contexts\n\n\nMetrics\n\n\nEvents\n\n\nReporters\n\n\n\n\n\n\nGobblin Metrics is a metrics library for emitting metrics and events instrumenting java applications. \nMetrics and events are easy to use and enriched with tags. Metrics allow full granularity, auto-aggregation, and configurable \nreporting schedules. Gobblin Metrics is based on \nDropwizard Metrics\n, enhanced to better support \nmodular applications (by providing hierarchical, auto-aggregated metrics) and their monitoring / auditing.\n\n\nQuick Start\n\n\nThe following code excerpt shows the functionality of Gobblin Metrics.\n\n\n// ========================================\n// METRIC CONTEXTS\n// ========================================\n\n// Create a Metric context with a Tag\nMetricContext context = MetricContext.builder(\nMyMetricContext\n).addTag(new Tag\nInteger\n(\nkey\n, value)).build();\n// Create a child metric context. It will automatically inherit tags from parent.\n// All metrics in the child context will be auto-aggregated in the parent context.\nMetricContext childContext = context.childBuilder(\nchildContext\n).build();\n\n// ========================================\n// METRICS\n// ========================================\n\n// Create a reporter for metrics. This reporter will write metrics to STDOUT.\nOutputStreamReporter.Factory.newBuilder().build(new Properties());\n// Start all metric reporters.\nRootMetricContext.get().startReporting();\n\n// Create a counter.\nCounter counter = childContext.counter(\nmy.counter.name\n);\n// Increase the counter. The next time metrics are reported, \nmy.counter.name\n will be reported as 1.\ncounter.inc();\n\n// ========================================\n// EVENTS\n// ========================================\n\n// Create an reporter for events. This reporter will write events to STDOUT.\nScheduledReporter eventReporter = OutputStreamEventReporter.forContext(context).build();\neventReporter.start();\n\n// Create an event submitter, can include default metadata.\nEventSubmitter eventSubmitter = new EventSubmitter.Builder(context, \nevents.namespace\n).addMetadata(\nmetadataKey\n, \nvalue\n).build();\n// Submit an event. Its metadata will contain all tags in context, all metadata in eventSubmitter,\n// and any additional metadata specified in the call.\n// This event will be displayed the next time the event reporter flushes.\neventSubmitter.submit(\nEventName\n, \nadditionalMetadataKey\n, \nvalue\n);\n\n\n\n\nMetric Contexts\n\n\nA metric context is a context from which users can emit metrics and events. These contexts contain a set of tags, each tag \nbeing a key-value pair. Contexts are hierarchical in nature: each context has one parent and children. They automatically \ninherit the tags of their parent, and can define or override more tags.\n\n\nGenerally, a metric context is associated with a specific instance of an object that should be instrumented. \nDifferent instances of the same object will have separate instrumentations. However, each context also aggregates \nall metrics defined by its descendants, providing with a full range of granularities for reporting. \nWith this functionality if, for example, an application has 10 different data writers,  users can monitor each writer \nindividually, or all at the same time.\n\n\nMetrics\n\n\nMetrics are used to monitor the progress of an application. Metrics are emitted regularly following a schedule and represent \nthe current state of the application. The metrics supported by Gobblin Metrics are the same ones as those supported \nby \nDropwizard Metrics Core\n, adapted for tagging and auto-aggregation. \nThe types supported are:\n\n\n\n\nCounter: simple long counter.\n\n\nMeter: counter with added computation of the rate at which the counter is changing.\n\n\nHistogram: stores a histogram of a value, divides all of the values observed into buckets, and reports the count for each bucket.\n\n\nTimer: a histogram for timing information.\n\n\nGauge: simply stores a value. Gauges are not auto-aggregated because the aggregation operation is context-dependent.\n\n\n\n\nEvents\n\n\nEvents are fire-and-forget messages indicating a milestone in the execution of an application, \nalong with metadata that can provide further information about that event (all tags of the metric context used to generate \nthe event are also added as metadata).\n\n\nReporters\n\n\nReporters periodically output the metrics and events to particular sinks following a configurable schedule. Events and Metrics reporters are kept separate to allow users more control in case they want to emit metrics and events to separate sinks (for example, different files). Reporters for a few sinks are implemented by default, but additional sinks can be implemented by extending the \nRecursiveScheduledMetricReporter\n and the \nEventReporter\n. Each of the included reporters has a simple builder.\n\n\nThe metric reporter implementations included with Gobblin Metrics are:\n\n\n\n\nOutputStreamReporter: Supports any output stream, including STDOUT and files.\n\n\nKafkaReporter: Emits metrics to a Kafka topic as Json messages.\n\n\nKafkaAvroReporter: Emits metrics to a Kafka topic as Avro messages.\n\n\nInfluxDBReporter: Emits metrics to Influx DB.\n\n\nGraphiteReporter: Emits metrics to Graphite.\n\n\nHadoopCounterReporter: Emits metrics as Hadoop counters.\n\n\n\n\nThe event reporter implementations included with Gobblin metrics are:\n\n\n\n\nOutputStreamEventReporter: Supports any output stream, including STDOUT and files.\n\n\nKafkaEventReporter: Emits events to Kafka as Json messages.\n\n\nKafkaEventAvroReporter: Emits events to Kafka as Avro messages.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics/#table-of-contents", 
            "text": "Table of Contents  Quick Start  Metric Contexts  Metrics  Events  Reporters    Gobblin Metrics is a metrics library for emitting metrics and events instrumenting java applications. \nMetrics and events are easy to use and enriched with tags. Metrics allow full granularity, auto-aggregation, and configurable \nreporting schedules. Gobblin Metrics is based on  Dropwizard Metrics , enhanced to better support \nmodular applications (by providing hierarchical, auto-aggregated metrics) and their monitoring / auditing.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics/#quick-start", 
            "text": "The following code excerpt shows the functionality of Gobblin Metrics.  // ========================================\n// METRIC CONTEXTS\n// ========================================\n\n// Create a Metric context with a Tag\nMetricContext context = MetricContext.builder( MyMetricContext ).addTag(new Tag Integer ( key , value)).build();\n// Create a child metric context. It will automatically inherit tags from parent.\n// All metrics in the child context will be auto-aggregated in the parent context.\nMetricContext childContext = context.childBuilder( childContext ).build();\n\n// ========================================\n// METRICS\n// ========================================\n\n// Create a reporter for metrics. This reporter will write metrics to STDOUT.\nOutputStreamReporter.Factory.newBuilder().build(new Properties());\n// Start all metric reporters.\nRootMetricContext.get().startReporting();\n\n// Create a counter.\nCounter counter = childContext.counter( my.counter.name );\n// Increase the counter. The next time metrics are reported,  my.counter.name  will be reported as 1.\ncounter.inc();\n\n// ========================================\n// EVENTS\n// ========================================\n\n// Create an reporter for events. This reporter will write events to STDOUT.\nScheduledReporter eventReporter = OutputStreamEventReporter.forContext(context).build();\neventReporter.start();\n\n// Create an event submitter, can include default metadata.\nEventSubmitter eventSubmitter = new EventSubmitter.Builder(context,  events.namespace ).addMetadata( metadataKey ,  value ).build();\n// Submit an event. Its metadata will contain all tags in context, all metadata in eventSubmitter,\n// and any additional metadata specified in the call.\n// This event will be displayed the next time the event reporter flushes.\neventSubmitter.submit( EventName ,  additionalMetadataKey ,  value );", 
            "title": "Quick Start"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics/#metric-contexts", 
            "text": "A metric context is a context from which users can emit metrics and events. These contexts contain a set of tags, each tag \nbeing a key-value pair. Contexts are hierarchical in nature: each context has one parent and children. They automatically \ninherit the tags of their parent, and can define or override more tags.  Generally, a metric context is associated with a specific instance of an object that should be instrumented. \nDifferent instances of the same object will have separate instrumentations. However, each context also aggregates \nall metrics defined by its descendants, providing with a full range of granularities for reporting. \nWith this functionality if, for example, an application has 10 different data writers,  users can monitor each writer \nindividually, or all at the same time.", 
            "title": "Metric Contexts"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics/#metrics", 
            "text": "Metrics are used to monitor the progress of an application. Metrics are emitted regularly following a schedule and represent \nthe current state of the application. The metrics supported by Gobblin Metrics are the same ones as those supported \nby  Dropwizard Metrics Core , adapted for tagging and auto-aggregation. \nThe types supported are:   Counter: simple long counter.  Meter: counter with added computation of the rate at which the counter is changing.  Histogram: stores a histogram of a value, divides all of the values observed into buckets, and reports the count for each bucket.  Timer: a histogram for timing information.  Gauge: simply stores a value. Gauges are not auto-aggregated because the aggregation operation is context-dependent.", 
            "title": "Metrics"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics/#events", 
            "text": "Events are fire-and-forget messages indicating a milestone in the execution of an application, \nalong with metadata that can provide further information about that event (all tags of the metric context used to generate \nthe event are also added as metadata).", 
            "title": "Events"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics/#reporters", 
            "text": "Reporters periodically output the metrics and events to particular sinks following a configurable schedule. Events and Metrics reporters are kept separate to allow users more control in case they want to emit metrics and events to separate sinks (for example, different files). Reporters for a few sinks are implemented by default, but additional sinks can be implemented by extending the  RecursiveScheduledMetricReporter  and the  EventReporter . Each of the included reporters has a simple builder.  The metric reporter implementations included with Gobblin Metrics are:   OutputStreamReporter: Supports any output stream, including STDOUT and files.  KafkaReporter: Emits metrics to a Kafka topic as Json messages.  KafkaAvroReporter: Emits metrics to a Kafka topic as Avro messages.  InfluxDBReporter: Emits metrics to Influx DB.  GraphiteReporter: Emits metrics to Graphite.  HadoopCounterReporter: Emits metrics as Hadoop counters.   The event reporter implementations included with Gobblin metrics are:   OutputStreamEventReporter: Supports any output stream, including STDOUT and files.  KafkaEventReporter: Emits events to Kafka as Json messages.  KafkaEventAvroReporter: Emits events to Kafka as Avro messages.", 
            "title": "Reporters"
        }, 
        {
            "location": "/metrics/Existing-Reporters/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nMetric Reporters\n\n\nEvent Reporters\n\n\n\n\n\n\nMetric Reporters\n\n\n\n\nOutput Stream Reporter\n: allows printing metrics to any OutputStream, including STDOUT and files.\n\n\nKafka Reporter\n: emits metrics to Kafka topic as Json messages.\n\n\nKafka Avro Reporter\n: emits metrics to Kafka topic as Avro messages with schema \nMetricReport\n.\n\n\nGraphite Reporter\n: emits metrics to Graphite. This reporter has a different, deprecated construction API included in its javadoc.\n\n\nInflux DB Reporter\n: emits metrics to Influx DB. This reporter has a different, deprecated construction API included in its javadoc.\n\n\nHadoop Counter Reporter\n: emits metrics as Hadoop counters at the end of the execution. Available for old and new Hadoop API. This reporter has a different, deprecated construction API included in its javadoc. Due to limits on the number of Hadoop counters that can be created, this reporter is not recommended except for applications with very few metrics.\n\n\n\n\nEvent Reporters\n\n\n\n\nOutput Stream Event Reporter\n: Emits events to any output stream, including STDOUT and files.\n\n\nKafka Event Reporter\n: Emits events to Kafka topic as Json messages.\n\n\nKafka Avro Event Reporter\n: Emits events to Kafka topic as Avro messages using the schema \nGobblinTrackingEvent\n.", 
            "title": "Existing Reporters"
        }, 
        {
            "location": "/metrics/Existing-Reporters/#table-of-contents", 
            "text": "Table of Contents  Metric Reporters  Event Reporters", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/metrics/Existing-Reporters/#metric-reporters", 
            "text": "Output Stream Reporter : allows printing metrics to any OutputStream, including STDOUT and files.  Kafka Reporter : emits metrics to Kafka topic as Json messages.  Kafka Avro Reporter : emits metrics to Kafka topic as Avro messages with schema  MetricReport .  Graphite Reporter : emits metrics to Graphite. This reporter has a different, deprecated construction API included in its javadoc.  Influx DB Reporter : emits metrics to Influx DB. This reporter has a different, deprecated construction API included in its javadoc.  Hadoop Counter Reporter : emits metrics as Hadoop counters at the end of the execution. Available for old and new Hadoop API. This reporter has a different, deprecated construction API included in its javadoc. Due to limits on the number of Hadoop counters that can be created, this reporter is not recommended except for applications with very few metrics.", 
            "title": "Metric Reporters"
        }, 
        {
            "location": "/metrics/Existing-Reporters/#event-reporters", 
            "text": "Output Stream Event Reporter : Emits events to any output stream, including STDOUT and files.  Kafka Event Reporter : Emits events to Kafka topic as Json messages.  Kafka Avro Event Reporter : Emits events to Kafka topic as Avro messages using the schema  GobblinTrackingEvent .", 
            "title": "Event Reporters"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nConfiguring Metrics and Event emission\n\n\nOperational Metrics\n\n\nExtractor Metrics\n\n\nConverter Metrics\n\n\nFork Operator Metrics\n\n\nRow Level Policy Metrics\n\n\nData Writer Metrics\n\n\n\n\n\n\nRuntime Events\n\n\nJob Progression Events\n\n\nJob Timing Events\n\n\n\n\n\n\nCustomizing Instrumentation\n\n\nCustom constructs\n\n\nInstrumentable Interface\n\n\nCallback Methods\n\n\n\n\n\n\nCustom Reporters\n\n\n\n\n\n\n\n\n\n\nGobblin ETL comes equipped with instrumentation using \nGobblin Metrics\n, as well as end points to easily extend this instrumentation.\n\n\nConfiguring Metrics and Event emission\n\n\nThe following configurations are used for metrics and event emission:\n\n\n\n\n\n\n\n\nConfiguration Key\n\n\nDefinition\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\nmetrics.enabled\n\n\nWhether metrics are enabled. If false, will not report metrics.\n\n\ntrue\n\n\n\n\n\n\nmetrics.report.interval\n\n\nMetrics report interval in milliseconds.\n\n\n30000\n\n\n\n\n\n\nmetrics.reporting.file.enabled\n\n\nWhether metrics will be reported to a file.\n\n\nfalse\n\n\n\n\n\n\nmetrics.log.dir\n\n\nIf file enabled, the directory where metrics will be written. If missing, will not report to file.\n\n\nN/A\n\n\n\n\n\n\nmetrics.reporting.kafka.enabled\n\n\nWhether metrics will be reported to Kafka.\n\n\nfalse\n\n\n\n\n\n\nmetrics.reporting.kafka.brokers\n\n\nKafka brokers for Kafka metrics emission.\n\n\nN/A\n\n\n\n\n\n\nmetrics.reporting.kafka.topic.metrics\n\n\nKafka topic where metrics (but not events) will be reported.\n\n\nN/A\n\n\n\n\n\n\nmetrics.reporting.kafka.topic.events\n\n\nKafka topic where events (but not metrics) will be reported.\n\n\nN/A\n\n\n\n\n\n\nmetrics.reporting.kafka.format\n\n\nFormat of metrics / events emitted to Kafka. (Options: json, avro)\n\n\njson\n\n\n\n\n\n\nmetrics.reporting.kafka.avro.use.schema.registry\n\n\nWhether to use a schema registry for Kafka emitting.\n\n\nfalse\n\n\n\n\n\n\nkafka.schema.registry.url\n\n\nIf using schema registry, the url of the schema registry.\n\n\nN/A\n\n\n\n\n\n\nmetrics.reporting.jmx.enabled\n\n\nWhether to report metrics to JMX.\n\n\nfalse\n\n\n\n\n\n\nmetrics.reporting.custom.builders\n\n\nComma-separated list of classes for custom metrics reporters. (See \nCustom Reporters\n)\n\n\n\n\n\n\n\n\n\n\nOperational Metrics\n\n\nEach construct in a Gobblin ETL run computes metrics regarding it's performance / progress. Each metric is tagged by default with the following tags:\n\n\n\n\njobName: Gobblin generated name for the job.\n\n\njobId: Gobblin generated id for the job.\n\n\nclusterIdentifier: string identifier the cluster / host where the job was run. Obtained from resource manager, job tracker, or the name of the host.\n\n\ntaskId: Gobblin generated id for the task that generated the metric.\n\n\nconstruct: construct type that generated the metric (e.g. extractor, converter, etc.)\n\n\nclass: specific class of the construct that generated the metric.\n\n\nfinalMetricReport: metrics are emitted regularly. Sometimes it is useful to select only the last report from each context. To aid with this, some reporters will add this tag with value \"true\" only to the final report from a metric context.\n\n\n\n\nThis is the list of operational metrics implemented by default, grouped by construct.\n\n\nExtractor Metrics\n\n\n\n\ngobblin.extractor.records.read: meter for records read.\n\n\ngobblin.extractor.records.failed: meter for records failed to read.\n\n\ngobblin.extractor.extract.time: timer for reading of records.\n\n\n\n\nConverter Metrics\n\n\n\n\ngobblin.converter.records.in: meter for records going into the converter.\n\n\ngobblin.converter.records.out: meter for records outputted by the converter.\n\n\ngobblin.converter.records.failed: meter for records that failed to be converted.\n\n\ngobblin.converter.convert.time: timer for conversion time of each record.\n\n\n\n\nFork Operator Metrics\n\n\n\n\ngobblin.fork.operator.records.in: meter for records going into the fork operator.\n\n\ngobblin.fork.operator.forks.out: meter for records going out of the fork operator (each record is counted once for each fork it is emitted to).\n\n\ngobblin.fork.operator.fork.time: timer for forking of each record.\n\n\n\n\nRow Level Policy Metrics\n\n\n\n\ngobblin.qualitychecker.records.in: meter for records going into the row level policy.\n\n\ngobblin.qualitychecker.records.passed: meter for records passing the row level policy check.\n\n\ngobblin.qualitychecker.records.failed: meter for records failing the row level policy check.\n\n\ngobblin.qualitychecker.check.time: timer for row level policy checking of each record.\n\n\n\n\nData Writer Metrics\n\n\n\n\ngobblin.writer.records.in: meter for records requested to be written.\n\n\ngobblin.writer.records.written: meter for records actually written.\n\n\ngobblin.writer.records.failed: meter for records failed to be written.\n\n\ngobblin.writer.write.time: timer for writing each record.\n\n\n\n\nRuntime Events\n\n\nThe Gobblin ETL runtime emits events marking its progress. All events have the following metadata:\n\n\n\n\njobName: Gobblin generated name for the job.\n\n\njobId: Gobblin generated id for the job.\n\n\nclusterIdentifier: string identifier the cluster / host where the job was run. Obtained from resource manager, job tracker, or the name of the host.\n\n\ntaskId: Gobblin generated id for the task that generated the metric (if applicable).\n\n\n\n\nThis is the list of events that are emitted by the Gobblin runtime:\n\n\nJob Progression Events\n\n\n\n\nLockInUse: emitted if a job fails because it fails to get a lock.\n\n\nWorkUnitsMissing: emitted if a job exits because source failed to get work units.\n\n\nWorkUnitsEmpty: emitted if a job exits because there were no work units to process.\n\n\nTasksSubmitted: emitted when tasks are submitted for execution. Metadata: tasksCount(number of tasks submitted).\n\n\nTaskFailed: emitted when a task fails. Metadata: taskId(id of the failed task).\n\n\nJob_Successful: emitted at the end of a successful job.\n\n\nJob_Failed: emitted at the end of a failed job.\n\n\n\n\nJob Timing Events\n\n\nThese events give information on timing on certain parts of the execution. Each timing event contains the following metadata:\n\n\n\n\nstartTime: timestamp when the timed processing started.\n\n\nendTime: timestamp when the timed processing finished.\n\n\ndurationMillis: duration in milliseconds of the timed processing.\n\n\neventType: always \"timingEvent\" for timing events.\n\n\n\n\nThe following timing events are emitted:\n\n\n\n\nFullJobExecutionTimer: times the entire job execution.\n\n\nWorkUnitsCreationTimer: times the creation of work units.\n\n\nWorkUnitsPreparationTime: times the preparation of work units.\n\n\nJobRunTimer: times the actual running of job (i.e. processing of all work units).\n\n\nJobCommitTimer: times the committing of work units.\n\n\nJobCleanupTimer: times the job cleanup.\n\n\nJobLocalSetupTimer: times the setup of a local job.\n\n\nJobMrStagingDataCleanTimer: times the deletion of staging directories from previous work units (MR mode).\n\n\nJobMrDistributedCacheSetupTimer: times the setting up of distributed cache (MR mode).\n\n\nJobMrSetupTimer: times the setup of the MR job (MR mode).\n\n\nJobMrRunTimer: times the execution of the MR job (MR mode).\n\n\n\n\nCustomizing Instrumentation\n\n\nCustom constructs\n\n\nWhen using a custom construct (for example a custom extractor for your data source), you will get the above mentioned instrumentation for free. However, you may want to implement additional metrics. To aid with this, instead of extending the usual class Extractor, you can extend the class \ngobblin.instrumented.extractor.InstrumentedExtractor\n. Similarly, for each construct there is an instrumented version that allows extension of the default metrics (\nInstrumentedExtractor\n, \nInstrumentedConverter\n, \nInstrumentedForkOperator\n, \nInstrumentedRowLevelPolicy\n, and \nInstrumentedDataWriter\n).\n\n\nAll of the instrumented constructs have Javadoc providing with additional information. In general, when extending an instrumented construct, you will have to implement a different method. For example, when extending an InstrumentedExtractor, instead of implementing \nreadRecord\n, you will implement \nreadRecordImpl\n. To make this clearer for the user, implementing \nreadRecord\n will throw a compilation error, and the javadoc of each method specifies the method that should be implemented.\n\n\nInstrumentable Interface\n\n\nInstrumented constructs extend the interface \nInstrumentable\n. It contains the following methods:\n\n\n\n\ngetMetricContext()\n: get the default metric context generated for that instance of the construct, with all the appropriate tags. Use this metric context to create any additional metrics.\n\n\nisInstrumentationEnabled()\n: returns true if instrumentation is enabled.\n\n\nswitchMetricsContext(List\nTag\n?\n)\n: switches the default metric context returned by \ngetMetricContext()\n to a metric context containing the supplied tags. All default metrics will be reported to the new metric context. This method is useful when the state of a construct changes during the execution, and the user desires to reflect that in the emitted tags (for example, Kafka extractor can handle multiple topics in the same extractor, and we want to reflect this in the metrics).\n\n\nswitchMetricContext(MetricContext)\n: similar to the above method, but uses the supplied metric context instead of generating a new metric context. It is the responsibility of the caller to ensure the new metric context has the correct tags and parent.\n\n\n\n\nThe following method can be re-implemented by the user:\n* \ngenerateTags(State)\n: this method should return a list of tags to use for metric contexts created for this construct. If overriding this method, it is always a good idea to call \nsuper()\n and only append tags to this list.\n\n\nCallback Methods\n\n\nInstrumented constructs have a set of callback methods that are called at different points in the processing of each record, and which can be used to update metrics. For example, the \nInstrumentedExtractor\n has the callbacks \nbeforeRead()\n, \nafterRead(D, long)\n, and \nonException(Exception)\n. The javadoc for the instrumented constructs has further descriptions for each callback. Users should always call \nsuper()\n when overriding this callbacks, as default metrics depend on that.\n\n\nCustom Reporters\n\n\nBesides the reporters implemented by default (file, Kafka, and JMX), users can add custom reporters to the classpath and instruct Gobblin to use these reporters. To do this, users should extend the interface \nCustomReporterFactory\n, and specify a comma-separated list of CustomReporterFactory classes in the configuration key \nmetrics.reporting.custom.builders\n.\n\n\nGobblin will automatically search for these CustomReporterFactory implementations, instantiate each one with a parameter-less constructor, and then call the method \nnewScheduledReporter(MetricContext, Properties)\n, where the properties contain all of the input configurations supplied to Gobblin. Gobblin will then manage this \nScheduledReporter\n.", 
            "title": "Metrics for Gobblin ETL"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#table-of-contents", 
            "text": "Table of Contents  Configuring Metrics and Event emission  Operational Metrics  Extractor Metrics  Converter Metrics  Fork Operator Metrics  Row Level Policy Metrics  Data Writer Metrics    Runtime Events  Job Progression Events  Job Timing Events    Customizing Instrumentation  Custom constructs  Instrumentable Interface  Callback Methods    Custom Reporters      Gobblin ETL comes equipped with instrumentation using  Gobblin Metrics , as well as end points to easily extend this instrumentation.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#configuring-metrics-and-event-emission", 
            "text": "The following configurations are used for metrics and event emission:     Configuration Key  Definition  Default      metrics.enabled  Whether metrics are enabled. If false, will not report metrics.  true    metrics.report.interval  Metrics report interval in milliseconds.  30000    metrics.reporting.file.enabled  Whether metrics will be reported to a file.  false    metrics.log.dir  If file enabled, the directory where metrics will be written. If missing, will not report to file.  N/A    metrics.reporting.kafka.enabled  Whether metrics will be reported to Kafka.  false    metrics.reporting.kafka.brokers  Kafka brokers for Kafka metrics emission.  N/A    metrics.reporting.kafka.topic.metrics  Kafka topic where metrics (but not events) will be reported.  N/A    metrics.reporting.kafka.topic.events  Kafka topic where events (but not metrics) will be reported.  N/A    metrics.reporting.kafka.format  Format of metrics / events emitted to Kafka. (Options: json, avro)  json    metrics.reporting.kafka.avro.use.schema.registry  Whether to use a schema registry for Kafka emitting.  false    kafka.schema.registry.url  If using schema registry, the url of the schema registry.  N/A    metrics.reporting.jmx.enabled  Whether to report metrics to JMX.  false    metrics.reporting.custom.builders  Comma-separated list of classes for custom metrics reporters. (See  Custom Reporters )", 
            "title": "Configuring Metrics and Event emission"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#operational-metrics", 
            "text": "Each construct in a Gobblin ETL run computes metrics regarding it's performance / progress. Each metric is tagged by default with the following tags:   jobName: Gobblin generated name for the job.  jobId: Gobblin generated id for the job.  clusterIdentifier: string identifier the cluster / host where the job was run. Obtained from resource manager, job tracker, or the name of the host.  taskId: Gobblin generated id for the task that generated the metric.  construct: construct type that generated the metric (e.g. extractor, converter, etc.)  class: specific class of the construct that generated the metric.  finalMetricReport: metrics are emitted regularly. Sometimes it is useful to select only the last report from each context. To aid with this, some reporters will add this tag with value \"true\" only to the final report from a metric context.   This is the list of operational metrics implemented by default, grouped by construct.", 
            "title": "Operational Metrics"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#extractor-metrics", 
            "text": "gobblin.extractor.records.read: meter for records read.  gobblin.extractor.records.failed: meter for records failed to read.  gobblin.extractor.extract.time: timer for reading of records.", 
            "title": "Extractor Metrics"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#converter-metrics", 
            "text": "gobblin.converter.records.in: meter for records going into the converter.  gobblin.converter.records.out: meter for records outputted by the converter.  gobblin.converter.records.failed: meter for records that failed to be converted.  gobblin.converter.convert.time: timer for conversion time of each record.", 
            "title": "Converter Metrics"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#fork-operator-metrics", 
            "text": "gobblin.fork.operator.records.in: meter for records going into the fork operator.  gobblin.fork.operator.forks.out: meter for records going out of the fork operator (each record is counted once for each fork it is emitted to).  gobblin.fork.operator.fork.time: timer for forking of each record.", 
            "title": "Fork Operator Metrics"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#row-level-policy-metrics", 
            "text": "gobblin.qualitychecker.records.in: meter for records going into the row level policy.  gobblin.qualitychecker.records.passed: meter for records passing the row level policy check.  gobblin.qualitychecker.records.failed: meter for records failing the row level policy check.  gobblin.qualitychecker.check.time: timer for row level policy checking of each record.", 
            "title": "Row Level Policy Metrics"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#data-writer-metrics", 
            "text": "gobblin.writer.records.in: meter for records requested to be written.  gobblin.writer.records.written: meter for records actually written.  gobblin.writer.records.failed: meter for records failed to be written.  gobblin.writer.write.time: timer for writing each record.", 
            "title": "Data Writer Metrics"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#runtime-events", 
            "text": "The Gobblin ETL runtime emits events marking its progress. All events have the following metadata:   jobName: Gobblin generated name for the job.  jobId: Gobblin generated id for the job.  clusterIdentifier: string identifier the cluster / host where the job was run. Obtained from resource manager, job tracker, or the name of the host.  taskId: Gobblin generated id for the task that generated the metric (if applicable).   This is the list of events that are emitted by the Gobblin runtime:", 
            "title": "Runtime Events"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#job-progression-events", 
            "text": "LockInUse: emitted if a job fails because it fails to get a lock.  WorkUnitsMissing: emitted if a job exits because source failed to get work units.  WorkUnitsEmpty: emitted if a job exits because there were no work units to process.  TasksSubmitted: emitted when tasks are submitted for execution. Metadata: tasksCount(number of tasks submitted).  TaskFailed: emitted when a task fails. Metadata: taskId(id of the failed task).  Job_Successful: emitted at the end of a successful job.  Job_Failed: emitted at the end of a failed job.", 
            "title": "Job Progression Events"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#job-timing-events", 
            "text": "These events give information on timing on certain parts of the execution. Each timing event contains the following metadata:   startTime: timestamp when the timed processing started.  endTime: timestamp when the timed processing finished.  durationMillis: duration in milliseconds of the timed processing.  eventType: always \"timingEvent\" for timing events.   The following timing events are emitted:   FullJobExecutionTimer: times the entire job execution.  WorkUnitsCreationTimer: times the creation of work units.  WorkUnitsPreparationTime: times the preparation of work units.  JobRunTimer: times the actual running of job (i.e. processing of all work units).  JobCommitTimer: times the committing of work units.  JobCleanupTimer: times the job cleanup.  JobLocalSetupTimer: times the setup of a local job.  JobMrStagingDataCleanTimer: times the deletion of staging directories from previous work units (MR mode).  JobMrDistributedCacheSetupTimer: times the setting up of distributed cache (MR mode).  JobMrSetupTimer: times the setup of the MR job (MR mode).  JobMrRunTimer: times the execution of the MR job (MR mode).", 
            "title": "Job Timing Events"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#customizing-instrumentation", 
            "text": "", 
            "title": "Customizing Instrumentation"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#custom-constructs", 
            "text": "When using a custom construct (for example a custom extractor for your data source), you will get the above mentioned instrumentation for free. However, you may want to implement additional metrics. To aid with this, instead of extending the usual class Extractor, you can extend the class  gobblin.instrumented.extractor.InstrumentedExtractor . Similarly, for each construct there is an instrumented version that allows extension of the default metrics ( InstrumentedExtractor ,  InstrumentedConverter ,  InstrumentedForkOperator ,  InstrumentedRowLevelPolicy , and  InstrumentedDataWriter ).  All of the instrumented constructs have Javadoc providing with additional information. In general, when extending an instrumented construct, you will have to implement a different method. For example, when extending an InstrumentedExtractor, instead of implementing  readRecord , you will implement  readRecordImpl . To make this clearer for the user, implementing  readRecord  will throw a compilation error, and the javadoc of each method specifies the method that should be implemented.", 
            "title": "Custom constructs"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#instrumentable-interface", 
            "text": "Instrumented constructs extend the interface  Instrumentable . It contains the following methods:   getMetricContext() : get the default metric context generated for that instance of the construct, with all the appropriate tags. Use this metric context to create any additional metrics.  isInstrumentationEnabled() : returns true if instrumentation is enabled.  switchMetricsContext(List Tag ? ) : switches the default metric context returned by  getMetricContext()  to a metric context containing the supplied tags. All default metrics will be reported to the new metric context. This method is useful when the state of a construct changes during the execution, and the user desires to reflect that in the emitted tags (for example, Kafka extractor can handle multiple topics in the same extractor, and we want to reflect this in the metrics).  switchMetricContext(MetricContext) : similar to the above method, but uses the supplied metric context instead of generating a new metric context. It is the responsibility of the caller to ensure the new metric context has the correct tags and parent.   The following method can be re-implemented by the user:\n*  generateTags(State) : this method should return a list of tags to use for metric contexts created for this construct. If overriding this method, it is always a good idea to call  super()  and only append tags to this list.", 
            "title": "Instrumentable Interface"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#callback-methods", 
            "text": "Instrumented constructs have a set of callback methods that are called at different points in the processing of each record, and which can be used to update metrics. For example, the  InstrumentedExtractor  has the callbacks  beforeRead() ,  afterRead(D, long) , and  onException(Exception) . The javadoc for the instrumented constructs has further descriptions for each callback. Users should always call  super()  when overriding this callbacks, as default metrics depend on that.", 
            "title": "Callback Methods"
        }, 
        {
            "location": "/metrics/Metrics-for-Gobblin-ETL/#custom-reporters", 
            "text": "Besides the reporters implemented by default (file, Kafka, and JMX), users can add custom reporters to the classpath and instruct Gobblin to use these reporters. To do this, users should extend the interface  CustomReporterFactory , and specify a comma-separated list of CustomReporterFactory classes in the configuration key  metrics.reporting.custom.builders .  Gobblin will automatically search for these CustomReporterFactory implementations, instantiate each one with a parameter-less constructor, and then call the method  newScheduledReporter(MetricContext, Properties) , where the properties contain all of the input configurations supplied to Gobblin. Gobblin will then manage this  ScheduledReporter .", 
            "title": "Custom Reporters"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nMetric Context\n\n\nMetrics\n\n\nEvents\n\n\nEvent Submitter\n\n\n\n\n\n\nReporters\n\n\nRecursiveScheduleMetricReporter\n\n\nEventReporter\n\n\n\n\n\n\n\n\n\n\n\n\nMetric Context\n\n\nMetric contexts are organized hierarchically in a tree. Each metric context has a set of Tags, each of which is just key-value pair. The keys of all tags are strings, while the values are allowed to be of any type. However, most reporters will serialize the tag values using their \ntoString()\n method.\n\n\nChildren contexts automatically inherit the tags of their parent context, and can add more tags, or override tags present in the parent. Tags can only be defined during construction of each metric context, and are immutable afterwards. This simplifies the inheritance and overriding of metrics. \n\n\nMetric Contexts are created using \nMetricContext.Builder\n, which allows adding tags and specifying the parent. This is the only time tags can be added to the context. When building, the tags of the parent and the new tags are merged to obtain the final tags for this context. When building a child context for Metric Context \ncontext\n, calling \ncontext.childBuilder(String)\n generates a Builder with the correct parent.\n\n\nEach metric context contains the following instance variables:\n\n\n\n\nA \nString\n \nname\n. The name is not used by the core metrics engine, but can be accessed by users to identify the context.\n\n\nA reference to the parent metric context, or null if it has no parent.\n\n\nA list of children metric context references, stored as soft references.\n\n\nAn object of type \nTagged\n containing the tags for this metric context.\n\n\nA \nSet\n of notification targets. Notification targets are objects of type \nFunction\nNotification\n, Void\n which are all called every time there is a new notification. Notifications can be submitted to the Metric Context using the method \nsendNotification(Notification)\n. Notification targets can be added using \naddNotificationTarget(Function\nNotification, Void\n)\n.\n\n\nA lazily instantiated \nExecutorService\n used for asynchronously executing the notification targets. The executor service will only be started the first time there is a notification and the number of notification targets is positive.\n\n\nA \nConcurrentMap\n from metric names to \nMetric\n for all metrics registered in this Metric Context. Metrics can be added to this map using the \nregister(Metric)\n, \nregister(String, Metric)\n, or \nregisterAll(MetricSet)\n, although it is recommended to instead use the methods to create and register the metrics. Metric Context implements getter methods for all metrics, as well as for each type of metric individually (\ngetMetrics\n, \ngetGauges\n, \ngetCounters\n, \ngetHistograms\n, \ngetMeters\n, \ngetTimers\n).\n\n\n\n\nMetrics\n\n\nAll metrics extend the interface \nContextAwareMetric\n. Each metric type in Dropwizard Metrics is extended to a Context Aware type: \nContextAwareCounter\n, \nContextAwareGauge\n, \nContextAwareHistogram\n, \nContextAwareMeter\n, \nContextAwareTimer\n.\n\n\nContext Aware metrics all always created from the Metric Context where they will be registered. For example, to get a counter under Metric Context \ncontext\n, the user would call \ncontext.counter(\"counter.name\")\n. This method first checks all registered metrics in the Metric Context to find a counter with that name, if it succeeds, it simply returns that counter. If a counter with that name has not been registered in \ncontext\n, then a new \nContextAwareCounter\n is created and registered in \ncontext\n.\n\n\nOn creation, each Context Aware metric (except Gauges) checks if its parent Metric Context has parents itself. If so, then it automatically creates a metric of the same type, with the same name, in that parent. This will be repeated recursively until, at the end, all ancestor Metric Contexts will all contain a context aware metric of the same type and with the same name. Every time the context aware metric is updated, the metric will automatically call the same update method, with the same update value, for its parent metric. Again, this will continue recursively until the corresponding metrics in all ancestor metric contexts are updated by the same value. If multiple children of a metric context \ncontext\n all have metrics with the same name, when either of them is updated, the corresponding metric in \ncontext\n will also get updated. In this way, the corresponding metric in \ncontext\n will aggregate all updated to the metrics in the children context.\n\n\nUsers can also register objects of type \ncom.codahale.metrics.Metric\n with any Metric Context, but they will not be auto-aggregated.\n\n\nEvents\n\n\nEvents are objects of type \nGobblinTrackingEvent\n, which is a type generated from an Avro schema. Events have:\n\n\n\n\nA \nnamespace\n.\n\n\nA \nname\n.\n\n\nA \ntimestamp\n.\n\n\nA \nMap\nString,String\n of \nmetadata\n.\n\n\n\n\nEvents are submitted using the \nMetricContext#submitEvent(GobblinTrackingEvent)\n method. When called, this method packages the event into an \nEventNotification\n and submits it to the metric context using the method \nMetricContext#sendNotification(Notification)\n. This notification is passed to all metrics context ancestors. Each notification target of each ancestor metric context will receive the EventNotification. Events are not stored by any Metric Context, so the notification targets need to handle these events appropriately.\n\n\nEvents can be created manually using Avro constructors, and using the method \ncontext.submitEvent(GobblinTrackinEvent)\n, but this is unfriendly when trying to build events incrementally, especially when using metadata. To address this, users can instead use \nEventSubmitter\n which is an abstraction around the Avro constructor for GobblinTrackingEvent.\n\n\nEvent Submitter\n\n\nAn event submitter is created using an \nEventSubmitter.Builder\n. It is associated with a Metric Context where it will submit all events, and it contains a \nnamespace\n and default \nmetadata\n that will be applied to all events generated through the event submitter. The user can then call \nEventSubmitter#submit\n which will package the event with the provided metadata and submit it to the Metric Context.\n\n\nReporters\n\n\nReporters export the metrics and/or events of a metric context to a sink. Reporters extend the interface \ncom.codahale.metrics.Reporter\n. Most reporters will attach themselves to a Metric Context. The reporter can then navigate the Metric Context tree where the Metric Context belongs, get tags and metrics, get notified of events, and export them to the sink.\n\n\nThe two best entry points for developing reporters are \nRecursiveScheduledMetricReporter\n and \nEventReporter\n. These classes do most of the heavy lifting for reporting metrics and events respectively. They are both scheduled reporters, meaning the will export their metrics / events following a configurable schedule.\n\n\nRecursiveScheduleMetricReporter\n\n\nThis abstract reporter base is used for emitting metrics on a schedule. The reporter, on creation, is attached to a particular Metric Report. Every time the reporter is required to emit events, the reporter selects the attached Metric Context and all descendant Metric Contexts. For each of these metric contexts, it queries the Metric Context for all metrics, filtered by an optional user supplied filter, and then calls \nRecursiveScheduledMetricReporter#report\n, providing the method with all appropriate metrics and tags. Developers need only implement the report method.\n\n\nEventReporter\n\n\nThis abstract reporter base is used for emitting events. The EventReporter, on creation, takes a Metric Context it should listen to. It registers a callback function as a notification target for that Metric Context. Every time the callback is called, if the notification is of type \nEventNotification\n, the EventReporter unpacks the event and adds it to a \nLinkedBlockingQueue\n of events.\n\n\nOn a configurable schedule, the event reporter calls the abstract method \nEventReporter#reportEventQueue(Queue\nGobblinTrackingEvent\n)\n, which should be implemented by the concrete subclass. To keep memory limited, the event queue has a maximum size. Whenever the queue reaches a size 2/3 of the maximum size, \nEventReporter#reportEventQueue\n is called immediately.", 
            "title": "Gobblin Metrics Architecture"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/#table-of-contents", 
            "text": "Table of Contents  Metric Context  Metrics  Events  Event Submitter    Reporters  RecursiveScheduleMetricReporter  EventReporter", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/#metric-context", 
            "text": "Metric contexts are organized hierarchically in a tree. Each metric context has a set of Tags, each of which is just key-value pair. The keys of all tags are strings, while the values are allowed to be of any type. However, most reporters will serialize the tag values using their  toString()  method.  Children contexts automatically inherit the tags of their parent context, and can add more tags, or override tags present in the parent. Tags can only be defined during construction of each metric context, and are immutable afterwards. This simplifies the inheritance and overriding of metrics.   Metric Contexts are created using  MetricContext.Builder , which allows adding tags and specifying the parent. This is the only time tags can be added to the context. When building, the tags of the parent and the new tags are merged to obtain the final tags for this context. When building a child context for Metric Context  context , calling  context.childBuilder(String)  generates a Builder with the correct parent.  Each metric context contains the following instance variables:   A  String   name . The name is not used by the core metrics engine, but can be accessed by users to identify the context.  A reference to the parent metric context, or null if it has no parent.  A list of children metric context references, stored as soft references.  An object of type  Tagged  containing the tags for this metric context.  A  Set  of notification targets. Notification targets are objects of type  Function Notification , Void  which are all called every time there is a new notification. Notifications can be submitted to the Metric Context using the method  sendNotification(Notification) . Notification targets can be added using  addNotificationTarget(Function Notification, Void ) .  A lazily instantiated  ExecutorService  used for asynchronously executing the notification targets. The executor service will only be started the first time there is a notification and the number of notification targets is positive.  A  ConcurrentMap  from metric names to  Metric  for all metrics registered in this Metric Context. Metrics can be added to this map using the  register(Metric) ,  register(String, Metric) , or  registerAll(MetricSet) , although it is recommended to instead use the methods to create and register the metrics. Metric Context implements getter methods for all metrics, as well as for each type of metric individually ( getMetrics ,  getGauges ,  getCounters ,  getHistograms ,  getMeters ,  getTimers ).", 
            "title": "Metric Context"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/#metrics", 
            "text": "All metrics extend the interface  ContextAwareMetric . Each metric type in Dropwizard Metrics is extended to a Context Aware type:  ContextAwareCounter ,  ContextAwareGauge ,  ContextAwareHistogram ,  ContextAwareMeter ,  ContextAwareTimer .  Context Aware metrics all always created from the Metric Context where they will be registered. For example, to get a counter under Metric Context  context , the user would call  context.counter(\"counter.name\") . This method first checks all registered metrics in the Metric Context to find a counter with that name, if it succeeds, it simply returns that counter. If a counter with that name has not been registered in  context , then a new  ContextAwareCounter  is created and registered in  context .  On creation, each Context Aware metric (except Gauges) checks if its parent Metric Context has parents itself. If so, then it automatically creates a metric of the same type, with the same name, in that parent. This will be repeated recursively until, at the end, all ancestor Metric Contexts will all contain a context aware metric of the same type and with the same name. Every time the context aware metric is updated, the metric will automatically call the same update method, with the same update value, for its parent metric. Again, this will continue recursively until the corresponding metrics in all ancestor metric contexts are updated by the same value. If multiple children of a metric context  context  all have metrics with the same name, when either of them is updated, the corresponding metric in  context  will also get updated. In this way, the corresponding metric in  context  will aggregate all updated to the metrics in the children context.  Users can also register objects of type  com.codahale.metrics.Metric  with any Metric Context, but they will not be auto-aggregated.", 
            "title": "Metrics"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/#events", 
            "text": "Events are objects of type  GobblinTrackingEvent , which is a type generated from an Avro schema. Events have:   A  namespace .  A  name .  A  timestamp .  A  Map String,String  of  metadata .   Events are submitted using the  MetricContext#submitEvent(GobblinTrackingEvent)  method. When called, this method packages the event into an  EventNotification  and submits it to the metric context using the method  MetricContext#sendNotification(Notification) . This notification is passed to all metrics context ancestors. Each notification target of each ancestor metric context will receive the EventNotification. Events are not stored by any Metric Context, so the notification targets need to handle these events appropriately.  Events can be created manually using Avro constructors, and using the method  context.submitEvent(GobblinTrackinEvent) , but this is unfriendly when trying to build events incrementally, especially when using metadata. To address this, users can instead use  EventSubmitter  which is an abstraction around the Avro constructor for GobblinTrackingEvent.", 
            "title": "Events"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/#event-submitter", 
            "text": "An event submitter is created using an  EventSubmitter.Builder . It is associated with a Metric Context where it will submit all events, and it contains a  namespace  and default  metadata  that will be applied to all events generated through the event submitter. The user can then call  EventSubmitter#submit  which will package the event with the provided metadata and submit it to the Metric Context.", 
            "title": "Event Submitter"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/#reporters", 
            "text": "Reporters export the metrics and/or events of a metric context to a sink. Reporters extend the interface  com.codahale.metrics.Reporter . Most reporters will attach themselves to a Metric Context. The reporter can then navigate the Metric Context tree where the Metric Context belongs, get tags and metrics, get notified of events, and export them to the sink.  The two best entry points for developing reporters are  RecursiveScheduledMetricReporter  and  EventReporter . These classes do most of the heavy lifting for reporting metrics and events respectively. They are both scheduled reporters, meaning the will export their metrics / events following a configurable schedule.", 
            "title": "Reporters"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/#recursiveschedulemetricreporter", 
            "text": "This abstract reporter base is used for emitting metrics on a schedule. The reporter, on creation, is attached to a particular Metric Report. Every time the reporter is required to emit events, the reporter selects the attached Metric Context and all descendant Metric Contexts. For each of these metric contexts, it queries the Metric Context for all metrics, filtered by an optional user supplied filter, and then calls  RecursiveScheduledMetricReporter#report , providing the method with all appropriate metrics and tags. Developers need only implement the report method.", 
            "title": "RecursiveScheduleMetricReporter"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Architecture/#eventreporter", 
            "text": "This abstract reporter base is used for emitting events. The EventReporter, on creation, takes a Metric Context it should listen to. It registers a callback function as a notification target for that Metric Context. Every time the callback is called, if the notification is of type  EventNotification , the EventReporter unpacks the event and adds it to a  LinkedBlockingQueue  of events.  On a configurable schedule, the event reporter calls the abstract method  EventReporter#reportEventQueue(Queue GobblinTrackingEvent ) , which should be implemented by the concrete subclass. To keep memory limited, the event queue has a maximum size. Whenever the queue reaches a size 2/3 of the maximum size,  EventReporter#reportEventQueue  is called immediately.", 
            "title": "EventReporter"
        }, 
        {
            "location": "/metrics/Implementing-New-Reporters/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nExtending Builders\n\n\nMetric Reporting\n\n\nEvent Reporting\n\n\nOther Reporters\n\n\n\n\n\n\nThe two best entry points for implementing custom reporters are \nRecursiveScheduledMetricReporter\n and \nEventReporter\n. Each of these classes automatically schedules reporting, extracts the correct metrics, and calls a single method that must be implemented by the developer. These methods also implement builder patterns that can be extended by the developer.\n\n\nIn the interest of giving more control to the users, metric and event reporters are kept separate, allowing users to more easily specify separate sinks for events and metrics. However, it is possible to implement a single report that handles both events and metrics.\n\n\n\n\nIt is recommended that each reporter has a constructor with signature \ninit\n(Properties)\n. In the near future we are planning to implement auto-starting, file-configurable reporting similar to Log4j architecture, and compliant reporters will be required to have such a constructor.\n\n\n\n\nExtending Builders\n\n\nThe builder patterns implemented in the base reporters are designed to be extendable. The architecture is a bit complicated, but a subclass of the base reporters wanting to use builder patterns should follow this pattern (replacing with RecursiveScheduledMetricReporter in the case of a metrics reporter):\n\n\nclass MyReporter extends EventReporter {\n\n  private MyReporter(Builder\n?\n builder) throws IOException {\n    super(builder);\n    // Other initialization logic.\n  }\n\n  // Concrete implementation of extendable Builder.\n  public static class BuilderImpl extends Builder\nBuilderImpl\n {\n    private BuilderImpl(MetricContext context) {\n      super(context);\n    }\n\n    @Override\n    protected BuilderImpl self() {\n      return this;\n    }\n  }\n\n  public static class Factory {\n    /**\n     * Returns a new {@link MyReporter.Builder} for {@link MyReporter}.\n     * Will automatically add all Context tags to the reporter.\n     *\n     * @param context the {@link gobblin.metrics.MetricContext} to report\n     * @return MyReporter builder\n     */\n    public static BuilderImpl forContext(MetricContext context) {\n      return new BuilderImpl(context);\n    }\n  }\n\n  /**\n   * Builder for {@link MyReporter}.\n   */\n  public static abstract class Builder\nT extends EventReporter.Builder\nT\n\n      extends EventReporter.Builder\nT\n {\n\n    // Additional instance variables needed to construct MyReporter.\n    private int myBuilderVariable;\n\n    protected Builder(MetricContext context) {\n      super(context);\n      this.myBuilderVariable = 0;\n    }\n\n    /**\n     * Set myBuilderVariable.\n     */\n    public T withMyBuilderVariable(int value) {\n      this.myBuilderVariable = value;\n      return self();\n    }\n\n    // Other setters for Builder variables.\n\n    /**\n     * Builds and returns {@link MyReporter}.\n     */\n    public MyReporter build() throws IOException {\n      return new MyReporter(this);\n    }\n\n  }\n}\n\n\n\n\nThis pattern allows users to simply call\n\n\nMyReporter reporter = MyReporter.Factory.forContext(context).build();\n\n\n\n\nto generate an instance of the reporter. Additionally, if you want to further extend MyReporter, following the exact same pattern except extending MyReporter instead of EventReporter will work correctly (which would not be true with standard Builder pattern).\n\n\nMetric Reporting\n\n\nDevelopers should extend \nRecursiveScheduledMetricReporter\n and implement the method \nRecursiveScheduledMetricReporter#report\n. The base class will call report when appropriate with the list of metrics, separated by type, and tags that should be reported.\n\n\nEvent Reporting\n\n\nDevelopers should extend \nEventReporter\n and implement the method \nEventReporter#reportEventQueue(Queue\nGobblinTrackingEvent\n)\n. The base class will call this method with a queue of all events to report as needed.\n\n\nOther Reporters\n\n\nIt is also possible to implement a reporter without using the suggested classes. Reporters are recommended, but not required, to extend the interface \nReporter\n. Reporters can use the public methods of \nMetricContext\n to navigate the Metric Context tree, query metrics, and register for notifications.", 
            "title": "Implementing New Reporters"
        }, 
        {
            "location": "/metrics/Implementing-New-Reporters/#table-of-contents", 
            "text": "Table of Contents  Extending Builders  Metric Reporting  Event Reporting  Other Reporters    The two best entry points for implementing custom reporters are  RecursiveScheduledMetricReporter  and  EventReporter . Each of these classes automatically schedules reporting, extracts the correct metrics, and calls a single method that must be implemented by the developer. These methods also implement builder patterns that can be extended by the developer.  In the interest of giving more control to the users, metric and event reporters are kept separate, allowing users to more easily specify separate sinks for events and metrics. However, it is possible to implement a single report that handles both events and metrics.   It is recommended that each reporter has a constructor with signature  init (Properties) . In the near future we are planning to implement auto-starting, file-configurable reporting similar to Log4j architecture, and compliant reporters will be required to have such a constructor.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/metrics/Implementing-New-Reporters/#extending-builders", 
            "text": "The builder patterns implemented in the base reporters are designed to be extendable. The architecture is a bit complicated, but a subclass of the base reporters wanting to use builder patterns should follow this pattern (replacing with RecursiveScheduledMetricReporter in the case of a metrics reporter):  class MyReporter extends EventReporter {\n\n  private MyReporter(Builder ?  builder) throws IOException {\n    super(builder);\n    // Other initialization logic.\n  }\n\n  // Concrete implementation of extendable Builder.\n  public static class BuilderImpl extends Builder BuilderImpl  {\n    private BuilderImpl(MetricContext context) {\n      super(context);\n    }\n\n    @Override\n    protected BuilderImpl self() {\n      return this;\n    }\n  }\n\n  public static class Factory {\n    /**\n     * Returns a new {@link MyReporter.Builder} for {@link MyReporter}.\n     * Will automatically add all Context tags to the reporter.\n     *\n     * @param context the {@link gobblin.metrics.MetricContext} to report\n     * @return MyReporter builder\n     */\n    public static BuilderImpl forContext(MetricContext context) {\n      return new BuilderImpl(context);\n    }\n  }\n\n  /**\n   * Builder for {@link MyReporter}.\n   */\n  public static abstract class Builder T extends EventReporter.Builder T \n      extends EventReporter.Builder T  {\n\n    // Additional instance variables needed to construct MyReporter.\n    private int myBuilderVariable;\n\n    protected Builder(MetricContext context) {\n      super(context);\n      this.myBuilderVariable = 0;\n    }\n\n    /**\n     * Set myBuilderVariable.\n     */\n    public T withMyBuilderVariable(int value) {\n      this.myBuilderVariable = value;\n      return self();\n    }\n\n    // Other setters for Builder variables.\n\n    /**\n     * Builds and returns {@link MyReporter}.\n     */\n    public MyReporter build() throws IOException {\n      return new MyReporter(this);\n    }\n\n  }\n}  This pattern allows users to simply call  MyReporter reporter = MyReporter.Factory.forContext(context).build();  to generate an instance of the reporter. Additionally, if you want to further extend MyReporter, following the exact same pattern except extending MyReporter instead of EventReporter will work correctly (which would not be true with standard Builder pattern).", 
            "title": "Extending Builders"
        }, 
        {
            "location": "/metrics/Implementing-New-Reporters/#metric-reporting", 
            "text": "Developers should extend  RecursiveScheduledMetricReporter  and implement the method  RecursiveScheduledMetricReporter#report . The base class will call report when appropriate with the list of metrics, separated by type, and tags that should be reported.", 
            "title": "Metric Reporting"
        }, 
        {
            "location": "/metrics/Implementing-New-Reporters/#event-reporting", 
            "text": "Developers should extend  EventReporter  and implement the method  EventReporter#reportEventQueue(Queue GobblinTrackingEvent ) . The base class will call this method with a queue of all events to report as needed.", 
            "title": "Event Reporting"
        }, 
        {
            "location": "/metrics/Implementing-New-Reporters/#other-reporters", 
            "text": "It is also possible to implement a reporter without using the suggested classes. Reporters are recommended, but not required, to extend the interface  Reporter . Reporters can use the public methods of  MetricContext  to navigate the Metric Context tree, query metrics, and register for notifications.", 
            "title": "Other Reporters"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nGeneralities\n\n\nHow to interpret these numbers\n\n\nWhat if I need larger QPS?\n\n\n\n\n\n\nUpdate Metrics Performance\n\n\nMultiple metric updates per iteration\n\n\nMulti-threading\n\n\nRunning Performance Tests\n\n\n\n\n\n\n\n\n\n\nGeneralities\n\n\nThese are the main resources used by Gobblin Metrics:\n\n\n\n\nCPU time for updating metrics: scales with number of metrics and frequency of metric update\n\n\nCPU time for metric emission and lifecycle management: scales with number of metrics and frequency of emission\n\n\nMemory for storing metrics: scales with number of metrics and metric contexts\n\n\nI/O for reporting metrics: scales with number of metrics and frequency of emission\n\n\nExternal resources for metrics emission (e.g. HDFS space, Kafka queue space, etc.): scales with number of metrics and frequency of emission\n\n\n\n\nThis page focuses on the CPU time for updating metrics, as these updates are usually in the critical performance path of an application. Each metric requires bounded memory, and having a few metrics should have no major effect on memory usage. Metrics and Metric Contexts are cleaned when no longer needed to further reduce this impact. Resources related to metric emission can always be reduced by reporting fewer metrics or decreasing the reporting frequency when necessary.\n\n\nHow to interpret these numbers\n\n\nThis document provides maximum QPS achievable by Gobblin Metrics. If the application attempts to update metrics at a higher rate than this, the metrics will effectively throttle the application. If, on the other hand, the application only updates metrics at 10% or less of the maximum QPS, the performance impact of Gobblin Metrics should be minimal.\n\n\nWhat if I need larger QPS?\n\n\nIf your application needs larger QPS, the recommendation is to batch metrics updates. Counters and Meters offer the option to increase their values by multiple units at a time. Histograms and Timers don't offer this option, but for very high throughput applications, randomly registering for example only 10% of the values will not affect statistics significantly (although you will have to adjust timer and histogram counts manually).\n\n\nUpdate Metrics Performance\n\n\nMetric updates are the most common interaction with Gobblin Metrics in an application. Every time a counter is increased, a meter is marked, or entries are added to histograms and timers, an update happens. As such, metric updates are the most likely to impact application performance.\n\n\nWe measured the max number of metric updates that can be executed per second. The performance of different metric types is different. Also, the performance of metrics depends on the depth in the Metric Context tree at which they are created. Metrics in the Root Metric Context are the fastest, while metrics deep in the tree are slower because they have to update all ancestors as well. The following table shows reference max QPS in updates per second as well as the equivalent single update delay in nanoseconds for each metric type in a i7 processor:\n\n\n\n\n\n\n\n\nMetric\n\n\nRoot level\n\n\nDepth: 1\n\n\nDepth: 2\n\n\nDepth: 3\n\n\n\n\n\n\n\n\n\n\nCounter\n\n\n76M (13ns)\n\n\n39M (25ns)\n\n\n29M (34ns)\n\n\n24M (41ns)\n\n\n\n\n\n\nMeter\n\n\n11M (90ns)\n\n\n7M (142ns)\n\n\n4.5M (222ns)\n\n\n3.5M (285ns)\n\n\n\n\n\n\nHistogram\n\n\n2.4M (416ns)\n\n\n2.4M (416ns)\n\n\n1.8M (555ns)\n\n\n1.3M (769ns)\n\n\n\n\n\n\nTimer\n\n\n1.4M (714ns)\n\n\n1.4M (714ns)\n\n\n1M (1us)\n\n\n1M (1us)\n\n\n\n\n\n\n\n\nMultiple metric updates per iteration\n\n\nIf a single thread updates multiple metrics, the average delay for metric updates will be the sum of the delays of each metric independently. For example, if each iteration the application is updating two counters, one timer, and one histogram at the root metric context level, the total delay will be \n13ns + 13ns + 416ns + 714ns = 1156ns\n for a max QPS of \n865k\n.\n\n\nMulti-threading\n\n\nUpdating metrics with different names can be parallelized efficiently, e.g. different threads updating metrics with different names will not interfere with each other. However, multiple threads updating metrics with the same names will interfere with each other, as the updates of common ancestor metrics are synchronized (to provide with auto-aggregation). In experiments we observed that updating metrics with the same name from multiple threads increases the maximum QPS sub-linearly, saturating at about 3x the single threaded QPS, i.e. the total QPS of metrics updates across any number of threads will not go about 3x the numbers shown in the table above.\n\n\nOn the other hand, if each thread is updating multiple metrics, the updates might interleave with each other, potentially increasing the max total QPS. In the example with two counters, one timer, and one histogram, one thread could be updating the timer while another could be updating the histogram, reducing interference, but never exceeding the max QPS of the single most expensive metric. Note that there is no optimization in code to produce this interleaving, it is merely an effect of synchronization, so the effect might vary.\n\n\nRunning Performance Tests\n\n\nTo run the performance tests\n\n\ncd gobblin-metrics\n../gradlew performance\n\n\n\n\nAfter finishing, it should create a TestNG report at \nbuild/gobblin-metrics/reports/tests/packages/gobblin.metrics.performance.html\n. Nicely printed performance results are available on the Output tab.", 
            "title": "Gobblin Metrics Performance"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/#table-of-contents", 
            "text": "Table of Contents  Generalities  How to interpret these numbers  What if I need larger QPS?    Update Metrics Performance  Multiple metric updates per iteration  Multi-threading  Running Performance Tests", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/#generalities", 
            "text": "These are the main resources used by Gobblin Metrics:   CPU time for updating metrics: scales with number of metrics and frequency of metric update  CPU time for metric emission and lifecycle management: scales with number of metrics and frequency of emission  Memory for storing metrics: scales with number of metrics and metric contexts  I/O for reporting metrics: scales with number of metrics and frequency of emission  External resources for metrics emission (e.g. HDFS space, Kafka queue space, etc.): scales with number of metrics and frequency of emission   This page focuses on the CPU time for updating metrics, as these updates are usually in the critical performance path of an application. Each metric requires bounded memory, and having a few metrics should have no major effect on memory usage. Metrics and Metric Contexts are cleaned when no longer needed to further reduce this impact. Resources related to metric emission can always be reduced by reporting fewer metrics or decreasing the reporting frequency when necessary.", 
            "title": "Generalities"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/#how-to-interpret-these-numbers", 
            "text": "This document provides maximum QPS achievable by Gobblin Metrics. If the application attempts to update metrics at a higher rate than this, the metrics will effectively throttle the application. If, on the other hand, the application only updates metrics at 10% or less of the maximum QPS, the performance impact of Gobblin Metrics should be minimal.", 
            "title": "How to interpret these numbers"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/#what-if-i-need-larger-qps", 
            "text": "If your application needs larger QPS, the recommendation is to batch metrics updates. Counters and Meters offer the option to increase their values by multiple units at a time. Histograms and Timers don't offer this option, but for very high throughput applications, randomly registering for example only 10% of the values will not affect statistics significantly (although you will have to adjust timer and histogram counts manually).", 
            "title": "What if I need larger QPS?"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/#update-metrics-performance", 
            "text": "Metric updates are the most common interaction with Gobblin Metrics in an application. Every time a counter is increased, a meter is marked, or entries are added to histograms and timers, an update happens. As such, metric updates are the most likely to impact application performance.  We measured the max number of metric updates that can be executed per second. The performance of different metric types is different. Also, the performance of metrics depends on the depth in the Metric Context tree at which they are created. Metrics in the Root Metric Context are the fastest, while metrics deep in the tree are slower because they have to update all ancestors as well. The following table shows reference max QPS in updates per second as well as the equivalent single update delay in nanoseconds for each metric type in a i7 processor:     Metric  Root level  Depth: 1  Depth: 2  Depth: 3      Counter  76M (13ns)  39M (25ns)  29M (34ns)  24M (41ns)    Meter  11M (90ns)  7M (142ns)  4.5M (222ns)  3.5M (285ns)    Histogram  2.4M (416ns)  2.4M (416ns)  1.8M (555ns)  1.3M (769ns)    Timer  1.4M (714ns)  1.4M (714ns)  1M (1us)  1M (1us)", 
            "title": "Update Metrics Performance"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/#multiple-metric-updates-per-iteration", 
            "text": "If a single thread updates multiple metrics, the average delay for metric updates will be the sum of the delays of each metric independently. For example, if each iteration the application is updating two counters, one timer, and one histogram at the root metric context level, the total delay will be  13ns + 13ns + 416ns + 714ns = 1156ns  for a max QPS of  865k .", 
            "title": "Multiple metric updates per iteration"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/#multi-threading", 
            "text": "Updating metrics with different names can be parallelized efficiently, e.g. different threads updating metrics with different names will not interfere with each other. However, multiple threads updating metrics with the same names will interfere with each other, as the updates of common ancestor metrics are synchronized (to provide with auto-aggregation). In experiments we observed that updating metrics with the same name from multiple threads increases the maximum QPS sub-linearly, saturating at about 3x the single threaded QPS, i.e. the total QPS of metrics updates across any number of threads will not go about 3x the numbers shown in the table above.  On the other hand, if each thread is updating multiple metrics, the updates might interleave with each other, potentially increasing the max total QPS. In the example with two counters, one timer, and one histogram, one thread could be updating the timer while another could be updating the histogram, reducing interference, but never exceeding the max QPS of the single most expensive metric. Note that there is no optimization in code to produce this interleaving, it is merely an effect of synchronization, so the effect might vary.", 
            "title": "Multi-threading"
        }, 
        {
            "location": "/metrics/Gobblin-Metrics-Performance/#running-performance-tests", 
            "text": "To run the performance tests  cd gobblin-metrics\n../gradlew performance  After finishing, it should create a TestNG report at  build/gobblin-metrics/reports/tests/packages/gobblin.metrics.performance.html . Nicely printed performance results are available on the Output tab.", 
            "title": "Running Performance Tests"
        }, 
        {
            "location": "/developer-guide/Customization-for-New-Source/", 
            "text": "To be updated.", 
            "title": "Customization for New Source"
        }, 
        {
            "location": "/developer-guide/Customization-for-Converter-and-Operator/", 
            "text": "To be updated.", 
            "title": "Customization for Converter and Operator"
        }, 
        {
            "location": "/developer-guide/CodingStyle/", 
            "text": "Overview\n\n\nThe code formatting standard in this project is based on the \nOracle/Sun Code Convention\n and \nGoogle Java Style\n.  \n\n\nGuideline\n\n\nThe coding style is consistent with most of the open source projects with the following callout:\n\n\n\n\n\n\nNaming Conventions\n\n\n\n\nVariables are camel case beginning with a lowercase letter, e.g. \nfooBar\n\n\nConstant variables are declared as static final and should be all uppercase ASCII letters delimited by underscore (\"_\"), e.g. \nFOO_BAR\n\n\n\n\n\n\n\n\nImport statement\n\n\n\n\nDo not use 'star' imports, e.g. \nimport java.io.*\n;\n\n\nImport order: \njava\n, \norg\n, \ncom\n, \ngobblin\n.\n\n\n\n\n\n\n\n\nIndentation\n\n\n\n\nTwo spaces should be used as the unit of indentation;\n\n\nTabs must expand to spaces and the tab width should be set to two;\n\n\nLine length: lines should not exceed 120 characters;\n\n\n\n\n\n\n\n\nWhite space\n\n\n\n\nBlank lines should be provided to improve readability:\n\n\nBetween the local variables in a method and its first statement\n\n\nBetween methods\n\n\n\n\n\n\nBlank spaces should be used in the following circumstances:\n\n\nA keyword followed by a parenthesis should be separated by a space (e.g. \nwhile (true) {\n)\n\n\nA binary operators except . should be separated from their operands by spaces (e.g. \na + b\n);\n\n\n\n\n\n\n\n\n\n\n\n\nComments:\n\n\n\n\nImplementation comments: Block comments (\n/* ... */\n), end-of-line comments (\n//...\n) can be used to illustrate a particular implementation;\n\n\nDocumentation comments (\n/** ... */\n) should be used to describe Java classes, interfaces, methods;\n\n\n\n\n\n\n\n\nCompound statements are lists of statements enclosed in curly braces and should be formatted according to the following conventions:\n\n\n\n\nThe enclosed statements should be indented one more level than the enclosing statement\n\n\nThe opening brace should be on the same line as the enclosing statement (e.g. the 'if' clause)\n\n\nThe closing brace should be on a line by itself indented to match the enclosing statement\n\n\nBraces are used around all statements, even single statements, when they are part of a control structure, such as if-else or for statements. This makes it easier to add statements without accidentally introducing bugs due to forgetting to add braces.\n\n\n\n\n\n\n\n\nCode Style Template File\n\n\n\n\nEclipse\n\n\nDownload the \ncodetyle-eclipse.xml\n, Import the file through Preferences \n Java \n Code Style \n Formatter\n\n\nDownload the \nprefs-eclipse.epf\n, Import the file File \n Import \n General \n Preferences\n\n\n\n\n\n\nIntelliJ\n\n\nDownload the \ncodestyle-intellij.xml\n, Copy the file to \n~/.IntelliJIdeal3/config/codestyles\n on Linux (or \n$HOME/Library/Preferences/IntelliJIdeal3/codestyles\n on Mac)\n\n\nRestart the IDE\n\n\nGo to File \n Settings \n Code Style \n General \n Scheme to select the new style", 
            "title": "Code Style Guide"
        }, 
        {
            "location": "/developer-guide/CodingStyle/#overview", 
            "text": "The code formatting standard in this project is based on the  Oracle/Sun Code Convention  and  Google Java Style .", 
            "title": "Overview"
        }, 
        {
            "location": "/developer-guide/CodingStyle/#guideline", 
            "text": "The coding style is consistent with most of the open source projects with the following callout:    Naming Conventions   Variables are camel case beginning with a lowercase letter, e.g.  fooBar  Constant variables are declared as static final and should be all uppercase ASCII letters delimited by underscore (\"_\"), e.g.  FOO_BAR     Import statement   Do not use 'star' imports, e.g.  import java.io.* ;  Import order:  java ,  org ,  com ,  gobblin .     Indentation   Two spaces should be used as the unit of indentation;  Tabs must expand to spaces and the tab width should be set to two;  Line length: lines should not exceed 120 characters;     White space   Blank lines should be provided to improve readability:  Between the local variables in a method and its first statement  Between methods    Blank spaces should be used in the following circumstances:  A keyword followed by a parenthesis should be separated by a space (e.g.  while (true) { )  A binary operators except . should be separated from their operands by spaces (e.g.  a + b );       Comments:   Implementation comments: Block comments ( /* ... */ ), end-of-line comments ( //... ) can be used to illustrate a particular implementation;  Documentation comments ( /** ... */ ) should be used to describe Java classes, interfaces, methods;     Compound statements are lists of statements enclosed in curly braces and should be formatted according to the following conventions:   The enclosed statements should be indented one more level than the enclosing statement  The opening brace should be on the same line as the enclosing statement (e.g. the 'if' clause)  The closing brace should be on a line by itself indented to match the enclosing statement  Braces are used around all statements, even single statements, when they are part of a control structure, such as if-else or for statements. This makes it easier to add statements without accidentally introducing bugs due to forgetting to add braces.", 
            "title": "Guideline"
        }, 
        {
            "location": "/developer-guide/CodingStyle/#code-style-template-file", 
            "text": "Eclipse  Download the  codetyle-eclipse.xml , Import the file through Preferences   Java   Code Style   Formatter  Download the  prefs-eclipse.epf , Import the file File   Import   General   Preferences    IntelliJ  Download the  codestyle-intellij.xml , Copy the file to  ~/.IntelliJIdeal3/config/codestyles  on Linux (or  $HOME/Library/Preferences/IntelliJIdeal3/codestyles  on Mac)  Restart the IDE  Go to File   Settings   Code Style   General   Scheme to select the new style", 
            "title": "Code Style Template File"
        }, 
        {
            "location": "/developer-guide/IDE-setup/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nIntroduction\n\n\nIntelliJ Integration\n\n\nEclipse Integration\n\n\nLombok\n\n\n\n\n\n\nIntroduction\n\n\nThis document is for users who want to import the Gobblin code base into an \nIDE\n and directly modify that Gobblin code base. This is not for users who want to just setup Gobblin as a Maven dependency.\n\n\nIntelliJ Integration\n\n\nGobblin uses standard build tools to import code into an IntelliJ project. Execute the following command to build the necessary \n*.iml\n files:\n\n\n./gradlew clean idea\n\n\n\n\nOnce the command finishes, use standard practices to import the project into IntelliJ.\n\n\nEclipse Integration\n\n\nGobblin uses standard build tools to import code into an Eclipse project. Execute the following command to build the necessary \n*.classpath\n and \n*.project\n files:\n\n\n./gradlew clean eclipse\n\n\n\n\nOnce the command finishes, use standard practices to import the project into Eclipse.\n\n\nLombok\n\n\nGobblin uses \nLombok\n for reducing boilerplate code. Lombok auto generates boilerplate code at runtime if you are building gobblin from command line.If you are using an IDE, you will see compile errors in some of the classes that use Lombok. Please follow the \nIDE setup instructions\n for your IDE to setup lombok.", 
            "title": "IDE setup"
        }, 
        {
            "location": "/developer-guide/IDE-setup/#table-of-contents", 
            "text": "Table of Contents  Introduction  IntelliJ Integration  Eclipse Integration  Lombok", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/developer-guide/IDE-setup/#introduction", 
            "text": "This document is for users who want to import the Gobblin code base into an  IDE  and directly modify that Gobblin code base. This is not for users who want to just setup Gobblin as a Maven dependency.", 
            "title": "Introduction"
        }, 
        {
            "location": "/developer-guide/IDE-setup/#intellij-integration", 
            "text": "Gobblin uses standard build tools to import code into an IntelliJ project. Execute the following command to build the necessary  *.iml  files:  ./gradlew clean idea  Once the command finishes, use standard practices to import the project into IntelliJ.", 
            "title": "IntelliJ Integration"
        }, 
        {
            "location": "/developer-guide/IDE-setup/#eclipse-integration", 
            "text": "Gobblin uses standard build tools to import code into an Eclipse project. Execute the following command to build the necessary  *.classpath  and  *.project  files:  ./gradlew clean eclipse  Once the command finishes, use standard practices to import the project into Eclipse.", 
            "title": "Eclipse Integration"
        }, 
        {
            "location": "/developer-guide/IDE-setup/#lombok", 
            "text": "Gobblin uses  Lombok  for reducing boilerplate code. Lombok auto generates boilerplate code at runtime if you are building gobblin from command line.If you are using an IDE, you will see compile errors in some of the classes that use Lombok. Please follow the  IDE setup instructions  for your IDE to setup lombok.", 
            "title": "Lombok"
        }, 
        {
            "location": "/developer-guide/Monitoring-Design/", 
            "text": "Metrics Collection Basics\n\n\nPlease refer to \nGobblin Metrics Architecture\n section.", 
            "title": "Monitoring Design"
        }, 
        {
            "location": "/developer-guide/Monitoring-Design/#metrics-collection-basics", 
            "text": "Please refer to  Gobblin Metrics Architecture  section.", 
            "title": "Metrics Collection Basics"
        }, 
        {
            "location": "/project/Feature-List/", 
            "text": "Currently, Gobblin supports the following feature list:\n\n\nDifferent Data Sources\n\n\n\n\n\n\n\n\nSource Type\n\n\nProtocol\n\n\nVendors\n\n\n\n\n\n\n\n\n\n\nRDMS\n\n\nJDBC\n\n\nMySQL/SQLServer\n\n\n\n\n\n\nFiles\n\n\nHDFS/SFTP/LocalFS\n\n\nN/A\n\n\n\n\n\n\nSalesforce\n\n\nREST\n\n\nSalesforce\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent Pulling Types\n\n\n\n\nSNAPSHOT-ONLY: Pull the snapshot of one dataset.\n\n\nSNAPSHOT-APPEND: Pull delta changes since last run, optionally merge delta changes into snapshot (Delta changes include updates to the dataset since last run).\n\n\nAPPEND-ONLY: Pull delta changes since last run, and append to dataset.\n\n\n\n\n\n\n\n\nDifferent Deployment Types\n\n\n\n\nstandalone deploy on a single machine\n\n\ncluster deploy on hadoop 1.2.1, hadoop 2.3.0\n\n\n\n\n\n\n\n\nCompaction\n\n\n\n\nMerge delta changes into snapshot.", 
            "title": "Feature List"
        }, 
        {
            "location": "/project/Feature-List/#different-data-sources", 
            "text": "Source Type  Protocol  Vendors      RDMS  JDBC  MySQL/SQLServer    Files  HDFS/SFTP/LocalFS  N/A    Salesforce  REST  Salesforce       Different Pulling Types   SNAPSHOT-ONLY: Pull the snapshot of one dataset.  SNAPSHOT-APPEND: Pull delta changes since last run, optionally merge delta changes into snapshot (Delta changes include updates to the dataset since last run).  APPEND-ONLY: Pull delta changes since last run, and append to dataset.     Different Deployment Types   standalone deploy on a single machine  cluster deploy on hadoop 1.2.1, hadoop 2.3.0     Compaction   Merge delta changes into snapshot.", 
            "title": "Different Data Sources"
        }, 
        {
            "location": "/project/Team/", 
            "text": "Current team members:\n\n\n\n\nAbhishek Tiwari\n\n\nChavdar Botev\n\n\nIssac Buenrostro\n\n\nKapil Surlaker\n\n\nMin Tu\n\n\nNarasimha Reddy\n\n\nPradhan Cadabam\n\n\nSahil Takiar\n\n\nShirshanka Das\n\n\nVasanth Rajamani\n\n\nYinan Li\n\n\nYing Dai\n\n\nZiyang Liu", 
            "title": "Contributors and Team"
        }, 
        {
            "location": "/project/Talks-and-Tech-Blogs/", 
            "text": "Infrastructure @ LinkedIn (Gobblin in the ecosystem)\n\n\n\n\nBigger, Faster, Easier: Building a Real-Time Self Service Data Analytics Ecosystem at LinkedIn\n (Hadoop Summit 2015)\n\n\n\n\nGobblin @Linkedin and beyond\n\n\n\n\nQCon presentation\n (Nov 5th, 2014)\n\n\nEngineering Blog Post\n (Nov 25th, 2014)\n\n\nGobblin: Unifying Data Ingestion for Hadoop\n (VLDB 2015)\n\n\nGobblin: Unifying Data Ingestion for Hadoop\n (VLDB 2015 slides)\n\n\nIngestion from Kafka using Gobblin\n (Gobblin Meetup November 2015)\n\n\nGobblin on Yarn: A Preview\n (Gobblin Meetup November 2015)\n\n\nGobblin@NerdWallet  External Use Case 1\n (Gobblin Meetup November 2015)\n\n\nGobblin@Intel  External Use Case 2\n (Gobblin Meetup November 2015)", 
            "title": "Talks and Tech Blog Posts"
        }, 
        {
            "location": "/project/News/", 
            "text": "Feb 5th, 2015\n: Switched development process to github.\n\n\n\n\n\n\nNov 25th, 2014\n: Released Linkedin Engineering Blog on Gobblin.\n\n\n\n\n\n\nNov 5th, 2014\n: Presented Gobblin at QConSF, 2014.", 
            "title": "News and Roadmap"
        }, 
        {
            "location": "/project/Posts/", 
            "text": "Gobblin Metrics: next generation instrumentation for applications", 
            "title": "Posts"
        }, 
        {
            "location": "/miscellaneous/Camus-to-Gobblin-Migration/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nAdvantages of Migrating to Gobblin\n\n\nKafka Ingestion Related Job Config Properties\n\n\nConfig properties for pulling Kafka topics\n\n\nConfig properties for compaction\n\n\n\n\n\n\nDeployment and Checkpoint Management\n\n\nMigrating from Camus to Gobblin in Production\n\n\n\n\n\n\nThis page is a guide for \nCamus\n \u2192 Gobblin migration, intended for users and organizations currently using Camus. Camus is LinkedIn's previous-generation Kafka-HDFS pipeline.\n\n\nIt is recommended that one read \nKafka-HDFS Ingestion\n before reading this page. This page focuses on the Kafka-related configuration properties in Gobblin vs Camus.\n\n\nAdvantages of Migrating to Gobblin\n\n\nOperability\n: Gobblin is a generic data ingestion pipeline that supports not only Kafka but several other data sources, and new data sources can be easily added. If you have multiple data sources, using a single tool to ingest data from these sources is a lot more pleasant operationally than deploying a separate tool for each source.\n\n\nPerformance\n: The performance of Gobblin in MapReduce mode is comparable to Camus', and faster in some cases (e.g., the average record size of a Kafka topic is not proportional to the average time of pulling a topic) due to a better mapper load balancing algorithm. In the new continuous ingestion mode (currently under development), the performance of Gobblin will further improve.\n\n\nMetrics and Monitoring\n: Gobblin has a powerful end-to-end metrics collection and reporting module for monitoring purpose, making it much easier to spot problems in time and find the root causes. See the \"Gobblin Metrics\" section in the wiki and \nthis post\n for more details.\n\n\nFeatures\n: In addition to the above, there are several other useful features for Kafka-HDFS ingestion in Gobblin that are not available in Camus, e.g., \nhandling late events in data compaction\n; dataset retention management; converter and quality checker; all-or-nothing job commit policy, etc. Also, Gobblin is under active development and new features are added frequently.\n\n\nKafka Ingestion Related Job Config Properties\n\n\nThis list contains Kafka-specific properties. For general configuration properties please refer to \nConfiguration Properties Glossary\n.\n\n\nConfig properties for pulling Kafka topics\n\n\n\n\n\n\n\n\nGobblin Property\n\n\nCorresponding Camus Property\n\n\nDefault value\n\n\n\n\n\n\n\n\n\n\ntopic.whitelist\n\n\nkafka.whitelist.topics\n\n\n.*\n\n\n\n\n\n\ntopic.blacklist\n\n\nkafka.blacklist.topics\n\n\na^\n\n\n\n\n\n\nmr.job.max.mappers\n\n\nmapred.map.tasks\n\n\n100\n\n\n\n\n\n\nkafka.brokers\n\n\nkafka.host.url\n\n\n(required)\n\n\n\n\n\n\ntopics.move.to.latest.offset\n\n\nkafka.move.to.last.offset.list\n\n\nempty\n\n\n\n\n\n\nbootstrap.with.offset\n\n\nnone\n\n\nlatest\n\n\n\n\n\n\nreset.on.offset.out.of.range\n\n\nnone\n\n\nnearest\n\n\n\n\n\n\n\n\nRemarks:\n\n\n\n\ntopic.whitelist and topic.blacklist supports regex.\n\n\ntopics.move.to.latest.offset: Topics in this list will always start from the latest offset (i.e., no records will be pulled). To move all topics to the latest offset, use \"all\". This property is useful in Camus for moving a new topic to the latest offset, but in Gobblin it should rarely, if ever, be used, since you can use bootstrap.with.offset to achieve the same purpose more conveniently.\n\n\nbootstrap with offset: For new topics / partitions, this property controls whether they start at the earliest offset or the latest offset. Possible values: earliest, latest, skip.\n\n\nreset.on.offset.out.of.range: This property controls what to do if a partition's previously persisted offset is out of the range of the currently available offsets. Possible values: earliest (always move to earliest available offset), latest (always move to latest available offset), nearest (move to earliest if the previously persisted offset is smaller than the earliest offset, otherwise move to latest), skip (skip this partition).\n\n\n\n\nConfig properties for compaction\n\n\nGobblin compaction is comparable to Camus sweeper, which can deduplicate records in an input folder. Compaction is useful for Kafka-HDFS ingestion for two reasons:\n\n\n\n\n\n\nAlthough Gobblin guarantees no loss of data, in rare circumstances where data is published on HDFS but checkpoints failed to be persisted into the state store, it may pull the same records twice.\n\n\n\n\n\n\nIf you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during replication.\n\n\n\n\n\n\nBelow are the configuration properties related to compaction. For more information please visit the MapReduce Compaction section in the \nCompaction\n page.\n\n\n\n\n\n\n\n\nGobblin Property\n\n\nCorresponding Camus Property\n\n\nDefault value\n\n\n\n\n\n\n\n\n\n\ncompaction.input.dir\n\n\ncamus.sweeper.source.dir\n\n\n(required)\n\n\n\n\n\n\ncompaction.dest.dir\n\n\ncamus.sweeper.dest.dir\n\n\n(required)\n\n\n\n\n\n\ncompaction.input.subdir\n\n\ncamus.sweeper.source.dir\n\n\nhourly\n\n\n\n\n\n\ncompaction.dest.subdir\n\n\ncamus.sweeper.dest.dir\n\n\ndaily\n\n\n\n\n\n\ncompaction.tmp.dest.dir\n\n\ncamus.sweeper.tmp.dir\n\n\n/tmp/gobblin-compaction\n\n\n\n\n\n\ncompaction.whitelist\n\n\ncamus.sweeper.whitelist\n\n\n.*\n\n\n\n\n\n\ncompaction.blacklist\n\n\ncamus.sweeper.blacklist\n\n\na^\n\n\n\n\n\n\ncompaction.high.priority.topics\n\n\nnone\n\n\na^\n\n\n\n\n\n\ncompaction.normal.priority.topics\n\n\nnone\n\n\na^\n\n\n\n\n\n\ncompaction.input.deduplicated\n\n\nnone\n\n\nfalse\n\n\n\n\n\n\ncompaction.output.deduplicated\n\n\nnone\n\n\ntrue\n\n\n\n\n\n\ncompaction.file.system.uri\n\n\nnone\n\n\n\n\n\n\n\n\ncompaction.timebased.max.time.ago\n\n\nnone\n\n\n3d\n\n\n\n\n\n\ncompaction.timebased.min.time.ago\n\n\nnone\n\n\n1d\n\n\n\n\n\n\ncompaction.timebased.folder.pattern\n\n\nnone\n\n\nYYYY/mm/dd\n\n\n\n\n\n\ncompaction.thread.pool.size\n\n\nnum.threads\n\n\n20\n\n\n\n\n\n\ncompaction.max.num.reducers\n\n\nmax.files\n\n\n900\n\n\n\n\n\n\ncompaction.target.output.file.size\n\n\ncamus.sweeper.target.file.size\n\n\n268435456\n\n\n\n\n\n\ncompaction.mapred.min.split.size\n\n\nmapred.min.split.size\n\n\n268435456\n\n\n\n\n\n\ncompaction.mapred.max.split.size\n\n\nmapred.max.split.size\n\n\n268435456\n\n\n\n\n\n\ncompaction.mr.job.timeout.minutes\n\n\nnone\n\n\n\n\n\n\n\n\n\n\nRemarks:\n\n\n\n\nThe following properties support regex: compaction.whitelist, compaction.blacklist, compaction.high.priority.topics, compaction.normal.priority.topics\n\n\ncompaction.input.dir is the parent folder of input topics, e.g., /data/kafka_topics, which contains topic folders such as /data/kafka_topics/Topic1, /data/kafka_topics/Topic2, etc. Note that Camus uses camus.sweeper.source.dir both as the input folder of Camus sweeper (i.e., compaction), and as the output folder for ingesting Kafka topics. In Gobblin, one should use data.publisher.final.dir as the output folder for ingesting Kafka topics.\n\n\ncompaction.output.dir is the parent folder of output topics, e.g., /data/compacted_kafka_topics.\n\n\ncompaction.input.subdir is the subdir name of output topics, if exists. For example, if the input topics are partitioned by hour, e.g., /data/kafka_topics/Topic1/hourly/2015/10/06/20, then compaction.input.subdir should be 'hourly'.\n\n\ncompaction.output.subdir is the subdir name of output topics, if exists. For example, if you want to publish compacted data into day-partitioned folders, e.g., /data/compacted_kafka_topics/Topic1/daily/2015/10/06, then compaction.output.subdir should be 'daily'.\n\n\nThere are 3 priority levels: high, normal, low. Topics not included in compaction.high.priority.topics or compaction.normal.priority.topics are considered low priority.\n\n\ncompaction.input.deduplicated and compaction.output.deduplicated controls the behavior of the compaction regarding deduplication. Please see the \nCompaction\n page for more details.\n\n\ncompaction.timebased.max.time.ago and compaction.timebased.min.time.ago controls the earliest and latest input folders to process, when using \nMRCompactorTimeBasedJobPropCreator\n. The format is ?m?d?h, e.g., 3m or 2d10h (m = month, not minute). For example, suppose \ncompaction.timebased.max.time.ago=3d\n, \ncompaction.timebased.min.time.ago=1d\n and the current time is 10/07 9am. Folders whose timestamps are before 10/04 9am, or folders whose timestamps are after 10/06 9am will not be processed.\n\n\ncompaction.timebased.folder.pattern: time pattern in the folder path, when using \nMRCompactorTimeBasedJobPropCreator\n. This should come after \ncompaction.input.subdir\n, e.g., if the input folder to a compaction job is \n/data/compacted_kafka_topics/Topic1/daily/2015/10/06\n, this property should be \nYYYY/mm/dd\n.\n\n\ncompaction.thread.pool.size: how many compaction MR jobs to run concurrently.\n\n\ncompaction.max.num.reducers: max number of reducers for each compaction job\n\n\ncompaction.target.output.file.size: This also controls the number of reducers. The number of reducers will be the smaller of \ncompaction.max.num.reducers\n and \ninput data size\n / compaction.target.output.file.size\n.\n\n\ncompaction.mapred.min.split.size and compaction.mapred.max.split.size are used to control the number of mappers.\n\n\n\n\nDeployment and Checkpoint Management\n\n\nFor deploying Gobblin in standalone or MapReduce mode, please see the \nDeployment\n page.\n\n\nGobblin and Camus checkpoint management are similar in the sense that they both create checkpoint files in each run, and the next run will load the checkpoint files created by the previous run and start from there. Their difference is that Gobblin creates a single checkpoint file per job run or per dataset per job run, and provides two job commit policies: \nfull\n and \npartial\n. In \nfull\n mode, data are only commited for the job/dataset if all workunits of the job/dataset succeeded. Otherwise, the checkpoint of all workunits/datasets will be rolled back. Camus writes one checkpoint file per mapper, and only supports the \npartial\n mode. For Gobblin's state management, please refer to the \nWiki page\n for more information.\n\n\nMigrating from Camus to Gobblin in Production\n\n\nIf you are currently running in production, you can use the following steps to migrate to Gobblin:\n1. Deploy Gobblin based on the instructions in \nDeployment\n and \nKafka-HDFS Ingestion\n, and set the properties mentioned in this page as well as other relevant properties in \nConfiguration Glossary\n to the appropriate values.\n2. Whitelist the topics in Gobblin ingestion, and schedule Gobblin to run at your desired frequency.\n3. Once Gobblin starts running, blacklist these topics in Camus.\n4. If compaction is applicable to you, set up the compaction jobs based on instructions in \nKafka-HDFS Ingestion\n and \nCompaction\n. Whitelist the topics you want to migrate in Gobblin and blacklist them in Camus.", 
            "title": "Camus to Gobblin Migration"
        }, 
        {
            "location": "/miscellaneous/Camus-to-Gobblin-Migration/#table-of-contents", 
            "text": "Table of Contents  Advantages of Migrating to Gobblin  Kafka Ingestion Related Job Config Properties  Config properties for pulling Kafka topics  Config properties for compaction    Deployment and Checkpoint Management  Migrating from Camus to Gobblin in Production    This page is a guide for  Camus  \u2192 Gobblin migration, intended for users and organizations currently using Camus. Camus is LinkedIn's previous-generation Kafka-HDFS pipeline.  It is recommended that one read  Kafka-HDFS Ingestion  before reading this page. This page focuses on the Kafka-related configuration properties in Gobblin vs Camus.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/miscellaneous/Camus-to-Gobblin-Migration/#advantages-of-migrating-to-gobblin", 
            "text": "Operability : Gobblin is a generic data ingestion pipeline that supports not only Kafka but several other data sources, and new data sources can be easily added. If you have multiple data sources, using a single tool to ingest data from these sources is a lot more pleasant operationally than deploying a separate tool for each source.  Performance : The performance of Gobblin in MapReduce mode is comparable to Camus', and faster in some cases (e.g., the average record size of a Kafka topic is not proportional to the average time of pulling a topic) due to a better mapper load balancing algorithm. In the new continuous ingestion mode (currently under development), the performance of Gobblin will further improve.  Metrics and Monitoring : Gobblin has a powerful end-to-end metrics collection and reporting module for monitoring purpose, making it much easier to spot problems in time and find the root causes. See the \"Gobblin Metrics\" section in the wiki and  this post  for more details.  Features : In addition to the above, there are several other useful features for Kafka-HDFS ingestion in Gobblin that are not available in Camus, e.g.,  handling late events in data compaction ; dataset retention management; converter and quality checker; all-or-nothing job commit policy, etc. Also, Gobblin is under active development and new features are added frequently.", 
            "title": "Advantages of Migrating to Gobblin"
        }, 
        {
            "location": "/miscellaneous/Camus-to-Gobblin-Migration/#kafka-ingestion-related-job-config-properties", 
            "text": "This list contains Kafka-specific properties. For general configuration properties please refer to  Configuration Properties Glossary .", 
            "title": "Kafka Ingestion Related Job Config Properties"
        }, 
        {
            "location": "/miscellaneous/Camus-to-Gobblin-Migration/#config-properties-for-pulling-kafka-topics", 
            "text": "Gobblin Property  Corresponding Camus Property  Default value      topic.whitelist  kafka.whitelist.topics  .*    topic.blacklist  kafka.blacklist.topics  a^    mr.job.max.mappers  mapred.map.tasks  100    kafka.brokers  kafka.host.url  (required)    topics.move.to.latest.offset  kafka.move.to.last.offset.list  empty    bootstrap.with.offset  none  latest    reset.on.offset.out.of.range  none  nearest     Remarks:   topic.whitelist and topic.blacklist supports regex.  topics.move.to.latest.offset: Topics in this list will always start from the latest offset (i.e., no records will be pulled). To move all topics to the latest offset, use \"all\". This property is useful in Camus for moving a new topic to the latest offset, but in Gobblin it should rarely, if ever, be used, since you can use bootstrap.with.offset to achieve the same purpose more conveniently.  bootstrap with offset: For new topics / partitions, this property controls whether they start at the earliest offset or the latest offset. Possible values: earliest, latest, skip.  reset.on.offset.out.of.range: This property controls what to do if a partition's previously persisted offset is out of the range of the currently available offsets. Possible values: earliest (always move to earliest available offset), latest (always move to latest available offset), nearest (move to earliest if the previously persisted offset is smaller than the earliest offset, otherwise move to latest), skip (skip this partition).", 
            "title": "Config properties for pulling Kafka topics"
        }, 
        {
            "location": "/miscellaneous/Camus-to-Gobblin-Migration/#config-properties-for-compaction", 
            "text": "Gobblin compaction is comparable to Camus sweeper, which can deduplicate records in an input folder. Compaction is useful for Kafka-HDFS ingestion for two reasons:    Although Gobblin guarantees no loss of data, in rare circumstances where data is published on HDFS but checkpoints failed to be persisted into the state store, it may pull the same records twice.    If you have a hierarchy of Kafka clusters where topics are replicated among the Kafka clusters, duplicate records may be generated during replication.    Below are the configuration properties related to compaction. For more information please visit the MapReduce Compaction section in the  Compaction  page.     Gobblin Property  Corresponding Camus Property  Default value      compaction.input.dir  camus.sweeper.source.dir  (required)    compaction.dest.dir  camus.sweeper.dest.dir  (required)    compaction.input.subdir  camus.sweeper.source.dir  hourly    compaction.dest.subdir  camus.sweeper.dest.dir  daily    compaction.tmp.dest.dir  camus.sweeper.tmp.dir  /tmp/gobblin-compaction    compaction.whitelist  camus.sweeper.whitelist  .*    compaction.blacklist  camus.sweeper.blacklist  a^    compaction.high.priority.topics  none  a^    compaction.normal.priority.topics  none  a^    compaction.input.deduplicated  none  false    compaction.output.deduplicated  none  true    compaction.file.system.uri  none     compaction.timebased.max.time.ago  none  3d    compaction.timebased.min.time.ago  none  1d    compaction.timebased.folder.pattern  none  YYYY/mm/dd    compaction.thread.pool.size  num.threads  20    compaction.max.num.reducers  max.files  900    compaction.target.output.file.size  camus.sweeper.target.file.size  268435456    compaction.mapred.min.split.size  mapred.min.split.size  268435456    compaction.mapred.max.split.size  mapred.max.split.size  268435456    compaction.mr.job.timeout.minutes  none      Remarks:   The following properties support regex: compaction.whitelist, compaction.blacklist, compaction.high.priority.topics, compaction.normal.priority.topics  compaction.input.dir is the parent folder of input topics, e.g., /data/kafka_topics, which contains topic folders such as /data/kafka_topics/Topic1, /data/kafka_topics/Topic2, etc. Note that Camus uses camus.sweeper.source.dir both as the input folder of Camus sweeper (i.e., compaction), and as the output folder for ingesting Kafka topics. In Gobblin, one should use data.publisher.final.dir as the output folder for ingesting Kafka topics.  compaction.output.dir is the parent folder of output topics, e.g., /data/compacted_kafka_topics.  compaction.input.subdir is the subdir name of output topics, if exists. For example, if the input topics are partitioned by hour, e.g., /data/kafka_topics/Topic1/hourly/2015/10/06/20, then compaction.input.subdir should be 'hourly'.  compaction.output.subdir is the subdir name of output topics, if exists. For example, if you want to publish compacted data into day-partitioned folders, e.g., /data/compacted_kafka_topics/Topic1/daily/2015/10/06, then compaction.output.subdir should be 'daily'.  There are 3 priority levels: high, normal, low. Topics not included in compaction.high.priority.topics or compaction.normal.priority.topics are considered low priority.  compaction.input.deduplicated and compaction.output.deduplicated controls the behavior of the compaction regarding deduplication. Please see the  Compaction  page for more details.  compaction.timebased.max.time.ago and compaction.timebased.min.time.ago controls the earliest and latest input folders to process, when using  MRCompactorTimeBasedJobPropCreator . The format is ?m?d?h, e.g., 3m or 2d10h (m = month, not minute). For example, suppose  compaction.timebased.max.time.ago=3d ,  compaction.timebased.min.time.ago=1d  and the current time is 10/07 9am. Folders whose timestamps are before 10/04 9am, or folders whose timestamps are after 10/06 9am will not be processed.  compaction.timebased.folder.pattern: time pattern in the folder path, when using  MRCompactorTimeBasedJobPropCreator . This should come after  compaction.input.subdir , e.g., if the input folder to a compaction job is  /data/compacted_kafka_topics/Topic1/daily/2015/10/06 , this property should be  YYYY/mm/dd .  compaction.thread.pool.size: how many compaction MR jobs to run concurrently.  compaction.max.num.reducers: max number of reducers for each compaction job  compaction.target.output.file.size: This also controls the number of reducers. The number of reducers will be the smaller of  compaction.max.num.reducers  and  input data size  / compaction.target.output.file.size .  compaction.mapred.min.split.size and compaction.mapred.max.split.size are used to control the number of mappers.", 
            "title": "Config properties for compaction"
        }, 
        {
            "location": "/miscellaneous/Camus-to-Gobblin-Migration/#deployment-and-checkpoint-management", 
            "text": "For deploying Gobblin in standalone or MapReduce mode, please see the  Deployment  page.  Gobblin and Camus checkpoint management are similar in the sense that they both create checkpoint files in each run, and the next run will load the checkpoint files created by the previous run and start from there. Their difference is that Gobblin creates a single checkpoint file per job run or per dataset per job run, and provides two job commit policies:  full  and  partial . In  full  mode, data are only commited for the job/dataset if all workunits of the job/dataset succeeded. Otherwise, the checkpoint of all workunits/datasets will be rolled back. Camus writes one checkpoint file per mapper, and only supports the  partial  mode. For Gobblin's state management, please refer to the  Wiki page  for more information.", 
            "title": "Deployment and Checkpoint Management"
        }, 
        {
            "location": "/miscellaneous/Camus-to-Gobblin-Migration/#migrating-from-camus-to-gobblin-in-production", 
            "text": "If you are currently running in production, you can use the following steps to migrate to Gobblin:\n1. Deploy Gobblin based on the instructions in  Deployment  and  Kafka-HDFS Ingestion , and set the properties mentioned in this page as well as other relevant properties in  Configuration Glossary  to the appropriate values.\n2. Whitelist the topics in Gobblin ingestion, and schedule Gobblin to run at your desired frequency.\n3. Once Gobblin starts running, blacklist these topics in Camus.\n4. If compaction is applicable to you, set up the compaction jobs based on instructions in  Kafka-HDFS Ingestion  and  Compaction . Whitelist the topics you want to migrate in Gobblin and blacklist them in Camus.", 
            "title": "Migrating from Camus to Gobblin in Production"
        }, 
        {
            "location": "/miscellaneous/Exactly-Once-Support/", 
            "text": "Table of Contents\n\n\n\n\n\n\nTable of Contents\n\n\nAchieving Exactly-Once Delivery with CommitStepStore\n\n\nScalability\n\n\nAPIs\n\n\n\n\n\n\nThis page outlines the design for exactly-once support in Gobblin. \n\n\nCurrently the flow of publishing data in Gobblin is:\n\n\n\n\nDataWriter writes to staging folder \n\n\nDataWriter moves files from staging folder to task output folder\n\n\nPublisher moves files from task output folder to job output folder\n\n\nPersists checkpoints (watermarks) to state store\n\n\nDelete staging folder and task-output folder.\n\n\n\n\nThis flow does not theoretically guarantee exactly-once delivery, rather, it guarantess at least once. Because if something bad happens in step 4, or between steps 3 and 4, it is possible that data is published but checkpoints are not, and the next run will re-extract and re-publish those records.\n\n\nTo guarantee exactly-once, steps 3 \n 4 should be atomic.\n\n\nAchieving Exactly-Once Delivery with \nCommitStepStore\n\n\nThe idea is similar as write-head logging. Before doing the atomic steps (i.e., steps 3 \n 4), first write all these steps (referred to as \nCommitStep\ns) into a \nCommitStepStore\n. In this way, if failure happens during the atomic steps, the next run can continue doing the rest of the steps before ingesting more data for this dataset.\n\n\nExample\n: Suppose we have a Kafka-HDFS ingestion job, where each Kafka topic is a dataset. Suppose a task generates three output files for topic 'MyTopic':\n\n\ntask-output/MyTopic/2015-12-09/1.avro\ntask-output/MyTopic/2015-12-09/2.avro\ntask-output/MyTopic/2015-12-10/1.avro\n\n\n\n\nwhich should be published to\n\n\njob-output/MyTopic/2015-12-09/1.avro\njob-output/MyTopic/2015-12-09/2.avro\njob-output/MyTopic/2015-12-10/1.avro\n\n\n\n\nAnd suppose this topic has two partitions, and the their checkpoints, i.e., the actual high watermarks are \noffset=100\n and \noffset=200\n.\n\n\nIn this case, there will be 5 CommitSteps for this dataset:\n\n\n\n\nFsRenameCommitStep\n: rename \ntask-output/MyTopic/2015-12-09/1.avro\n to \njob-output/MyTopic/2015-12-09/1.avro\n\n\nFsRenameCommitStep\n: rename \ntask-output/MyTopic/2015-12-09/2.avro\n to \njob-output/MyTopic/2015-12-09/2.avro\n\n\nFsRenameCommitStep\n: rename \ntask-output/MyTopic/2015-12-10/1.avro\n to \njob-output/MyTopic/2015-12-10/1.avro\n\n\nHighWatermarkCommitStep\n: set the high watermark for partition \nMyTopic:0 = 100\n\n\nHighWatermarkCommtiStep\n: set the high watermark for partition \nMyTopic:1 = 200\n\n\n\n\nIf all these \nCommitStep\ns are successful, we can proceed with deleting task-output folder and deleting the above \nCommitStep\ns from the \nCommitStepStore\n. If any of these steps fails, these steps will not be deleted. When the next run starts, for each dataset, it will check whether there are \nCommitStep\ns for this dataset in the CommitStepStore. If there are, it means the previous run may not have successfully executed some of these steps, so it will verify whether each step has been done, and re-do the step if not. If the re-do fails for a certain number of times, this dataset will be skipped. Thus the \nCommitStep\n interface will have two methods: \nverify()\n and \nexecute()\n.\n\n\nScalability\n\n\nThe above approach potentially affects scalability for two reasons:\n\n\n\n\nThe driver needs to write all \nCommitStep\ns to the \nCommitStepStore\n for each dataset, once it determines that all tasks for the dataset have finished. This may cause scalability issues if there are too many \nCommitStep\ns, too many datasets, or too many tasks.\n\n\nUpon the start of the next run, the driver needs to verify all \nCommitStep\ns and redo the \nCommitStep\ns that the previous run failed to do. This may also cause scalability issues if there are too many \nCommitStep\ns.\n\n\n\n\nBoth issues can be resolved by moving the majority of the work to containers, rather than doing it in the driver. \n\n\nFor #1, we can make each container responsible for writing \nCommitStep\ns for a subset of the datasets. Each container will keep polling the \nTaskStateStore\n to determine whether all tasks for each dataset that it is responsible for have finished, and if so, it writes \nCommitStep\ns for this dataset to the \nCommitStepStore\n.\n\n\n#2 can also easily be parallelized where we have each container responsible for a subset of datasets.\n\n\nAPIs\n\n\nCommitStep\n:\n\n\n/**\n * A step during committing in a Gobblin job that should be atomically executed with other steps.\n */\npublic abstract class CommitStep {\n\n  private static final Gson GSON = new Gson();\n\n  public static abstract class Builder\nT extends Builder\n?\n {\n  }\n\n  protected CommitStep(Builder\n?\n builder) {\n  }\n\n  /**\n   * Verify whether the CommitStep has been done.\n   */\n  public abstract boolean verify() throws IOException;\n\n  /**\n   * Execute a CommitStep.\n   */\n  public abstract boolean execute() throws IOException;\n\n  public static CommitStep get(String json, Class\n? extends CommitStep\n clazz) throws IOException {\n    return GSON.fromJson(json, clazz);\n  }\n}\n\n\n\n\nCommitSequence\n:\n\n\n@Slf4j\npublic class CommitSequence {\n  private final String storeName;\n  private final String datasetUrn;\n  private final List\nCommitStep\n steps;\n  private final CommitStepStore commitStepStore;\n\n  public CommitSequence(String storeName, String datasetUrn, List\nCommitStep\n steps, CommitStepStore commitStepStore) {\n    this.storeName = storeName;\n    this.datasetUrn = datasetUrn;\n    this.steps = steps;\n    this.commitStepStore = commitStepStore;\n  }\n\n  public boolean commit() {\n    try {\n      for (CommitStep step : this.steps) {\n        if (!step.verify()) {\n          step.execute();\n        }\n      }\n      this.commitStepStore.remove(this.storeName, this.datasetUrn);\n      return true;\n    } catch (Throwable t) {\n      log.error(\nCommit failed for dataset \n + this.datasetUrn, t);\n      return false;\n    }\n  }\n}\n\n\n\n\nCommitStepStore\n:\n\n\n/**\n * A store for {@link CommitStep}s.\n */\npublic interface CommitStepStore {\n\n  /**\n   * Create a store with the given name.\n   */\n  public boolean create(String storeName) throws IOException;\n\n  /**\n   * Create a new dataset URN in a store.\n   */\n  public boolean create(String storeName, String datasetUrn) throws IOException;\n\n  /**\n   * Whether a dataset URN exists in a store.\n   */\n  public boolean exists(String storeName, String datasetUrn) throws IOException;\n\n  /**\n   * Remove a given store.\n   */\n  public boolean remove(String storeName) throws IOException;\n\n  /**\n   * Remove all {@link CommitStep}s for the given dataset URN from the store.\n   */\n  public boolean remove(String storeName, String datasetUrn) throws IOException;\n\n  /**\n   * Put a {@link CommitStep} with the given dataset URN into the store.\n   */\n  public boolean put(String storeName, String datasetUrn, CommitStep step) throws IOException;\n\n  /**\n   * Get the {@link CommitSequence} associated with the given dataset URN in the store.\n   */\n  public CommitSequence getCommitSequence(String storeName, String datasetUrn) throws IOException;\n\n}", 
            "title": "Exactly Once Support"
        }, 
        {
            "location": "/miscellaneous/Exactly-Once-Support/#table-of-contents", 
            "text": "Table of Contents  Achieving Exactly-Once Delivery with CommitStepStore  Scalability  APIs    This page outlines the design for exactly-once support in Gobblin.   Currently the flow of publishing data in Gobblin is:   DataWriter writes to staging folder   DataWriter moves files from staging folder to task output folder  Publisher moves files from task output folder to job output folder  Persists checkpoints (watermarks) to state store  Delete staging folder and task-output folder.   This flow does not theoretically guarantee exactly-once delivery, rather, it guarantess at least once. Because if something bad happens in step 4, or between steps 3 and 4, it is possible that data is published but checkpoints are not, and the next run will re-extract and re-publish those records.  To guarantee exactly-once, steps 3   4 should be atomic.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/miscellaneous/Exactly-Once-Support/#achieving-exactly-once-delivery-with-commitstepstore", 
            "text": "The idea is similar as write-head logging. Before doing the atomic steps (i.e., steps 3   4), first write all these steps (referred to as  CommitStep s) into a  CommitStepStore . In this way, if failure happens during the atomic steps, the next run can continue doing the rest of the steps before ingesting more data for this dataset.  Example : Suppose we have a Kafka-HDFS ingestion job, where each Kafka topic is a dataset. Suppose a task generates three output files for topic 'MyTopic':  task-output/MyTopic/2015-12-09/1.avro\ntask-output/MyTopic/2015-12-09/2.avro\ntask-output/MyTopic/2015-12-10/1.avro  which should be published to  job-output/MyTopic/2015-12-09/1.avro\njob-output/MyTopic/2015-12-09/2.avro\njob-output/MyTopic/2015-12-10/1.avro  And suppose this topic has two partitions, and the their checkpoints, i.e., the actual high watermarks are  offset=100  and  offset=200 .  In this case, there will be 5 CommitSteps for this dataset:   FsRenameCommitStep : rename  task-output/MyTopic/2015-12-09/1.avro  to  job-output/MyTopic/2015-12-09/1.avro  FsRenameCommitStep : rename  task-output/MyTopic/2015-12-09/2.avro  to  job-output/MyTopic/2015-12-09/2.avro  FsRenameCommitStep : rename  task-output/MyTopic/2015-12-10/1.avro  to  job-output/MyTopic/2015-12-10/1.avro  HighWatermarkCommitStep : set the high watermark for partition  MyTopic:0 = 100  HighWatermarkCommtiStep : set the high watermark for partition  MyTopic:1 = 200   If all these  CommitStep s are successful, we can proceed with deleting task-output folder and deleting the above  CommitStep s from the  CommitStepStore . If any of these steps fails, these steps will not be deleted. When the next run starts, for each dataset, it will check whether there are  CommitStep s for this dataset in the CommitStepStore. If there are, it means the previous run may not have successfully executed some of these steps, so it will verify whether each step has been done, and re-do the step if not. If the re-do fails for a certain number of times, this dataset will be skipped. Thus the  CommitStep  interface will have two methods:  verify()  and  execute() .", 
            "title": "Achieving Exactly-Once Delivery with CommitStepStore"
        }, 
        {
            "location": "/miscellaneous/Exactly-Once-Support/#scalability", 
            "text": "The above approach potentially affects scalability for two reasons:   The driver needs to write all  CommitStep s to the  CommitStepStore  for each dataset, once it determines that all tasks for the dataset have finished. This may cause scalability issues if there are too many  CommitStep s, too many datasets, or too many tasks.  Upon the start of the next run, the driver needs to verify all  CommitStep s and redo the  CommitStep s that the previous run failed to do. This may also cause scalability issues if there are too many  CommitStep s.   Both issues can be resolved by moving the majority of the work to containers, rather than doing it in the driver.   For #1, we can make each container responsible for writing  CommitStep s for a subset of the datasets. Each container will keep polling the  TaskStateStore  to determine whether all tasks for each dataset that it is responsible for have finished, and if so, it writes  CommitStep s for this dataset to the  CommitStepStore .  #2 can also easily be parallelized where we have each container responsible for a subset of datasets.", 
            "title": "Scalability"
        }, 
        {
            "location": "/miscellaneous/Exactly-Once-Support/#apis", 
            "text": "CommitStep :  /**\n * A step during committing in a Gobblin job that should be atomically executed with other steps.\n */\npublic abstract class CommitStep {\n\n  private static final Gson GSON = new Gson();\n\n  public static abstract class Builder T extends Builder ?  {\n  }\n\n  protected CommitStep(Builder ?  builder) {\n  }\n\n  /**\n   * Verify whether the CommitStep has been done.\n   */\n  public abstract boolean verify() throws IOException;\n\n  /**\n   * Execute a CommitStep.\n   */\n  public abstract boolean execute() throws IOException;\n\n  public static CommitStep get(String json, Class ? extends CommitStep  clazz) throws IOException {\n    return GSON.fromJson(json, clazz);\n  }\n}  CommitSequence :  @Slf4j\npublic class CommitSequence {\n  private final String storeName;\n  private final String datasetUrn;\n  private final List CommitStep  steps;\n  private final CommitStepStore commitStepStore;\n\n  public CommitSequence(String storeName, String datasetUrn, List CommitStep  steps, CommitStepStore commitStepStore) {\n    this.storeName = storeName;\n    this.datasetUrn = datasetUrn;\n    this.steps = steps;\n    this.commitStepStore = commitStepStore;\n  }\n\n  public boolean commit() {\n    try {\n      for (CommitStep step : this.steps) {\n        if (!step.verify()) {\n          step.execute();\n        }\n      }\n      this.commitStepStore.remove(this.storeName, this.datasetUrn);\n      return true;\n    } catch (Throwable t) {\n      log.error( Commit failed for dataset   + this.datasetUrn, t);\n      return false;\n    }\n  }\n}  CommitStepStore :  /**\n * A store for {@link CommitStep}s.\n */\npublic interface CommitStepStore {\n\n  /**\n   * Create a store with the given name.\n   */\n  public boolean create(String storeName) throws IOException;\n\n  /**\n   * Create a new dataset URN in a store.\n   */\n  public boolean create(String storeName, String datasetUrn) throws IOException;\n\n  /**\n   * Whether a dataset URN exists in a store.\n   */\n  public boolean exists(String storeName, String datasetUrn) throws IOException;\n\n  /**\n   * Remove a given store.\n   */\n  public boolean remove(String storeName) throws IOException;\n\n  /**\n   * Remove all {@link CommitStep}s for the given dataset URN from the store.\n   */\n  public boolean remove(String storeName, String datasetUrn) throws IOException;\n\n  /**\n   * Put a {@link CommitStep} with the given dataset URN into the store.\n   */\n  public boolean put(String storeName, String datasetUrn, CommitStep step) throws IOException;\n\n  /**\n   * Get the {@link CommitSequence} associated with the given dataset URN in the store.\n   */\n  public CommitSequence getCommitSequence(String storeName, String datasetUrn) throws IOException;\n\n}", 
            "title": "APIs"
        }
    ]
}